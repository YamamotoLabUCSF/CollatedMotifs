{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## CollatedMotifs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "https://github.com/YamamotoLabUCSF/CollatedMotifs  \n",
    "v1.1/Committed 07-01-2021\n",
    " \n",
    "DNA sequence-selective transcription factors (TFs) mediate gene regulation; their interactions with DNA contribute to the formation of nucleoprotein structures that modulate transcription at target genes.  These functional units -- **response elements** (*e.g.*, enhancers/*cis*-regulatory modules) -- integrate cellular signals to regulate the types of gene transcripts produce by a cell, and when and how much of each transcript type is made.  Genomic editing by programmable nucleases (*e.g.*, CRISPR-Cas9) routinely yields mixed allelic mutation at target loci (*e.g.*, variable insertion *vs.* deletion, indel length across edited cells).  For editing efforts targeted to putative response elements, widely available pattern-matching tools enable prediction of transcription factor binding sites (TFBS) at altered loci, based on matches to position frequency matrices of known TFs.  Awareness of altered TFBSs in genomically edited alleles can aid prediction and/or interpretation of functional consequences associated with mutations.\n",
    "\n",
    "<img src=\"CollatedMotifs_img/CollatedMotifs_thumbnail.png\" align=\"right\" width=\"650\"> \n",
    "\n",
    "**This script automates allele prediction and TFBS collation for deeply sequenced amplicons, and reports TFBS 'lost' and 'new' relative to user-supplied reference sequence(s).**\n",
    "\n",
    "### Potential uses:  \n",
    "This script was developed to enable rapid assessment of TFBS differences at target loci in mutant clones, following Cas9-editing (CRISPR-Cas9 mutagenesis) and clonal isolation.\n",
    "\n",
    "### Synopsis:  \n",
    "**This script returns allele definitions annotated with lost and/or gained TFBS (relative to a reference sequence), for samples from a demultiplexed NGS fastq dataset** \n",
    ">(see 'Output notes' for file output details).  \n",
    "\n",
    "**Users are asked for paths to specific directories (*e.g.*, output and input directories), locally installed executables (BLASTN & MAKEBLASTDB (NCBI), FIMO & FASTA-GET-MARKOV (MEME)), and files (fasta file containing reference sequence(s) for TFBS comparison, fasta file containing reference sequence(s) for alignment, text file containing position frequency matrices for TFBS)**  \n",
    ">(see 'Input notes' for details).\n",
    "    \n",
    "Python3, BLASTN (NCBI), MAKEBLASTDB (NCBI), FIMO (MEME), and FASTA-GET-MARKOV (MEME) are required for operation.  \n",
    "\n",
    "BLASTN & its associated executable MAKEBLASTDB can be downloaded and locally installed at https://www.ncbi.nlm.nih.gov/guide/howto/run-blast-local/.  \n",
    "\n",
    "FIMO, its associated executable FASTA-GET-MARKOV, and positional frequency matrix files can be downloaded and locally installed at http://meme-suite.org/doc/fimo.html.\n",
    "\n",
    "For usage details, please refer to README file at GitHub and to the following manuscript:  \n",
    ">*Ehmsen, Knuesel, Martinez, Asahina, Aridomi, Yamamoto (2021)*\n",
    "    \n",
    "Please cite usage as:  \n",
    ">CollatedMotifs.py  \n",
    ">*Ehmsen, Knuesel, Martinez, Asahina, Aridomi, Yamamoto (2021)*\n",
    " \n",
    "--------\n",
    "\n",
    "### Operation notes:  \n",
    "*What does this script do?*\n",
    " 1. **classify & count reads:** merges R1 and R2 sequences into a single read, counts unique read types per well (*i.e.*, sample); fastq file name provides the sample name  \n",
    " \n",
    " \n",
    " 2. **identify top 5 reads** per well (in terms of read abundance); calculates representation among reads within the well at four levels:  \n",
    " \n",
    "   (a) raw frequency (% read type in question, relative to total reads)  \n",
    "   (b) percentile (% of other read types that fall below the frequency of the read type in question)  \n",
    "   (c) adjusted frequency @ 1% (% read type in question, relative to reads that occur at >1% frequency)  \n",
    "   (d) adjusted frequency @ 10% (% read type in question, relative to reads that occur at >10% frequency)  \n",
    " \n",
    " \n",
    " 3. **align to reference database:** aligns top 5 reads to reference sequence(s) using BLASTN  \n",
    " *(National Center for Biotechnology Information;\n",
    "    Altschul S.F. et al. (1990) \"Basic local alignment search tool\", J Mol Biol. 15(3):403-10)*  \n",
    "      * Alignment database is created within the script by MAKEBLASTDB, from user-provided, fasta-formatted reference sequence(s)  \n",
    "    <img src=\"CollatedMotifs_img/MAKEBLASTDB_and_BLASTN_reference_database_thumbnail.png\" align=\"left\" width=\"300\">\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    " 4. **identify TFBSs in reference and allele sequences:** for user-provided reference sequences, uses FIMO and user-provided positional frequency matrix file to find matches to TFBS motifs  \n",
    "     * Background Markov file for TFBS match statistics is created within the script by FASTA-GET-MARKOV, from user-provided, fasta-formatted reference sequence(s)  \n",
    "    *(FIMO: Grant C.E. et al. (2011) \"FIMO: Scanning for occurrences of a given motif\", Bioinformatics 27(7):1017–1018)*  \n",
    "    *(MEME Suite; Bailey T.L. et al. (2015) \"The MEME Suite\", Nucleic Acids Res 43(Web Server issue):W39–W49)*\n",
    "<img src=\"CollatedMotifs_img/FASTAGETMARKOV_and_Markov_background_thumbnail.png\" align=\"left\" width=\"350\">\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    " 5. **return collation of novel *vs.* lost TFBSs:** compares TFBS in reads to TFBS in specific reference sequence, outputting 'new' and 'lost' TFBSs relative to the reference sequence.\n",
    " \n",
    "\n",
    " \n",
    "--------\n",
    "### Input notes:\n",
    "You will be prompted for the following user-specific information (up to 11 items):\n",
    "\n",
    "**Required** (10 strings: 9 strings specifying directory, executable, or file locations, + 1 string specifying prefix to be assigned to BLASTN database files) \n",
    "\n",
    "  * paths to directories (2)\n",
    "    <ul>\n",
    "    <li>where should output files go?</li>  \n",
    "    <i>path to <strong>output directory</strong> for output files</i>\n",
    "    <li>where are input files found?</li>\n",
    "    <i>path to single directory containing <strong>demultiplexed fastq files</strong></i> \n",
    "    </ul>\n",
    "<br clear=\"all\" />\n",
    "  * paths to executables (4)  \n",
    "     <ul>\n",
    "     <li>where is BLASTN executable found?</li>\n",
    "         <i>path to <strong>BLASTN</strong> installation</i>\n",
    "     <li>where is MAKEBLASTDB executable found?</li>\n",
    "         <i>path to <strong>MAKEBLASTDB</strong> installation</i>\n",
    "     <li>where is FIMO executable found?</li>\n",
    "         <i>path to <strong>FIMO</strong> installation</i>\n",
    "     <li>where is FASTA-GET-MARKOV executable found?</li>\n",
    "         <i>path to <strong>FASTA-GET-MARKOV</strong> installation</i>\n",
    "     </ul>\n",
    "<br clear=\"all\" />      \n",
    "  * paths to files (3)\n",
    "     <ul>\n",
    "     <li>what are your reference sequence(s), to which you will (a) align sequenced reads, and (b) compare sequenced reads for TFBS occurrence?</li>\n",
    "        <i>path to single <strong>fasta file</strong>, containing <strong>reference sequence(s)</strong> for processing by (a) MAKEBLASTDB, to generate a database reference for BLASTN, and (b) FIMO, to establish TFBS occurrence(s) to be evaluated relative to sequenced reads</i>\n",
    "     <li>what are the TFBS motif(s) for which you will search, and for which you will draw comparisons for presence/absence between sequences?</li>\n",
    "        <i>path to single <strong>text file</strong>, containing <strong>position frequency matrix(ces)</strong> for TFs</i>\n",
    "     <li>what DNA sequence(s) will you use as a basis for markov background estimation, to be used by FIMO?</li>\n",
    "        <i>path to single <strong>text/fasta file</strong>, containing DNA sequence(s) from which a <strong>markov background file</strong> will be generated for use by FIMO</i>    \n",
    "     </ul>\n",
    "<br clear=\"all\" />\n",
    "  * label for database files created in 'alignment_directory' by MAKEBLASTDB (1)\n",
    "     <ul>\n",
    "     <li>what common prefix (*) will you assign to the six files (*.nin, *.nhr, *.nog, *.nsd, *.nsg, *.nsi) created by MAKEBLASTDB, as the alignment database for BLASTN?</li>\n",
    "    </ul>  \n",
    "   \n",
    "**Optional** (1 string specifying transcription factor (TF) of interest) \n",
    "  * transcription factor (TF) of interest (1)\n",
    "     <ul>\n",
    "\n",
    "------\n",
    "\n",
    "### Output notes:\n",
    "This script produces 6 output files in the user-specified output directory, plus 3 sub-directories:  \n",
    "\n",
    "  - 3 **sub-directories** comprise outputs of MAKEBLASTDB and FIMO:  \n",
    "  \n",
    "    - two directories contain FIMO output files (fimo_out and fimo_out_ref); each of these sub-directories contains 5 subsidiary files created by FIMO (cisml.xml, fimo.gff, fimo.html, fimo.tsv, fimo.xml)  \n",
    "    - one directory comprises BLASTN alignment database (alignment_database); this directory contains 6 subsidiary files created by MAKEBLASTDB operation on user-supplied fasta file containing reference sequence(s) (\\*.nin, \\*.nhr, \\*.nog, \\*.nsd, \\*.nsg, \\*.nsi)  \n",
    " <br clear=\"all\" />\n",
    "  - 6 **output files** in the user-specified output directory; these include:\n",
    "     \n",
    "  \n",
    "    1. fasta.fa  \n",
    "        (collection of fasta entries representing top 5 most abundant sequences assigned to a single sample ID)  \n",
    "\n",
    "\t2. blastn_alignments.txt  \n",
    "        (output of BLASTN operation on fasta.fa)  \n",
    "        \n",
    "    3. markov_background.txt  \n",
    "        (output of FASTA-GET-MARKOV operation on user-supplied fasta reference file)  \n",
    "        \n",
    "    4. collated_TFBS.txt  \n",
    "        (output of script operation on FIMO-generated .tsv files in fimo_out and fimo_out_ref)\n",
    "        \n",
    "    5. collated_TFBS.xlsx\n",
    "        (output of script interpretation of lost and gained TFBS, detailed for inferred alleles in spreadsheet)\n",
    "     \n",
    "    6. script_metrics.txt  \n",
    "        (summary/analysis of script operation metrics (metadata))\n",
    "\n",
    "           Directory structure under an output directory specified as 'CollatedMotifs', for example,\n",
    "           would contain the following subdirectories and files following CollatedMotifs.py operations:\n",
    "\n",
    "           /CollatedMotifs \n",
    "                          `-----/alignment_database\n",
    "                                        `----------*.nin\n",
    "                                        `----------*.nhr\n",
    "                                        `----------*.nog\n",
    "                                        `----------*.nsd\n",
    "                                        `----------*.nsg\n",
    "                                        `----------*.nsi\n",
    "                          `-----blastn_alignments.txt\n",
    "                          `-----collated_TFBS.txt\n",
    "                          `-----collated_TFBS.xlsx\n",
    "                          `-----fasta.fa\n",
    "                          `-----/fimo_out\n",
    "                                        `----------cisml.xml\n",
    "                                        `----------fimo.gff\n",
    "                                        `----------fimo.html\n",
    "                                        `----------fimo.tsv\n",
    "                                        `----------fimo.xml\n",
    "                          `-----/fimo_out_ref\n",
    "                                        `----------cisml.xml\n",
    "                                        `----------fimo.gff\n",
    "                                        `----------fimo.html\n",
    "                                        `----------fimo.tsv\n",
    "                                        `----------fimo.xml\n",
    "                          `-----markov_background.txt  \n",
    "                          `-----script_metrics.txt\n",
    "--------\n",
    "### Visual summary of key script operations:  \n",
    "In short, sequencing data in a sample-specific **fastq file** (*e.g.,* below), are converted to user-interpretable allele definitions (alignments to a reference sequence) annotated with **TFBS motif(s) lost and/or gained relative to a reference sequence** (**key output files**, below), for 100s to 1000s of samples.  \n",
    "##### example of input fastq file  \n",
    "<img src=\"CollatedMotifs_img/fastq_example.png\" align=\"left\" width=\"700\">\n",
    "<br clear=\"all\" />  \n",
    "\n",
    "#### Key output files:  \n",
    "##### collated_TFBS.txt\n",
    "<img src=\"CollatedMotifs_img/Example_CollatedMotifs_output.png\" align=\"left\" width=\"900\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "##### collated_TFBS.xlsx  \n",
    "*up to **8 worksheets** occur in this Excel spreadsheet file (example with NR3C1 as TF of interest):*  \n",
    "<img src=\"CollatedMotifs_img/Example_CollatedMotifs_output_xlsx_tab_names.png\" align=\"left\" width=\"700\">\n",
    "<br clear=\"all\" />  \n",
    "*these worksheets are:*   \n",
    "<img src=\"CollatedMotifs_img/CollatedMotifs_output_xlsx_synopsis.png\" align=\"left\" width=\"700\">\n",
    "<br clear=\"all\" />  \n",
    "*example below is for one of the 8 worksheets (\"2 TBFS, lost-regained pairs\"), which interprets TFBSs lost in an allele (relative to reference) that positionally coincide with a FIMO-identified new TFBS for the same TF (therefore designated as 'lost-regained' pairs):*    \n",
    "<img src=\"CollatedMotifs_img/Example_CollatedMotifs_output_xlsx.png\" align=\"left\" width=\"1000\">\n",
    "<br clear=\"all\" />  \n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "**Welcome.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code display can be toggled on/off here\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I. Setup  \n",
    "Import libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for availability of Python dependencies in path\n",
    "missing_dependencies_list = []\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('psutil')\n",
    "    \n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('numpy')\n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('scipy')\n",
    "    \n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('pandas')\n",
    "    \n",
    "if len(missing_dependencies_list) > 0:\n",
    "    print('ModuleNotFoundError\\n')\n",
    "    print('Please note, the following required Python module(s) are not found in your Python system path:')\n",
    "    for i in missing_dependencies_list:\n",
    "        print('   '+i)\n",
    "    print('\\nPlease exit the script and install these Python dependencies in your system path.')\n",
    "    print(\"\"\"\\nGuidelines for installation of Python dependencies can be found in the README file for CollatedMotifs.py ('System Setup')\"\"\")\n",
    "    print(\"\"\"    (Creation of a Python virtual environment is recommended)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Operating system interfaces\n",
    "import os\n",
    "\n",
    "# Time access and conversions, Basic data and time types\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# System-specific parameters and functions\n",
    "import sys\n",
    "\n",
    "# Process and system utilities\n",
    "import psutil\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Gzip to read GNU zipped files\n",
    "import gzip\n",
    "\n",
    "# Low-level networking interface\n",
    "import socket\n",
    "\n",
    "# System version information\n",
    "import platform\n",
    "\n",
    "# Unix-style pathname pattern expansion\n",
    "import glob\n",
    "\n",
    "# NumPy (numeric operations)\n",
    "import numpy\n",
    "\n",
    "# SciPy (for percentile) \n",
    "from scipy import stats\n",
    "\n",
    "# Container datatypes (for Counter operation)\n",
    "from collections import Counter\n",
    "\n",
    "# Decimal fixed point and floating point arithmetic\n",
    "from decimal import Decimal\n",
    "\n",
    "# Regular expression operations\n",
    "import re\n",
    "\n",
    "# Object-oriented filesystem paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Internationalization services (for use of thousands separator in numbers where appropriate)\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# start time\n",
    "initialTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions  \n",
    "*User inputs can be entered either in rapid succession ('List' format), or in response to individually coached prompts. 'Prompts' defines a series of 11 coached entries that provide a user with instructive detail regarding the nature of required input.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'prompts' function for coached user input\n",
    "def prompts():\n",
    "    \"\"\"Coached prompts to collect user input\"\"\"\n",
    "    # Make variables assigned in prompts() function globally available\n",
    "    global output_directory\n",
    "    global fastq_directory\n",
    "    global fasta_ref\n",
    "    global blastn_path\n",
    "    global makeblastdb_path\n",
    "    global db_prefix\n",
    "    global fimo_path\n",
    "    global fimo_motifs_path\n",
    "    global fasta_get_markov_path\n",
    "    global markov_background_file\n",
    "    global TF_of_interest\n",
    "    # 1-Specify output directory.\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Location of OUTPUT DIRECTORY for output files\n",
    "---------------------------------------------\n",
    "    \n",
    "This script produces 6 output files in the user-specified output directory, plus three directories:\n",
    "two directories and subsidiary files created by FIMO (fimo_out and fimo_out_ref) and one directory\n",
    "and subsidiary files created by MAKEBLASTDB (alignment_database).\n",
    "    \n",
    "CollatedMotifs.py output files include:\n",
    "    \n",
    "    1. fasta.fa\n",
    "\n",
    "    2. blastn_alignments.txt\n",
    "        (output of BLASTN operation on fasta.fa)\n",
    "\n",
    "    3. markov_background.txt\n",
    "        (output of FASTA-GET-MARKOV operation on user-supplied fasta reference file)\n",
    "\n",
    "    4. collated_TFBS.txt\n",
    "        (output of script operation on FIMO-generated .tsv files in fimo_out and fimo_out_ref)\n",
    "        \n",
    "    5. collated_TFBS.xlsx\n",
    "        (output of script interpretation of lost and gained TFBS, detailed for inferred alleles in spreadsheet)\n",
    "         \n",
    "    6. script_metrics.txt (summary/analysis of script operation metrics [metadata])\n",
    "    \n",
    "        Note: \n",
    "        * These files do not exist before the script is run. The files are made by the script.\n",
    "        * The primary data outputs for TFBS comparisons are found in collated_TFBS.txt\n",
    "        \n",
    "At this prompt, indicate an absolute path to a ** directory ** that will be created by the script as the location\n",
    "for output files.  This directory should not exist yet -- it will be created as an output of this script, and will\n",
    "be populated with the file outputs of this specific instance of the script operation.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators, regardless of operating system (Mac or Windows).\n",
    "\n",
    "Example: if you'd like to create a directory ('CollatedMotifs') in an existing directory ('Illumina'), accessed\n",
    "with absolute path of '/Users/myname/Illumina/CollatedMotifs' (Mac) or 'C:\\Users\\myname\\Illumina\\CollatedMotifs'\n",
    "(Windows), enter '/Users/myname/Illumina/CollatedMotifs' at the command line prompt. Replace 'myname' with the\n",
    "appropriate intervening directory identifiers. Do *not* flank your entry with quotation marks (') at the\n",
    "command-line.\n",
    "    \n",
    "Alternatively, simply enter a desired directory name (e.g., 'CollatedMotifs') and run this script from\n",
    "within a directory where you'd like to create this new directory.\"\"\"+'\\n')\n",
    "    output_directory = input(r\"\"\"    -----> Output directory name and path:  \"\"\")\n",
    "    # 2-Specify the fastq files to be used for input, by indicating directory location of the file list.\n",
    "    print(r\"\"\"\n",
    "------------------------------------------------------------------------------\n",
    "Location of INPUT FILES (single directory containing demutiplexed fastq files)\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "You will now be asked to enter the path to the directory containing the fastq files\n",
    "to be processed as CollatedMotifs.py input.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your fastq input files are named file1.fastq, file2.fastq, etc. and are found in a directory\n",
    "named 'Sequences' with absolute path of '/Users/myname/Sequences' (Mac) or 'C:\\Users\\myname\\Sequences' (PC),\n",
    "enter '/Users/myname/Sequences' at the command line prompt.\n",
    "\n",
    "When you're done entering the fastq file location, press 'Enter' again to proceed in the script.\"\"\"+'\\n')\n",
    "    fastq_directory = input(r\"\"\"    -----> Directory name and path:  \"\"\")\n",
    "    # 3-Specify fasta file containing reference sequences as basis for TFBS motif comparisons/contrasts.\n",
    "    print(r\"\"\"\n",
    "-----------------------------------------\n",
    "Location of FIMO REFERENCE SEQUENCES FILE\n",
    "-----------------------------------------\n",
    "\n",
    "This script aligns and compares your top sample read sequence(s) to a defined reference sequence,\n",
    "as its basis for determining distinct vs. common TFBS motifs. Please indicate the absolute path to a\n",
    "fasta file containing reference sequence(s).\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "**Important**: Each fasta entry definition line (defline) should be named such that the defline name\n",
    "matches a unique descriptor (character string) that occurs in the fastq file names for samples that\n",
    "will be aligned and compared to the corresponding fasta entry. For all ranked alleles for a given sample,\n",
    "the fastq file name is incorporated into the allele names; the script then relies on a character string\n",
    "to match between the allele name and an entry in the fasta reference file, to understand which reference\n",
    "sequence to use for alignment and comparison of TFBSs that occur between the allele and the reference.\n",
    "\n",
    "Example: if you have samples screened by PCR amplification across three distinct loci (Locus1, Locus2,\n",
    "and Locus3), the fastq file names might be named Locus1_A01.fastq, Locus1_A02.fastq, etc.; Locus2_A01.fastq,\n",
    "Locus2_A02.fastq, etc.; Locus3_A01.fastq, Locus3_A02.fastq, etc.\n",
    "\n",
    "For the fasta reference sequences, you would designate deflines for the three different reference sequences such\n",
    "that the deflines are character strings with diagnostic matches to character strings that occur in the \n",
    "corresponding sample fastq file names (such as 'Locus1', 'Locus2', 'Locus3' for the example sample sets above.\n",
    "Prepare the reference sequences in fasta format, saved in a single text file.\n",
    "    \n",
    "    >Locus1\n",
    "    GATCGACTAGAGCGAGCATTCATCATATCACGAGTAGCATCGACGTGCACGATCGATCGTAGCTAGCTAGTCATGCATGCATGCTAGATTCGAGCATGCATGCTAC\n",
    "    >Locus2\n",
    "    AGTAGCTGTGATGCTAGTCATCTAGCTAGCAGCGTAGCTAGCGATCGATCTAGAGCCGATCGATCGAGCATCTAGCTATCAGCGGCGGGATCATCTATCTACGGG\n",
    "    >Locus3\n",
    "    CGATGCAGCGCGATCGAGCGCGATCGATATTAGCATGCGCAGCTAGCTAGCTGGCGATCGATGCATGCTAGCTGTGTCAGTCGACGATCACACGATCACACTGTGTG\n",
    "\n",
    "When you're done entering the path to the reference sequence file, press 'Enter' again to proceed in the script.\"\"\"+'\\n')\n",
    "    fasta_ref = input(r\"\"\"    -----> Path to fasta file containing reference sequences:  \"\"\")\n",
    "    # 4-Collect path to blastn executable.\n",
    "    print(r\"\"\"\n",
    "-----------------------------\n",
    "Location of BLASTN EXECUTABLE\n",
    "-----------------------------\n",
    "\n",
    "This script uses BLASTN (NCBI) to align reads from your fastq files to a reference sequence database.\n",
    "Please indicate the absolute path to the BLASTN executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your BLASTN executable is found at absolute path /Users/myname/blastn, type '/Users/myname/blastn'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    blastn_path = input(r\"\"\"    -----> Path to BLASTN executable:  \"\"\")\n",
    "    # 5-Collect path to makeblastdb executable.\n",
    "    print(r\"\"\"\n",
    "----------------------------------\n",
    "Location of MAKEBLASTDB EXECUTABLE\n",
    "----------------------------------\n",
    "\n",
    "Because this script uses BLASTN (NCBI) to align reads from your fastq files to a reference sequence database,\n",
    "a compatible reference sequence database is required. This script uses MAKEBLASTDB (NCBI) to generate\n",
    "a reference sequence database from the reference sequences in the fasta file you provided earlier.\n",
    "    \n",
    "Please indicate the absolute path to the MAKEBLASTDB executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your MAKEBLASTDB executable is found at absolute path /Users/myname/makeblastdb,\n",
    "type '/Users/myname/makeblastdb' and press Enter.\"\"\"+'\\n')\n",
    "    makeblastdb_path = input(r\"\"\"    -----> Path to MAKEBLASTDB executable:  \"\"\")\n",
    "    # 6-Specify prefix to files in database\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Prefix for files in BLASTN ALIGNMENT DATABASE\n",
    "---------------------------------------------\n",
    "\n",
    "Because this script uses BLASTN (NCBI) and an alignment reference database, a common prefix identifier for the six\n",
    "database files generated by MAKEBLASTDB is needed.\n",
    "\n",
    "Please indicate a prefix to assign to each of the database files.\n",
    "\n",
    "Example: if your alignment reference was generated by MAKEBLASTDB from a fasta file called GRCh38.fa,\n",
    "the alignment database files will have been assigned the prefix 'GRCh38'; you would type 'GRCh38'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    db_prefix = input(r\"\"\"    -----> Prefix for alignment reference sequence database files:  \"\"\")\n",
    "    # 7-Specify path to FIMO installation\n",
    "    print(r\"\"\"\n",
    "---------------------------\n",
    "Location of FIMO EXECUTABLE\n",
    "---------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate the absolute path to the FIMO installation.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your FIMO executable is found at absolute path /Users/myname/fimo, type '/Users/myname/fimo'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    fimo_path = input(r\"\"\"    -----> Path to FIMO executable:  \"\"\")\n",
    "    # 8-Specify path to FIMO motif file.\n",
    "    print(r\"\"\"\n",
    "----------------------------\n",
    "Location of FIMO MOTIFS FILE\n",
    "----------------------------\n",
    "\n",
    "This script uses FIMO from the meme suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBS motifs.\n",
    "\n",
    "Please indicate the absolute path to the FIMO motifs file (containing position frequency matrix/matrices).\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "When you're done entering the location of the motifs file, press Enter.\"\"\"+'\\n')\n",
    "    fimo_motifs_path = input(r\"\"\"    -----> Path to FIMO motifs file:  \"\"\")\n",
    "    # 9-Specify path to FIMO fasta-get-markov installation.\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Location of FIMO FASTA-GET-MARKOV EXECUTABLE\n",
    "---------------------------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate an absolute path to the location of the FASTA-GET-MARKOV executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "When you're done entering the location of the executable, press Enter.\"\"\"+'\\n')\n",
    "    fasta_get_markov_path = input(r\"\"\"    -----> Path to FASTA-GET-MARKOV executable:  \"\"\")\n",
    "    # 10-Specify path to markov background file.\n",
    "    print(r\"\"\"\n",
    "------------------------------------------------------------\n",
    "Location of FIMO FASTA-GET-MARKOV BACKGROUND REFERENCE FILE\n",
    "------------------------------------------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate an absolute path to the location of the fasta file you will use as your background reference\n",
    "(on which FASTA-GET-MARKOV will operate to generate a markov background file).\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "    \n",
    "When you're done entering the location of the reference sequence, press Enter.\"\"\"+'\\n')\n",
    "    markov_background_file = input(r\"\"\"    -----> Path to background reference file:  \"\"\")\n",
    "    # 11-Specify transcription factor (TF) of interest, for which to search for lost TFBS occurrences in alleles.\n",
    "    print(r\"\"\"\n",
    "------------------------------------------------\n",
    "TRANSCRIPTION FACTOR (TF) of interest (optional)\n",
    "------------------------------------------------\n",
    "\n",
    "This script collates lost and gained TFBS for sample-associated allele(s) relative to a reference sequence;\n",
    "if detailed analysis of alleles that have lost TFBS matches for a specific transcription factor (TF) are desired,\n",
    "the identity of an individual TF of interest can be provided (optional).\n",
    "\n",
    "If you would like the script to further analyze alleles for TFBS matches to a specific TF, please indicate the\n",
    "TF here.  Otherwise, press Enter. \n",
    "\n",
    "Important: Use only the standardized Entrez gene name for the TF of interest (such as NR3C1), rather than the\n",
    "matrix model stable ID (for example, MA0113 for NR3C1) or stable ID with version number (for example, MA0113.3\n",
    "for NR3C1).\n",
    "\n",
    "Example: if you are interested in losses of TFBS for the TF NR3C1, you would type 'NR3C1'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    TF_of_interest = input(r\"\"\"    -----> Transcription Factor (TF) of interest:  \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 'convert_bytes' and 'path_size' functions to be used in data collection for script_metrics.txt        \n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    This function converts bytes to convenient order of magnitude prefixes\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "        \n",
    "def path_size(given_path):\n",
    "    \"\"\"\n",
    "    This function returns file or directory size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(given_path):\n",
    "        file_info = os.stat(given_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "    elif os.path.isdir(given_path):\n",
    "        dir_info = os.stat(given_path)\n",
    "        return convert_bytes(dir_info.st_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*'merge' and 'merge1' define functions that merge R1 & R2 (reverse complement), or append if they do not overlap; nt_dict is called upon to reverse complement R2* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 'merge' function to merge R1 & R2 reads\n",
    "def merge(s1, s2):\n",
    "    i = 0\n",
    "    while not s2.startswith(s1[i:]):\n",
    "        i += 1\n",
    "    if i < len(s2):\n",
    "        return s1[:i] + s2\n",
    "    else:\n",
    "        return 'no overlap'\n",
    "    \n",
    "# Define 'merge1' function to append two strings that do not overlap\n",
    "def merge1(s1, s2):\n",
    "    i = 0\n",
    "    while not s2.startswith(s1[i:]):\n",
    "        i += 1\n",
    "    return s1[:i] + s2\n",
    "\n",
    "# Define nt complement dictionary      \n",
    "nt_dict = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N', '-':'-'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II. Define user-specified variables\n",
    "\n",
    "A user defines input variables by entering individual lines of text at the Jupyter interface.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specify whether user input is provided at individual coached prompts or as single-list entry\n",
    "print(r\"\"\"\n",
    "---------------------------------------------------------------------\n",
    "User-specified input: choice of coached prompts vs. single list entry\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Values for the user-specified input indicated above can be entered at individually coached command-line prompts\n",
    "(default), or as a single list of variables provided in a single command-line entry without coached prompts.\n",
    "\n",
    "To proceed with input at individual command-line PROMPTS, type 'Prompt' and press Enter;\n",
    "To proceed with input provided as a single LIST in one command-line entry, type 'List' and press Enter:  \n",
    "    \"\"\")\n",
    "user_input = input(r\"\"\"    -----> List or Prompt: \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if user_input == 'Prompt':\n",
    "    prompts()\n",
    "elif user_input == 'List':\n",
    "    print(\"\"\"\n",
    "----------------------------------\n",
    "User-specified input (list format)\n",
    "----------------------------------\n",
    "    \n",
    "Please paste input values directly at the interpreter prompt, specifying the following 10 or 11 values:\n",
    "\n",
    "    1-Location of OUTPUT DIRECTORY for output files\n",
    "    2-Location of INPUT FILES (directory containing fastq files)\n",
    "    3-Location of REFERENCE FASTA FILE\n",
    "    4-Location of BLASTN EXECUTABLE\n",
    "    5-Location of MAKEBLASTDB EXECUTABLE\n",
    "    6-Prefix to assign to BLASTN sequence database files\n",
    "    7-Location of FIMO EXECUTABLE\n",
    "    8-Location of POSITION FREQUENCY MATRIX FILE\n",
    "    9-Location of FASTA-GET-MARKOV EXECUTABLE\n",
    "    10-Location of MARKOV BACKGROUND FILE\n",
    "    11-Identity of TRANSCRIPTION FACTOR (TF) of interest (optional)\n",
    "\n",
    "** Input the values in the specified order in a single line of text\n",
    "** Separate each value by a single semicolon (';')\n",
    "    \n",
    "For example (if specifying TF of interest, 11 values):\n",
    "\n",
    "/Users/myname/CollatedMotifsOutput; /Users/myname/fastq_files; Users/myname/ref_fasta.fa; /Users/myname/bin/blastn;\n",
    "/Users/myname/bin/makeblastdb; ref_name; /Users/myname/Meme/bin/fimo;\n",
    "/Users/myname/JASPAR_CORE_2016_vertebrates.meme; /Users/myname/Meme/bin/fasta-get-markov; /Users/myname/hg38.fa;\n",
    "NR3C1\n",
    "\n",
    "For example (if no specification for TF of interest, 10 values):\n",
    "\n",
    "/Users/myname/CollatedMotifsOutput; /Users/myname/fastq_files; Users/myname/ref_fasta.fa; /Users/myname/bin/blastn;\n",
    "/Users/myname/bin/makeblastdb; ref_name; /Users/myname/Meme/bin/fimo;\n",
    "/Users/myname/JASPAR_CORE_2016_vertebrates.meme; /Users/myname/Meme/bin/fasta-get-markov; /Users/myname/hg38.fa\n",
    "\n",
    "Press 'Enter' to complete.\n",
    "    \n",
    "\"\"\")\n",
    "    input_list = []\n",
    "    input_str_temp = input()\n",
    "    input_str = input_str_temp.strip()\n",
    "    for x in input_str.split(';'):\n",
    "        input_list.append(x.strip())\n",
    "    output_directory = input_list[0].strip()\n",
    "    fastq_directory = input_list[1].strip()\n",
    "    fasta_ref = input_list[2].strip()\n",
    "    blastn_path = input_list[3].strip()\n",
    "    makeblastdb_path = input_list[4].strip()\n",
    "    db_prefix = input_list[5].strip()\n",
    "    fimo_path = input_list[6].strip()\n",
    "    fimo_motifs_path = input_list[7].strip()\n",
    "    fasta_get_markov_path = input_list[8].strip()\n",
    "    markov_background_file = input_list[9].strip()\n",
    "    if len(input_list) == 11:\n",
    "        TF_of_interest = input_list[10].strip()\n",
    "    else:\n",
    "        TF_of_interest = ''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Convert directory and executable strings to operating system-appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wait to create the directories and files until after input has been reviewed and accepted.\n",
    "# Convert fastq_directory input to operating system-appropriate filepath.\n",
    "output_directory = Path(str(output_directory))\n",
    "# Convert fastq_directory input to operating system-appropriate filepath.\n",
    "fastq_directory = Path(str(fastq_directory))\n",
    "# Convert fasta_ref input to operating system-appropriate filepath.\n",
    "fasta_ref = Path(str(fasta_ref))\n",
    "# Convert blastn_path input to operating system-appropriate filepath.\n",
    "blastn_path = Path(str(blastn_path))\n",
    "# Convert makeblastdb_path input to operating system-appropriate filepath.\n",
    "makeblastdb_path = Path(str(makeblastdb_path))\n",
    "# Convert fimo_path input to operating system-appropriate filepath.\n",
    "fimo_path = Path(str(fimo_path))\n",
    "# Convert fimo_motifs_path input to operating system-appropriate filepath.\n",
    "fimo_motifs_path = Path(str(fimo_motifs_path))\n",
    "# Convert fasta_get_markov_path input to operating system-appropriate filepath.\n",
    "fasta_get_markov_path = Path(str(fasta_get_markov_path))\n",
    "# Convert markov_background_file input to operating system-appropriate filepath.\n",
    "markov_background_file = Path(str(markov_background_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Collect fastq files from directory; sort alphanumerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "myFastqFilenames = [file for file in glob.glob(str(fastq_directory)+'/*') if Path(file).suffix in [\".gz\",\".fastq\"]]\n",
    "\n",
    "#Sort fastq file names\n",
    "myFastqFilenames = sorted(myFastqFilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Print fastq file names, to double-check file inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for file in myFastqFilenames:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Collect overview of fastq file contents:  \n",
    "<ul>\n",
    "  <li>Illumina runID</li>   \n",
    "  <li>read count in each fastq file</li>    \n",
    "  <li>file size</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect Illumina run IDs from fastq files, consolidate to unique run IDs\n",
    "runIDlist = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:\n",
    "            runID = \":\".join(f.readline().split(\":\",-2)[:2])\n",
    "            if not runID in runIDlist:\n",
    "                runIDlist.append(runID) \n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        with open(sourcefile, \"r\") as f:\n",
    "            runID = \":\".join(f.readline().split(\":\",-2)[:2])\n",
    "            if not runID in runIDlist:\n",
    "                runIDlist.append(runID)\n",
    "\n",
    "# Collect total read counts for fastq files\n",
    "readcount = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:    \n",
    "            readcount.append(int(len((f).readlines())/4))\n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        with open(sourcefile, \"r\") as f:\n",
    "            readcount.append(int(len((f).readlines())/4))\n",
    "        \n",
    "# Collect file sizes for fastq files\n",
    "filesize = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:\n",
    "            filesize.append(round((os.path.getsize(sourcefile)/1048576),5))\n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        filesize.append(round((os.path.getsize(sourcefile)/1048576),5))\n",
    "\n",
    "# fastq_overview prepares summation of fastq file names, their sizes, and read counts, to be reported in script_metrics.txt    \n",
    "fastq_overview = list(zip(myFastqFilenames, filesize, readcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Double-check whether user-specified entries look good. If a variable is inaccurately assigned, prompt user to restart kernel to begin again.\n",
    "\n",
    "Retrieve and/or calculate the following properties across the fastq files to be processed (these values will be reported in script_metrics.txt):  \n",
    "<ul>\n",
    "  <li>Illumina sequencing run ID(s)</li>\n",
    "  <li>Total number of fastq files</li>\n",
    "  <li>Total number of sequencing reads</li>\n",
    "  <li>Size distribution of fastq files</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Double-check whether entries look good:\n",
    "print(\"\"\"\n",
    "---------------------------------------------------------------\n",
    "Preparation for output:\n",
    "Please double-check that your inputs were recorded as expected.\n",
    "---------------------------------------------------------------\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "Your OUTPUT DIRECTORY was recorded as:\n",
    "\"\"\")\n",
    "print(str(output_directory))\n",
    "\n",
    "print(\"\"\"\n",
    "Your directory containing fastq INPUT FILES was recorded as:\n",
    "\"\"\")\n",
    "print(str(fastq_directory))\n",
    "\n",
    "print(\"\"\"The following data were collected:  \"\"\")\n",
    "print(\"    Illumina sequencing run ID(s): \")\n",
    "for i in runIDlist:\n",
    "    print('        '+i)\n",
    "\n",
    "print(\"    # of fastq files to process: {0}\".format(len(myFastqFilenames)))\n",
    "\n",
    "print(\"    size distribution of fastq files to process: \\n      total... \"+str(round((sum(file for file in filesize))))+' MB \\n      range... max: '+str(round((max(file for file in filesize)),2))+' MB; min: '+str(round((min(file for file in filesize)),5))+' MB; median: '+str(round((numpy.median([file for file in filesize])),3))+' MB; mean +/- stdev: '+str(round((numpy.mean([file for file in filesize])),3))+' +/- '+str(round((numpy.std([file for file in filesize])),3))+' MB')\n",
    "\n",
    "print(\"    read distribution within fastq files to process: \\n      total... \"+locale.format_string(\"%d\", sum(readcount), grouping=True)+' reads \\n      range... max: '+str((max(file for file in readcount)))+' reads; min: '+str((min(file for file in readcount)))+' reads; median: '+str((numpy.median([file for file in readcount])))+' reads; mean +/- stdev: '+str(round((numpy.mean([file for file in readcount]))))+' +/- '+str(round((numpy.std([file for file in readcount]))))+' reads')\n",
    "\n",
    "print(\"\"\"\n",
    "Your FASTA REFERENCE FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(str(fasta_ref))\n",
    "\n",
    "print(\"\"\"\n",
    "Your BLASTN EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(str(blastn_path))\n",
    "\n",
    "print(\"\"\"\n",
    "Your MAKEBLASTDB EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(makeblastdb_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your BLASTN DATABASE FILE PREFIX was recorded as:\n",
    "\"\"\")\n",
    "print(db_prefix)\n",
    "\n",
    "print(\"\"\"\n",
    "Your FIMO EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(fimo_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your POSITION FREQUENCY FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(fimo_motifs_path)\n",
    "\n",
    "# Examine the reference file and indicate the ID, number of motifs, etc. print out list of factors for query\n",
    "motifcountlist = []\n",
    "with open(fimo_motifs_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if bool(re.search('MOTIF', line)): \n",
    "            motifcountlist.append(line.strip())\n",
    "\n",
    "print(\"\"\"\n",
    "# of TFBS motifs examined: \"\"\"+str(len(motifcountlist)))\n",
    "\n",
    "motifID = [i.split(' ')[2] for i in motifcountlist]\n",
    "motifID = sorted(motifID)\n",
    "chunked_motifID = [motifID[i: i+8] for i in range(0, len(motifID), 8)]\n",
    "\n",
    "print('Identities of TFBS motifs examined: ')\n",
    "\n",
    "for row in chunked_motifID:\n",
    "    itemnumber = (len(row)*'{: ^13} ').rstrip()\n",
    "    print(itemnumber.format(*row))\n",
    "\n",
    "print(\"\"\"\n",
    "Your FASTA-GET-MARKOV EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(fasta_get_markov_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your MARKOV BACKGROUND FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(markov_background_file)\n",
    "\n",
    "print(\"\"\"\n",
    "Your TF of interest was recorded as:\n",
    "\"\"\")\n",
    "if TF_of_interest != '':\n",
    "    print(TF_of_interest)\n",
    "else:\n",
    "    print('No TF of interest was provided')\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
    "* Please check that this list is accurately recorded.                                         *\n",
    "*                                                                                             *\n",
    "* If you have corrections to make, please return to the appropriate cell to reset variables.  *\n",
    "* To continue in the script, move to the next cell.                                           *\n",
    "* To restart the script, click on the menu 'Kernel -> Restart'.                               *\n",
    "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Generate output directory and files, ready for script output  \n",
    "Script generates a single directory, populated with 5 files ready to accept script output (6th file, Excel workbook, is generated during later script operations).  \n",
    "Files are automatically named as in **'Output notes'** above, with current date appended to filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Start the clock on script operation duration\n",
    "startTime = datetime.now()\n",
    "startTimestr = str(startTime).split(' ')[1].split('.')[0]\n",
    "\n",
    "# Proceed to file processing\n",
    "# Generate the directory and its files (to accept content later in script)\n",
    "path = str(output_directory)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "output_path = Path(output_directory)\n",
    "\n",
    "# Create output files\n",
    "filename_list = ['fasta.fa', 'blastn_alignments.txt', 'collated_TFBS.txt', 'markov_background.txt', 'script_metrics.txt']\n",
    "\n",
    "# Define current date as prefix to all filenames\n",
    "processdate = datetime.today().strftime(\"%m%d%Y\")\n",
    "\n",
    "for filename in filename_list:\n",
    "    with open(os.path.join(path, processdate+'_'+filename), 'wb') as file:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The file **script_metrics.txt** records script operation metadata (summarizes script input and performance); peform initial log of system information, user-defined variables and fastq file properties to script_metrics.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Collect RAM info\n",
    "mem = virtual_memory()\n",
    "ramem = mem.total/1073741824\n",
    "\n",
    "# Use print redirection to write to target file, in append mode (begin script_metrics.txt)\n",
    "with open(fasta_ref, \"r\") as f:\n",
    "    ref_seqs = f.readlines()\n",
    "    \n",
    "filename = Path(str(output_path)+'/'+processdate+'_script_metrics.txt')\n",
    "with open(filename, 'a') as f:\n",
    "    print(\"\"\"CollatedMotifs.py: Script Metrics\n",
    "Date: \"\"\" + (datetime.today().strftime(\"%m/%d/%Y\")) +\n",
    "\"\"\"\\n\\nOperating system information:\n",
    "    name: \"\"\" + socket.gethostname() +\n",
    "'\\n    platform: ' + platform.platform() +\n",
    "'\\n    RAM (GB): ' + str(ramem) +\n",
    "'\\n    physical CPU/effective CPU: ' + str(psutil.cpu_count(logical=False)) +'/'+ str(psutil.cpu_count()) +\n",
    "'\\n    executable: ' + psutil.Process().exe() +\n",
    "\"\"\"\\n\\nUser-entered variables:\n",
    "    output_directory: \"\"\"+ str(output_directory) +\n",
    "\"\\n    fastq_directory: \"+ str(fastq_directory) +\n",
    "\"\\n    fasta_ref: \"+ str(fasta_ref) +\n",
    "\"\\n    blastn_path: \"+ str(blastn_path) +\n",
    "\"\\n    makeblastdb_path: \"+ str(makeblastdb_path) +\n",
    "\"\\n    db_prefix: \"+ str(db_prefix) +\n",
    "\"\\n    fimo_path: \"+ str(fimo_path) +\n",
    "\"\\n    fimo_motifs_path: \"+ str(fimo_motifs_path) +\n",
    "\"\\n    fasta_get_markov_path: \"+ str(fasta_get_markov_path) +\n",
    "\"\\n    markov_background_file: \"+ str(markov_background_file) +\n",
    "\"\\n    TF_of_interest: \"+ TF_of_interest +\n",
    "\"\"\"\\n\\nfastq file information:\n",
    "    Illumina sequencing run ID(s): \"\"\"+ str(runIDlist).strip('[]').replace(\"'\",\"\") +\n",
    "\"\\n    Number of fastq files processed: \"+ str(len(myFastqFilenames)) +\n",
    "\"\"\"\\n    Size distribution of fastq files processed: \n",
    "        total... \"\"\" +str(round((sum(file for file in filesize))))+' MB \\n        range... max: '+str(round((max(file for file in filesize)),2))+' MB; min: '+str(round((min(file for file in filesize)),5))+' MB; median: '+str(round((numpy.median([file for file in filesize])),3))+' MB; mean +/- stdev: '+str(round((numpy.mean([file for file in filesize])),3))+' +/- '+str(round((numpy.std([file for file in filesize])),3))+' MB' +\n",
    "\"\\n    Read distribution within fastq files to process: \\n        total... \"+locale.format_string(\"%d\", sum(readcount), grouping=True)+' reads \\n        range... max: '+str((max(file for file in readcount)))+' reads; min: '+str((min(file for file in readcount)))+' reads; median: '+str((numpy.median([file for file in readcount])))+' reads; mean +/- stdev: '+str(round((numpy.mean([file for file in readcount]))))+' +/- '+str(round((numpy.std([file for file in readcount]))))+' reads', file = f)\n",
    "    print(\"\\nfastq files processed (name, size (MB), reads): \", file = f)\n",
    "    for i in (sorted(fastq_overview)):\n",
    "        print(\"    \" + str(i).strip(\"()\").replace(\"'\",\"\"), file = f)\n",
    "    print(\"\\nReference sequences provided in fasta_ref file: \", file = f)\n",
    "    for i in ref_seqs:\n",
    "        print(\"    \" + i.strip('\\n'), file = f)     \n",
    "    print(\"\\n# of TFBS motifs examined: \"+str(len(motifcountlist))+\n",
    "\"\\nIdentities of TFBS motifs examined: \", file = f)\n",
    "    for row in chunked_motifID:\n",
    "        itemnumber = (len(row)*'{: ^13} ').rstrip()\n",
    "        print(itemnumber.format(*row), file = f)        \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Create accessory files for BLASTN and FIMO operations:  \n",
    "- **MAKEBLASTDB** (NCBI) will now be used to prepare a reference sequence database for BLASTN alignments.  \n",
    "*The output of this operation is a set of 6 database files in alignments_directory.*  \n",
    "\n",
    "\n",
    "- **FASTA-GET-MARKOV** (MEME) will then be used to prepare a background markov file for FIMO statistical operations.  \n",
    "*The output of this operation is a single file, markov_background.txt, supplied to FIMO with sample and reference fasta files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on makeblastdb and fastgetmarkov operations\n",
    "startTime_makeblastdb_fastagetmarkov_operations = datetime.now()\n",
    "\n",
    "# Construct alignment database, in alignment_database directory\n",
    "# Reference sequence input\n",
    "mydb_input = Path(fasta_ref)\n",
    "\n",
    "# Alignment database directory\n",
    "mydb_output = Path(str(output_directory)+'/alignment_database')\n",
    "\n",
    "os.makedirs(mydb_output)\n",
    "\n",
    "# 'Make blastn database' command (usage: makeblastdb -in mydb.fsa -parse_seqids -dbtype nucl -out path)\n",
    "cmd_makeblastndb = str(makeblastdb_path)+' -in '+str(mydb_input)+' -parse_seqids -dbtype nucl -out '+str(mydb_output)+'/'+db_prefix\n",
    "\n",
    "os.system(cmd_makeblastndb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Construct background markov file to be used by FIMO\n",
    "# Markov input file (markov background file)\n",
    "markovbackground_input = Path(markov_background_file)\n",
    "\n",
    "# Markov background output file  \n",
    "markovbackground_output = Path(str(output_directory)+'/'+processdate+'_markov_background.txt')\n",
    "\n",
    "# 'Make markov background file' command (usage: fasta-get-markov [options] [<sequence file> [<background file>]])\n",
    "cmd_fastagetmarkov = str(fasta_get_markov_path)+' -dna '+str(markovbackground_input)+' '+str(markovbackground_output)\n",
    "\n",
    "os.system(cmd_fastagetmarkov)\n",
    "\n",
    "# Log makeblastdb and fastgetmarkov operations time duration\n",
    "makeblastdb_fastagetmarkov_operationsDuration = str(datetime.now()- startTime_makeblastdb_fastagetmarkov_operations).split(':')[0]+' hr|'+str(datetime.now() - startTime_makeblastdb_fastagetmarkov_operations).split(':')[1]+' min|'+str(datetime.now() - startTime_makeblastdb_fastagetmarkov_operations).split(':')[2].split('.')[0]+' sec|'+str(datetime.now()- startTime_makeblastdb_fastagetmarkov_operations).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Identify candidate alleles: fasta file, BLASTN alignment, and assignment of alleles to samples  \n",
    "Deep sequencing of amplicons can yield hundreds to thousands of reads per sample; read frequencies can be used to gauge relative read abundance and, ultimately, to infer probable genotype (sequence ID(s) of the source template(s)).\n",
    "<img src=\"CollatedMotifs_img/fasta_thumbnail.png\" align=\"left\" width=\"560\">  \n",
    "\n",
    "**Count reads.** This script parses sample-specific fastq files for unique read types, counts the abundance of these read types, and reports the top 5 most abundant read types in the form of fasta entries. For each sample, **each of the 5 ranked sequences is reported with its frequency metrics** in a corresponding fasta definition line (defline).  \n",
    "The output of this step is a fasta file (.fa) that will be created in the user-specified OUTPUT DIRECTORY.  \n",
    "\n",
    "*(rationale for top 5 ranked sequences: 5 ranked reads facilitate user interpretation of genotype, because homozygous or heterozygous genotypes exhibit top-ranked allele(s) (one if homozygous, two if heterozygous) with frequency(ies) substantially higher than the remaining ranked reads (which may then be inferred as PCR and/or sequencing artefacts); alternatively, if sample is multiploid and/or otherwise heterogenous, the 5 ranked sequences sample into the underlying diversity)*   \n",
    "\n",
    "**Align reads to reference.** This fasta file is then presented to **BLASTN** (with the reference sequence database specified during user input) for alignments.\n",
    "\n",
    "**Define candidate alleles.** The script then parses the alignments to organize alignment data for the 'top 5' reads assigned to each sample, in a single dictionary called **'alignmentoutput_dict'**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on read count duration      \n",
    "startTime_readcount = datetime.now()\n",
    "\n",
    "# Populate fasta files for fasta.fa, in preparation for fimo analysis      \n",
    "query_input = Path(str(output_directory)+'/'+processdate+'_fasta.fa')\n",
    "\n",
    "# define Nextera adaptor sequence, in preparation to trim read 3' ends if necessary\n",
    "adaptor_str = 'CTGTCTCTTATACACATCT'\n",
    "adaptor_str_rev = 'AGATGTGTATAAGAGACAG'\n",
    "\n",
    "# Merge R1 and R2 reads, if present, as single sequence\n",
    "R1_file_list = [sourcefile for sourcefile in myFastqFilenames if bool(re.split('_',os.path.basename(sourcefile))[3] == 'R1')]\n",
    "R2_file_list = [sourcefile for sourcefile in myFastqFilenames if bool(re.split('_',os.path.basename(sourcefile))[3] == 'R2')]  \n",
    "\n",
    "# R1, R2 cluster mapping\n",
    "processed_files_list = []\n",
    "R1_R2_map_list = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if sourcefile in processed_files_list:\n",
    "        pass\n",
    "    else:\n",
    "        testname = ''.join(re.split('_',os.path.basename(sourcefile))[0:3])\n",
    "        for sourcefile1 in R1_file_list:\n",
    "            if testname == ''.join(re.split('_',os.path.basename(sourcefile1))[0:3]):\n",
    "                R1 = sourcefile\n",
    "                if sourcefile not in processed_files_list:\n",
    "                    processed_files_list.append(sourcefile)\n",
    "        for sourcefile2 in R2_file_list:\n",
    "            if testname == ''.join(re.split('_',os.path.basename(sourcefile2))[0:3]):\n",
    "                R2 = sourcefile2\n",
    "                if sourcefile2 not in processed_files_list:\n",
    "                    processed_files_list.append(sourcefile2)\n",
    "        R1_R2_map_list.append((R1, R2))\n",
    "        \n",
    "# Make fasta file of read entries, direct top read count output and annotation to fasta.fa   \n",
    "for file_pair in R1_R2_map_list:\n",
    "    R1_file = file_pair[0]\n",
    "    R2_file = file_pair[1]\n",
    "    fastaname = re.split('_', os.path.basename(R1_file))\n",
    "    cluster_sequence_R1_dict = {}\n",
    "    cluster_sequence_R2_dict = {}\n",
    "    cluster_sequence_R2_revcomp_dict = {}\n",
    "    cluster_merged_R1_R2revcomp_dict = {}\n",
    "    cluster_merged_R1_R2revcomp_dict2 = {}\n",
    "    merged_read_list = []\n",
    "    counter=()\n",
    "    if Path(R1_file).suffix == \".gz\":\n",
    "        with gzip.open(R1_file, \"rt\") as f:\n",
    "            lines_R1 = f.readlines()\n",
    "    elif Path(R1_file).suffix == \".fastq\":\n",
    "        with open(R1_file, 'r') as f:\n",
    "            lines_R1 = f.readlines()    \n",
    "    for x in range(0,len(lines_R1),4):\n",
    "        # trim adaptor sequence and up to 3' end of read from R1 sequence, if adaptor sequence found\n",
    "        cluster_sequence_R1_dict[lines_R1[x].split(':')[5]+':'+lines_R1[x].split(':')[6].split(' ')[0]] = lines_R1[x+1].strip('\\n')[:lines_R1[x+1].strip('\\n').index(adaptor_str)] if adaptor_str in lines_R1[x+1].strip('\\n') else lines_R1[x+1].strip('\\n') \n",
    "    #cluster_IDs_list_R1 = [x.split(':')[5]+':'+x.split(':')[6].split(' ')[0] for x in lines_R1[0::4]]\n",
    "    if Path(R2_file).suffix == \".gz\":\n",
    "        with gzip.open(R2_file, \"rt\") as f:\n",
    "            lines_R2 = f.readlines()\n",
    "    elif Path(R2_file).suffix == \".fastq\":\n",
    "        with open(R2_file, 'r') as f:\n",
    "            lines_R2 = f.readlines()\n",
    "    for x in range(0,len(lines_R2),4):\n",
    "        # trim adaptor sequence and up to 3' end of read from R2 sequence, if adaptor sequence found\n",
    "        cluster_sequence_R2_dict[lines_R2[x].split(':')[5]+':'+lines_R2[x].split(':')[6].split(' ')[0]] = lines_R2[x+1].strip('\\n')[:lines_R2[x+1].strip('\\n').index(adaptor_str)] if adaptor_str in lines_R2[x+1].strip('\\n') else lines_R2[x+1].strip('\\n') \n",
    "    #cluster_IDs_list_R2 = [x.split(':')[5]+':'+x.split(':')[6].split(' ')[0] for x in lines_R2[0::4]]\n",
    "    for cluster in cluster_sequence_R2_dict:\n",
    "        cluster_sequence_R2_revcomp_dict[cluster] = ''.join(reversed(''.join(nt_dict.get(nt) for nt in cluster_sequence_R2_dict.get(cluster))))\n",
    "    for cluster in cluster_sequence_R1_dict:\n",
    "        if cluster in cluster_sequence_R2_revcomp_dict:\n",
    "            if merge(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster)) != 'no overlap':\n",
    "                cluster_merged_R1_R2revcomp_dict[cluster] = merge(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster))\n",
    "            else:\n",
    "                cluster_merged_R1_R2revcomp_dict2[cluster] = merge1(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster))\n",
    "    for cluster in cluster_merged_R1_R2revcomp_dict:\n",
    "        merged_read_list.append(cluster_merged_R1_R2revcomp_dict.get(cluster))\n",
    "    # create dictionary (counter) relating unique read sequence to its # of occurrences\n",
    "    counter=Counter(merged_read_list)\n",
    "    modified_read_list_top5 = []\n",
    "    for index, i in enumerate(counter.most_common(5)):\n",
    "        filtered1 = sum([x for x in counter.values() if x/(sum(counter.values())) > 0.01])\n",
    "        filtered10 = sum([x for x in counter.values() if x/(sum(counter.values())) > 0.1])\n",
    "        raw_freq = round((100*i[1]/sum(counter.values())),2)\n",
    "        modified_read_list_top5.append([i[0], '['+str(i[1])+'/'+str(sum(counter.values()))+']', 'rank'+str(index+1), raw_freq, int(stats.percentileofscore([i for i in counter.values()], i[1], 'rank')), round((100*i[1]/sum([i[1] for i in counter.most_common(5)])),2), round((100*i[1]/filtered1),2) if filtered1 > 0 and raw_freq >= 1 else 'None', round((100*i[1]/filtered10),2) if filtered10 > 0 and raw_freq >= 10 else 'None'])\n",
    "    with open(str(query_input), 'a+') as file:\n",
    "        for i in modified_read_list_top5:\n",
    "              file.write('>'+fastaname[0]+'_'+'R1+R2'+'_'+str(i[1])+'_'+i[2]+'_%totalreads:'+str(i[3])+'_percentile:'+str(i[4])+'_%top5reads:'+str(i[5])+'_%readsfilteredfor1%:'+str(i[6])+'_%readsfilteredfor10%:'+str(i[7])+'\\n'+i[0]+'\\n')\n",
    "                \n",
    "# Log read count time duration      \n",
    "readcountDuration = str(datetime.now()- startTime_readcount).split(':')[0]+' hr|'+str(datetime.now() - startTime_readcount).split(':')[1]+' min|'+str(datetime.now() - startTime_readcount).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_readcount).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process alignments to reference sequence database, using **BLASTN** (NCBI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on blastn alignments duration  \n",
    "startTime_alignments = datetime.now()\n",
    "\n",
    "# Process alignments relative to reference sequence database, using blastn\n",
    "# Reference database\n",
    "db_input = mydb_output / db_prefix\n",
    "\n",
    "# Alignment output\n",
    "query_output = str(output_directory)+'/'+processdate+'_blastn_alignments.txt'\n",
    "\n",
    "# Alignment command\n",
    "cmd_align = str(blastn_path)+' -strand plus -query '+str(query_input)+' -db '+str(db_input)+' -out '+str(query_output)+' -gapopen 1 -gapextend 1 -outfmt \"5\"'\n",
    "\n",
    "os.system(cmd_align)\n",
    "\n",
    "# Log alignment time duration\n",
    "alignmentsDuration = str(datetime.now()- startTime_alignments).split(':')[0]+' hr|'+str(datetime.now()- startTime_alignments).split(':')[1]+' min|'+str(datetime.now()- startTime_alignments).split(':')[2].split('.')[0]+' sec|'+str(datetime.now()- startTime_alignments).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Define alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on allele definitions duration      \n",
    "startTime_alleles = datetime.now()\n",
    " \n",
    "# Import blastn alignments output as a list of strings (each string corresponds to a query alignment)      \n",
    "alignments_list = []\n",
    "with open(str(query_output), 'r') as file:\n",
    "    reader = file.read()\n",
    "    for i,part in enumerate(reader.split('<Iteration_iter-num>')):\n",
    "        alignments_list.append(part)\n",
    "# Remove blastn header line from alignments_list\n",
    "alignments_list = alignments_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert alignments_list to list of lists (i.e., each query alignment string is encapsulateed into its own sublist within alignments_list2)\n",
    "alignments_list2 = [alignments_list[i:i+1] for i in range(0, len(alignments_list))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Subset sample IDs and/or associated reads for which *(1) no alignment* was found in reference database, or *(2) multiple hits* were identified in reference database. These are ultimately removed from further analysis, but the identities of samples and/or associated reads that were filtered by these criteria are ultimately reported in 'population_summary.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify & subset queries for which no alignments were found in reference database ('no hits found')\n",
    "no_hits_list = []\n",
    "for i in alignments_list2:\n",
    "    if re.search('No hits found', str(i)):\n",
    "        no_hits_list.append(str(i).split('<Iteration_query-def>')[1].split('</Iteration_query-def>')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Record sample names having reads with no alignment hits      \n",
    "no_hits_samplename_list = []\n",
    "for i in no_hits_list:\n",
    "    samplename = i.split('_')[0]\n",
    "    if samplename not in no_hits_samplename_list:\n",
    "        no_hits_samplename_list.append(samplename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Within each sublist of alignments_list2, split each line into an individual string, remove beginning and trailing whitespace, and recapture specified subset of alignment information in alignments_list3\n",
    "alignments_list3 = []\n",
    "for i in alignments_list2:\n",
    "    if str(i).split('<Iteration_query-def>')[1].split('</Iteration_query-def>')[0] not in no_hits_list:\n",
    "        alignments_list3.append([y.strip() for x in i for y in x.split('\\n') if y.strip().startswith(('<Iteration_query-ID>', '<Iteration_query-def>', '<Hit_num>', '<Hit_id>', '<Hit_def>', '<Hsp_hit-from>', '<Hsp_hit-to>', '<Hsp_qseq>', '<Hsp_hseq>', '<Hsp_midline>'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify & subset reads with >1 alignment to sequences in reference database\n",
    "# Some reads with >1 alignment will be recovered to 'reconstitute' hypothesized allele (if BLASTN has split the read into multiple 'hits' or 'high-scoring pairs' (hsp's) within the span of the user-provided reference sequence)\n",
    "\n",
    "# There are in principle at least 3 ways a read could potentially align to >1 position in reference database (1 & 2a,b below):\n",
    "# (1) same sequence span aligns to >1 different locus (disparate <Hit_id>'s)\n",
    "# [unlike in Genotypes.py, this scenario is not anticipated in CollatedMotifs.py, unless significant sequence overlap occurs in user-provided fasta file containing reference sequence(s)]\n",
    "# (2) one sequence span may be split into two (ore more) different alignment matches, because of intervening gap(s) or insertion(s) that exceed ~60 bp (an apparent BLASTN gap limit)\n",
    "#    (a) if the two (or more) 'split matches' align to the same <Hit_id>, but to different coordinates of that <Hit_id>, they will be presented by BLASTN as belonging to the same <Hit_num>, but to different <Hsp_num> (Hsp=high scoring pair)\n",
    "#    (b) if the two (or more) 'split matches' span different <Hit_id>'s (essentially different 'chunks' of sequence with unique names, as organized within the alignment database), they will be presented by BLASTN as belonging to different <Hit_num>\n",
    "# These observations suggest that it is important to distinguish a read with alignment to >1 sequence as either one with poor resolution among >1 reference sequences (if >1 reference sequence is provided), vs. one that harbors sizeable deletions or insertions relative to the reference sequence\n",
    "# CollatedMotifs.py assumes continuity of 2 or more hsp's if they are assigned to the same user-provided reference sequence,\n",
    "# and therefore attempts to reconstitute hypothesized alleles that span multiple non-overlapping hsp's (but does not attempt to reconstitute across multiple hits or for ambiguous reconstructions from overlapping hsp's)\n",
    "\n",
    "# Organize reads with multiple hit IDs\n",
    "# These reads are deprecated (not further analyzed)\n",
    "multiple_alignments_hits_list = []\n",
    "for i in alignments_list3:\n",
    "    if len(re.findall('<Hit_num>', str(i))) > 1:\n",
    "        multiple_alignments_hits_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dictionary linking sample names to their reads having >1 alignment to sequences in reference database    \n",
    "multiple_alignments_samplename_list = []\n",
    "for i in multiple_alignments_hits_list:\n",
    "    multiple_alignments_samplename_list.append(i[1].split('>')[1].split('_')[0])\n",
    "    \n",
    "multiple_alignments_dict = {}\n",
    "for i in multiple_alignments_samplename_list:\n",
    "    multiple_alignments_dict [\"{0}\".format(i)] = tuple(x for x in multiple_alignments_hits_list if bool(re.search(i, x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize reads with single hit, but multiple associated hsp IDs\n",
    "# These reads will be processed separately to 'reconstitute' potential alleles, with high-scoring alignment pairs matched to the reference sequence, but split into separate matches due to intervening non-aligning span between the alignment matches\n",
    "multiple_alignments_hsp_list = []\n",
    "for i in alignments_list3:\n",
    "    if len(re.findall('<Hit_num>', str(i))) > 1:\n",
    "        pass\n",
    "    elif len(re.findall('<Hsp_hit-from>', str(i))) > 1:\n",
    "        multiple_alignments_hsp_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for multiple hsp's that can be reasonably reconstructed (i.e., >1 hsp's that do not overlap. Overlapping hsp's cannot readily be reconstructed and alleles with overlapping hsp's will be deprecated from analysis)\n",
    "alleles_with_multiple_hsps_that_can_be_reconstructed_list = []\n",
    "for i in alignments_list3:\n",
    "    count = 0\n",
    "    for x in i:\n",
    "        if re.search('<Hsp_hit-from>', x):\n",
    "            count = count+1\n",
    "    if count > 1:\n",
    "        overlapping_hsp = False\n",
    "        index_split_list = []\n",
    "        hsp_list = []\n",
    "        hsp_to_from_list = []\n",
    "        for index, x in enumerate(i):\n",
    "            if re.search('<Hsp_hit-from>', x):\n",
    "                index_split_list.append(index)\n",
    "        for index, y in enumerate(index_split_list):\n",
    "            hsp_list.append(i[y:y+(index+1)*5])\n",
    "        hsp_to_from_list_chunked = []\n",
    "        for index, w in enumerate(range(0,len(hsp_list))):\n",
    "            hsp_to_from_list_chunked.append(hsp_list[index])\n",
    "        for index, hsp in enumerate(hsp_list):\n",
    "            range_to_check = range(int(hsp[0].split('>')[1].split('<')[0]),int(hsp[1].split('>')[1].split('<')[0]))\n",
    "            # ranges of other hsp's in hsp_list\n",
    "            other_hsps_list = hsp_list.copy()\n",
    "            del other_hsps_list[index]\n",
    "            for other_hsp in other_hsps_list:\n",
    "                other_hsp_range = range(int(other_hsp[0].split('>')[1].split('<')[0]),int(other_hsp[1].split('>')[1].split('<')[0]))\n",
    "                if set(range_to_check).intersection(other_hsp_range):\n",
    "                    overlapping_hsp = True\n",
    "                else:\n",
    "                    pass\n",
    "        if overlapping_hsp is False:\n",
    "            alleles_with_multiple_hsps_that_can_be_reconstructed_list.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify alleles with multiple hsp's that were not able to be reconstructed; note these later in script_metrics.txt\n",
    "alleles_with_multiple_hsps_that_cannot_be_reconstructed_list = []\n",
    "for i in multiple_alignments_hsp_list:\n",
    "    if i not in alleles_with_multiple_hsps_that_can_be_reconstructed_list:\n",
    "        alleles_with_multiple_hsps_that_cannot_be_reconstructed_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to reconstruct long-ranging alignment for alleles with >1 BLASTN hsp that can be reasonably reconstituted\n",
    "reconstructed_alleles_with_multiple_hsps_list = []\n",
    "\n",
    "with open(mydb_input) as file:\n",
    "    ref_candidates = file.readlines()\n",
    "ref_candidates2 = [i.strip('>\\n') for i in ref_candidates]\n",
    "iterator = iter(ref_candidates2)\n",
    "ref_candidates3 = list(zip(iterator,iterator))\n",
    "\n",
    "for allele in alleles_with_multiple_hsps_that_can_be_reconstructed_list:\n",
    "    # get allele as it appears intact in fasta file\n",
    "    with open(query_input) as file:\n",
    "        for index, line in enumerate(file):\n",
    "            if line.strip('\\n') == '>'+allele[1].split('>')[1].split('<')[0]:\n",
    "                allele_fasta = next(file).strip()\n",
    "    allele_reconstruction_list = [i for i in allele[0:5]]\n",
    "    allele_reconstruction_temp_list = []\n",
    "    allele_ref = allele[3].split('>')[1].split('<')[0]\n",
    "    for ref in ref_candidates3:\n",
    "        if ref[0] == allele_ref:\n",
    "            allele_ref_sequence = ref[1]\n",
    "        else:\n",
    "            pass\n",
    "    # collect info re: hsp match spans and alignments to reference\n",
    "    for hsp in range(0,int(((len(allele)-5)/5))):\n",
    "        hsp_from = int(allele[5+hsp*5].split('>')[1].split('<')[0])\n",
    "        hsp_to = int(allele[6+hsp*5].split('>')[1].split('<')[0])\n",
    "        hsp_qseq = allele[7+hsp*5].split('>')[1].split('<')[0]\n",
    "        hsp_hseq = allele[8+hsp*5].split('>')[1].split('<')[0]\n",
    "        hsp_midline = allele[9+hsp*5].split('>')[1].split('<')[0]\n",
    "        \n",
    "        if hsp_from == 1:\n",
    "            allele_reconstruction_temp_list.append((hsp_from, str(hsp_from)+':'+str(hsp_to), allele_ref_sequence[0:hsp_to], hsp_qseq, hsp_hseq, hsp_midline))\n",
    "        else:\n",
    "            allele_reconstruction_temp_list.append((hsp_from, str(hsp_from)+':'+str(hsp_to), allele_ref_sequence[hsp_from-1:hsp_to], hsp_qseq, hsp_hseq, hsp_midline))\n",
    "\n",
    "    allele_span_list = []\n",
    "    reference_span_list = []\n",
    "    alignment_midline_list = []\n",
    "    # prepare to account for bp spans from allele read as represented in fasta file, which are represented in hsps\n",
    "    allele_fasta_span_bp_accounting_set = set()\n",
    "    allele_fasta_span_bp = range(1,len(allele_fasta)+1)\n",
    "    allele_fasta_spans_in_hsps = set()\n",
    "    # prepare to account for bp spans from reference as represented in reference file, which are represented in hsps\n",
    "    reference_span_bp_accounting_set = set()\n",
    "    reference_span_bp = range(1,len(allele_ref_sequence)+1)\n",
    "    reference_spans_in_hsps = set()\n",
    "    # assess hsp match positions relative to ref span, and re-order if needed based on relative order of start and stop positions of hsp alignment\n",
    "    for subregion in sorted(allele_reconstruction_temp_list):\n",
    "        reference_spans_in_hsps.update(set(range(int(subregion[1].split(':')[0]),int(subregion[1].split(':')[1]))))\n",
    "        match = re.search(subregion[3].replace('-',''), allele_fasta)\n",
    "        allele_fasta_spans_in_hsps.update(range(match.span()[0],match.span()[1]))\n",
    "        \n",
    "    for index, subregion in enumerate(sorted(allele_reconstruction_temp_list)):\n",
    "        # subregion[0] is start position of hsp alignment match in reference; subregion[1] is string form of hsp coordinate span (start:end) relative to reference\n",
    "        # subregion[2] is direct sequence span from reference sequence; subregion[3] is query from allele; subregion[4] is hit from reference; subregion[5] is midline\n",
    "        if index == 0:\n",
    "            if subregion[0] == 1:\n",
    "                allele_span_list.append(subregion[3])\n",
    "                reference_span_list.append(subregion[4])\n",
    "                alignment_midline_list.append(subregion[5])\n",
    "                reference_span_bp_accounting_set.update(range(1,int(subregion[1].split(':')[1])))\n",
    "                # check for coverage in allele as represented in fasta file\n",
    "                match = re.search(subregion[3].replace('-',''), allele_fasta)\n",
    "                allele_fasta_span_bp_accounting_set.update(range(match.span()[0],match.span()[1]))\n",
    "                \n",
    "            else:\n",
    "                allele_span_list.append(subregion[3])\n",
    "                reference_span_list.append(subregion[4])\n",
    "                alignment_midline_list.append(subregion[5])\n",
    "                reference_span_bp_accounting_set.update(range(int(subregion[1].split(':')[0]),int(subregion[1].split(':')[1])))\n",
    "                # check for coverage in allele as represented in fasta file\n",
    "                match = re.search(subregion[3].replace('-',''), allele_fasta)\n",
    "                allele_fasta_span_bp_accounting_set.update(range(match.span()[0],match.span()[1]))\n",
    "                #allele_span_list.append('-'*subregion[0])\n",
    "                #reference_span_list.append(allele_ref_sequence[0:subregion[0]])\n",
    "                #alignment_midline_list.append(' '*subregion[0])\n",
    "                #reference_span_bp_accounting_set.update(range(1,int(subregion[1].split(':')[1]))) \n",
    "        elif len(sorted(allele_reconstruction_temp_list)) > index > 0:\n",
    "            test_span = range(int(sorted(allele_reconstruction_temp_list)[index-1][1].split(':')[1]), int(subregion[1].split(':')[0]))\n",
    "            \n",
    "            if reference_span_bp_accounting_set.intersection(test_span):\n",
    "                allele_span_list.append(subregion[3])\n",
    "                reference_span_list.append(subregion[4])\n",
    "                alignment_midline_list.append(subregion[5])\n",
    "                reference_span_bp_accounting_set.update(range(int(subregion[1].split(':')[0]),int(subregion[1].split(':')[1])))\n",
    "                match = re.search(subregion[3].replace('-',''), allele_fasta)\n",
    "                allele_fasta_span_bp_accounting_set.update(range(match.span()[0],match.span()[1]))\n",
    "            else:\n",
    "                match = re.search(subregion[3].replace('-',''), allele_fasta)\n",
    "                allele_fasta_span_bp_accounting_set.update(range(match.span()[0],match.span()[1]))\n",
    "                bases_in_fasta_allele_not_accounted_for_in_alignment = sorted(set(range(sorted(allele_fasta_span_bp_accounting_set)[0],sorted(allele_fasta_span_bp_accounting_set)[-1]))-allele_fasta_span_bp_accounting_set)\n",
    "                if len(bases_in_fasta_allele_not_accounted_for_in_alignment) > 0:\n",
    "                    bases_to_add = allele_fasta[bases_in_fasta_allele_not_accounted_for_in_alignment[0]:bases_in_fasta_allele_not_accounted_for_in_alignment[-1]+1]\n",
    "                    allele_fasta_span_bp_accounting_set.update(range(bases_in_fasta_allele_not_accounted_for_in_alignment[0],bases_in_fasta_allele_not_accounted_for_in_alignment[1]))\n",
    "                else:\n",
    "                    bases_to_add = ''\n",
    "                allele_span_list.append(bases_to_add)\n",
    "                allele_span_list.append('-'*(int(subregion[0])-int(sorted(allele_reconstruction_temp_list)[index-1][1].split(':')[1])-1-len(bases_to_add)))\n",
    "                reference_span_list.append(allele_ref_sequence[int(sorted(allele_reconstruction_temp_list)[index-1][1].split(':')[1]):int(subregion[0])-1])\n",
    "                alignment_midline_list.append(' '*(int(subregion[0])-int(sorted(allele_reconstruction_temp_list)[index-1][1].split(':')[1])-1))\n",
    "                reference_span_bp_accounting_set.update(range(int(subregion[1].split(':')[0]),int(subregion[1].split(':')[1])))\n",
    "                allele_span_list.append(subregion[3])\n",
    "                reference_span_list.append(subregion[4])\n",
    "                alignment_midline_list.append(subregion[5])\n",
    "                reference_span_bp_accounting_set.update(range(int(sorted(allele_reconstruction_temp_list)[index-1][1].split(':')[1])-1,int(subregion[1].split(':')[0])))\n",
    "\n",
    "    \n",
    "    # missing region of allele sequence as it appears in fasta\n",
    "    bases_in_fasta_allele_not_accounted_for_in_alignment = sorted(set(range(sorted(allele_fasta_span_bp_accounting_set)[0],sorted(allele_fasta_span_bp_accounting_set)[-1]))-allele_fasta_span_bp_accounting_set)\n",
    "    if len(bases_in_fasta_allele_not_accounted_for_in_alignment) > 0:   \n",
    "        bases_to_add = allele_fasta[bases_in_fasta_allele_not_accounted_for_in_alignment[0]:bases_in_fasta_allele_not_accounted_for_in_alignment[-1]+1]\n",
    "    else:\n",
    "        bases_to_add = ''\n",
    "        \n",
    "    reconstructed_hsp_from = str(int(sorted(list(reference_span_bp_accounting_set))[0])+1)\n",
    "    reconstructed_hsp_to = str(int(sorted(list(reference_span_bp_accounting_set))[-1])+1)\n",
    "    reconstructed_hsp_qseq =  ''.join(allele_span_list).strip('-')\n",
    "    reconstructed_hsp_hseq = ''.join(reference_span_list).strip('-')\n",
    "    reconstructed_hsp_midline = ''.join(alignment_midline_list)\n",
    "    \n",
    "    allele_reconstruction_list.append('<Hsp_hit-from>'+str(reconstructed_hsp_from)+'</Hsp_hit-from>')\n",
    "    allele_reconstruction_list.append('<Hsp_hit-to>'+str(reconstructed_hsp_to)+'</Hsp_hit-to>')\n",
    "    allele_reconstruction_list.append('<Hsp_qseq>'+reconstructed_hsp_qseq+'</Hsp_qseq>')\n",
    "    allele_reconstruction_list.append('<Hsp_hseq>'+reconstructed_hsp_hseq+'</Hsp_hseq>')\n",
    "    allele_reconstruction_list.append('<Hsp_midline>'+reconstructed_hsp_midline+'</Hsp_midline>')\n",
    "    \n",
    "    reconstructed_alleles_with_multiple_hsps_list.append(allele_reconstruction_list)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments_list4 = []\n",
    "for i in alignments_list3:\n",
    "    if i not in multiple_alignments_hsp_list:\n",
    "        alignments_list4.append(i)\n",
    "for i in reconstructed_alleles_with_multiple_hsps_list:\n",
    "        alignments_list4.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In script_metrics.txt, log samples and allele IDs identified as having (1) no hits in alignment database or (2) multiple hits in alignment database, as well as (3) samples and allele IDs having more than 1 high-scoring pair (hsp) that the script was unable to reconstruct toward an alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use print redirection to write to target file, in append mode (append to script_metrics.txt)\n",
    "\n",
    "filename = Path(str(output_path)+'/'+processdate+'_script_metrics.txt')\n",
    "with open(filename, 'a') as f:\n",
    "    print(\"\\nRecord of ranked alleles deprecated from analysis output:\", file = f)\n",
    "    print(\"\\n    No hits identified by BLASTN in alignment database: \", file = f)\n",
    "    if len(no_hits_list) == 0:\n",
    "        print(\"        None\", file = f)    \n",
    "    else:\n",
    "        for i in no_hits_list:\n",
    "            print(\"        \"+i, file = f)\n",
    "    print(\"\\n    Multiple hits identified by BLASTN in alignment database: \", file = f)\n",
    "    if len(multiple_alignments_hits_list) == 0:\n",
    "        print(\"        None\", file = f) \n",
    "    else:\n",
    "        for i in multiple_alignments_hits_list:\n",
    "            print(\"        \"+i[1].split('>')[1].split('<')[0], file = f)\n",
    "    print(\"\\n    >1 high-scoring pair (hsp) identified by BLASTN, and hsp's were reconstructed into a hypothesized allele: \", file = f)\n",
    "    if len(alleles_with_multiple_hsps_that_can_be_reconstructed_list) == 0:\n",
    "        print(\"        None\", file = f) \n",
    "    else:\n",
    "        for i in alleles_with_multiple_hsps_that_can_be_reconstructed_list:\n",
    "            print(\"        \"+i[1].split('>')[1].split('<')[0], file = f)     \n",
    "    print(\"\\n    >1 high-scoring pair (hsp) identified by BLASTN, but hsp's could not be reconstructed into a hypothesized allele: \", file = f)\n",
    "    if len(alleles_with_multiple_hsps_that_cannot_be_reconstructed_list) == 0:\n",
    "        print(\"        None\", file = f) \n",
    "    else:\n",
    "        for i in alleles_with_multiple_hsps_that_cannot_be_reconstructed_list:\n",
    "            print(\"        \"+i[1].split('>')[1].split('<')[0], file = f) \n",
    "    print(\"\\n\", file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finalize list containing candidate alleles with single alignment hit in reference database.  \n",
    "Prepare **'alignmentoutput_dict'**, a dictionary that aggregates all sample-associated alleles as sublists within a single list (value) assigned to appropriate sample name ID (key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Among lists containing alignment data in alignments_list4, determine which queries (reads) correspond to the same sample; where querydef = i[1].split(\">\")[1].split(\"_[\")[0], reads belonging to the same sample share identical querydef\n",
    "# Fasta deflines encode frequency metrics for reads, based on defline format:\n",
    "# sampleID_[reads/total reads]_percentile_% read abundance_% top 10 reads_% reads filtered for 1%_% reads filtered for 10%\n",
    "querydef_list = []\n",
    "for i in alignments_list3:\n",
    "    querydef = i[1].split(\">\")[1].split(\"_\")[0]\n",
    "    querydef_list.append(querydef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "querydef_uniq_list = []\n",
    "for i in querydef_list:\n",
    "    if i in querydef_uniq_list:\n",
    "        pass\n",
    "    else:\n",
    "        querydef_uniq_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare dictionary relating sample IDs to their associated reads ('alleles')      \n",
    "alignmentoutput_dict = {}\n",
    "for i in querydef_uniq_list:\n",
    "    alignmentoutput_dict[\"{0}\".format(i)] = tuple(x for x in alignments_list4 if bool(re.search(i, x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify sample IDs for which no valid candidate alleles were identified. These samples are not further analyzed, but their identities are reported in 'script_metrics.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify & subset sample ID's that do not have output alleles (empty tuple values in dictionary)\n",
    "empty_sampleIDs_list = []\n",
    "for i in alignmentoutput_dict:\n",
    "    if bool(alignmentoutput_dict.get(i) == ()):\n",
    "        empty_sampleIDs_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of alignmentoutput_dict, removing dictionary keys with empty tuple values\n",
    "alignmentoutput_dict2 = { k : v for k,v in alignmentoutput_dict.items() if v}\n",
    "# Alignmentoutput_dict2 is the key dictionary for alignment information\n",
    "\n",
    "# Log allele definitions time duration\n",
    "allele_definitionsDuration = str(datetime.now() - startTime_alleles).split(':')[0]+' hr|'+str(datetime.now() - startTime_alleles).split(':')[1]+' min|'+str(datetime.now() - startTime_alleles).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_alleles).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Identify TFBS matches to motifs (FIMO) in reference and candidate allele sequences\n",
    "Data for sample-specific alleles were assembled in **alignmentoutput_dict**, a dictionary that collected alignment data for each sample's top 5 reads, with each read's frequency metrics maintained in the allele name (defline). The contents of this dictionary are now further parsed, along with TFBS data collected by FIMO in **fimo.tsv** files, to generate repositories for TFBS matches identified for reference and allele sequences (**dict_ref_TFBS**, **dict_allele_TFBS**). TFBS in allele sequences are then compared to TFBS in cognate reference sequences to assemble **dict_allele_TFBS_synopsis**, which logs TFBS **gained** and **lost** in each allele relative to reference sequence.\n",
    "\n",
    "--------\n",
    "The output of these analytics is reported in **'collated_TFBS.txt'** and **'collated_TFBS.xlsx'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on FIMO operations duration      \n",
    "startTime_fimo = datetime.now()\n",
    "\n",
    "# Reference sequence input\n",
    "ref_input = Path(fasta_ref)\n",
    "\n",
    "# Reference sequence(s): FIMO file output directory\n",
    "ref_TFBS_output = output_path / 'fimo_out_ref'\n",
    "\n",
    "# alleles: FIMO file output directory      \n",
    "allele_TFBS_output = output_path / 'fimo_out'\n",
    "\n",
    "# Reference sequence(s): FIMO command (usage: fimo --bfile <background file> <motif file> <sequence file>)       \n",
    "cmd_TFBS = str(fimo_path)+' --bfile '+str(markovbackground_output)+' --o '+str(ref_TFBS_output)+' --thresh 1e-4'+' '+str(fimo_motifs_path)+' '+str(ref_input)\n",
    "\n",
    "os.system(cmd_TFBS)\n",
    "\n",
    "# Alleles: FIMO command (usage: fimo --bfile <background file> <motif file> <sequence file>) \n",
    "cmd_TFBS = str(fimo_path)+' --bfile '+str(markovbackground_output)+' --o '+str(allele_TFBS_output)+' --thresh 1e-4'+' '+str(fimo_motifs_path)+' '+str(query_input)\n",
    "\n",
    "os.system(cmd_TFBS)\n",
    "\n",
    "# Log FIMO operations time duration\n",
    "fimoDuration = str(datetime.now() - startTime_fimo).split(':')[0]+' hr|'+str(datetime.now() - startTime_fimo).split(':')[1]+' min|'+str(datetime.now() - startTime_fimo).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_fimo).split(':')[2].split('.')[1]+' microsec' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate TFBSs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on TFBS collation operations duration\n",
    "startTime_TFBScollation = datetime.now()\n",
    "\n",
    "# TFBS output files exist for both reference sequences and alleles (two separate files, in two separate directories: fimo_out & fimo_out_ref)  \n",
    "# Prepare dictionary of TFBSs ID'ed, for each reference sample \n",
    "with open(str(ref_TFBS_output)+'/fimo.tsv', 'r') as file:\n",
    "    ref_lines = file.readlines()\n",
    "    \n",
    "# Remove FIMO header lines, etc.\n",
    "ref_lines = ref_lines[1:]\n",
    "for line in ref_lines.copy():\n",
    "    if len(line.split('\\t')) < 10:\n",
    "        ref_lines.remove(line)\n",
    "        \n",
    "# Convert to set for faster processing\n",
    "ref_lines = set(ref_lines)\n",
    "\n",
    "with open(str(ref_input)) as file:\n",
    "    fasta_lines = file.readlines()\n",
    "fasta_names = fasta_lines[0::2]\n",
    "fasta_names = [i.strip('\\n').strip('>') for i in fasta_names]\n",
    "ref_set = set(fasta_names)\n",
    "\n",
    "dict_ref_TFBS = {}\n",
    "for ref in ref_set:\n",
    "    dict_ref_TFBS[ref] = [] \n",
    "    \n",
    "# Take 3rd field of all lines as search for presence of key in dictionary, and add line as string in value list of key (allele)\n",
    "for line in ref_lines:\n",
    "    if line.split('\\t')[2].strip() in dict_ref_TFBS:\n",
    "        dict_ref_TFBS[line.split('\\t')[2]].append(line.strip())\n",
    "        \n",
    "# Prepare allele dictionary; first, populate with 'all_sites'.  Then, run comparison to 'dict_TFBS_ref' to define sites that are lost vs. gained relative to reference sequence.\n",
    "# Prepare dictionary of TFBSs ID'ed, for each sample allele\n",
    "\n",
    "dict_allele_TFBS = {}\n",
    "for allele in alignmentoutput_dict2:\n",
    "    dict_allele_TFBS[allele] = {}\n",
    "\n",
    "for allele in alignmentoutput_dict2:\n",
    "    for x in range(0, len(alignmentoutput_dict2.get(allele))):\n",
    "        dict_allele_TFBS[allele].update({alignmentoutput_dict2.get(allele)[x][1].split(\">\")[1].split(\"<\")[0]:[]})\n",
    "        \n",
    "with open(str(allele_TFBS_output)+'/fimo.tsv', 'r') as file:\n",
    "    allele_lines = file.readlines() \n",
    "\n",
    "# Remove FIMO header lines, etc.\n",
    "allele_lines = allele_lines[1:]\n",
    "for line in allele_lines.copy():\n",
    "    if len(line.split('\\t')) < 10:\n",
    "        allele_lines.remove(line)\n",
    "        \n",
    "# Convert to set for faster processing\n",
    "allele_lines = set(allele_lines)\n",
    "\n",
    "# Populate each allele with its 'all_sites' information\n",
    "for line in allele_lines:\n",
    "    dict_allele_TFBS_sample_key = line.split('\\t')[2].strip().split('_')[0]\n",
    "    dict_allele_TFBS_allele_key = line.split('\\t')[2].strip()\n",
    "    if dict_allele_TFBS_sample_key in dict_allele_TFBS:\n",
    "        if dict_allele_TFBS.get(dict_allele_TFBS_sample_key).get(dict_allele_TFBS_allele_key) is not None:\n",
    "            dict_allele_TFBS.get(dict_allele_TFBS_sample_key).get(dict_allele_TFBS_allele_key).append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare synopsis dictionary: *gained*, *lost*, and *all_sites* sublists for each allele (*gained* and *lost* sublists populated by comparison of allele's *all_sites* list to *all_sites* list of reference sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare synopsis dictionary with alleles as keys, and list of 3 sublists: gained, lost, all_sites      \n",
    "# Run comparison of 'all_sites' information relative to reference allele information.\n",
    "dict_allele_TFBS_synopsis = {}\n",
    "for allele in alignmentoutput_dict2:\n",
    "    dict_allele_TFBS_synopsis[allele] = {}\n",
    "    \n",
    "for allele in alignmentoutput_dict2:\n",
    "    for x in range(0, len(alignmentoutput_dict2.get(allele))):\n",
    "        dict_allele_TFBS_synopsis[allele].update({alignmentoutput_dict2.get(allele)[x][1].split(\">\")[1].split(\"<\")[0]:{'gained':[],'lost':[],'all_sites':[], 'TFs':{}, 'allele_sequence':[\n",
    "            alignmentoutput_dict2.get(allele)[x][7]]+[alignmentoutput_dict2.get(allele)[x][8]]+[alignmentoutput_dict2.get(allele)[x][9]]}})\n",
    "        \n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        for motif in dict_allele_TFBS.get(sample).get(allele):\n",
    "            dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites').append(motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+'),'+motif.split('\\t')[5]+','+motif.split('\\t')[9]+','+motif.split('\\t')[7]+','+motif.split('\\t')[3]+','+motif.split('\\t')[4])\n",
    "            if motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')' not in dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs'):\n",
    "                count = 1\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').update({motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')':count})\n",
    "            else:\n",
    "                count = dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').get(motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')')+1\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').update({motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')':count})\n",
    "\n",
    "# Run comparisons:\n",
    "# Make ref_TFBS_synopsis dictionary\n",
    "dict_ref_TFBS_synopsis = {}\n",
    "for ref in dict_ref_TFBS:\n",
    "    dict_ref_TFBS_synopsis[ref] = {'all_sites':[], 'TFs':{}}\n",
    "    \n",
    "\n",
    "# Summarize TF counts in reference sequences      \n",
    "for ref in dict_ref_TFBS_synopsis:    \n",
    "    for motif in dict_ref_TFBS.get(ref):\n",
    "        if motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')' not in dict_ref_TFBS_synopsis.get(ref).get('TFs'):\n",
    "            count = 1\n",
    "            dict_ref_TFBS_synopsis.get(ref).get('TFs').update({motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+')':count})\n",
    "        else:\n",
    "            count = dict_ref_TFBS_synopsis.get(ref).get('TFs').get(motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')')+1\n",
    "            dict_ref_TFBS_synopsis.get(ref).get('TFs').update({motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+')':count})\n",
    "            \n",
    "# Catalog TFBSs in reference sequences, in format akin to 'all_sites' format in dict_allele_TFBS_synopsis          \n",
    "for ref in dict_ref_TFBS_synopsis:\n",
    "    for motif in dict_ref_TFBS.get(ref):\n",
    "        dict_ref_TFBS_synopsis.get(ref).get('all_sites').append(motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+'),'+motif.split('\\t')[5]+','+motif.split('\\t')[9]+','+motif.split('\\t')[7]+','+motif.split('\\t')[3]+','+motif.split('\\t')[4])\n",
    "        \n",
    "# Run comparisons, populating into dict_allele_TFBS_synopsis\n",
    "ref_options = [ref for ref in dict_ref_TFBS]\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    # define reference sequence appropriate to sample\n",
    "    for ref in ref_options:\n",
    "        if re.search(ref, sample):\n",
    "            sample_ref = ref\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        # check only for motifs that overlap aligned region between allele and ref\n",
    "        for x in alignmentoutput_dict2.get(sample):\n",
    "            if x[1].split('>')[1].split('<')[0] == allele:\n",
    "                to_from_range = range(int(x[5].split('>')[1].split('<')[0]),int(x[6].split('>')[1].split('<')[0]))\n",
    "        dict_allele_TFBS_synopsis.get(sample).get(allele)['ref_coordinates_span'] = to_from_range\n",
    "        ref_spans_represented_in_allele_hsps_temp = []\n",
    "        # get hsp aligment spans relative to reference coordinates\n",
    "        for index, x in enumerate(alignments_list3):\n",
    "            if x[1].split('>')[1].split('<')[0] == allele:\n",
    "                for i in range(5, len(alignments_list3[index]), 5):\n",
    "                    ref_spans_represented_in_allele_hsps_temp.append(range(int(alignments_list3[index][i].split('>')[1].split('<')[0])+1, int(alignments_list3[index][i+1].split('>')[1].split('<')[0])))\n",
    "                    ref_spans_represented_in_allele_hsps = sorted(list(i) for i in ref_spans_represented_in_allele_hsps_temp)\n",
    "        dict_allele_TFBS_synopsis.get(sample).get(allele)['hsp_alignment_spans'] = ref_spans_represented_in_allele_hsps   \n",
    "        ref_TFBS_set_to_include_in_evaluation = []\n",
    "        for ref_motif in dict_ref_TFBS_synopsis.get(sample_ref).get('all_sites'):\n",
    "            ref_motif_range = range(int(ref_motif.split(',')[4]), int(ref_motif.split(',')[5]))\n",
    "            if set(to_from_range).intersection(ref_motif_range):\n",
    "                ref_TFBS_set_to_include_in_evaluation.append(ref_motif)  \n",
    "        for motif in dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites'):\n",
    "            # limit ref range\n",
    "            if motif.split(',')[:4] in [i.split(',')[:4] for i in ref_TFBS_set_to_include_in_evaluation]:\n",
    "                pass\n",
    "            else:\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained').append(motif)\n",
    "        for motif in ref_TFBS_set_to_include_in_evaluation:\n",
    "            if motif.split(',')[:4] in [i.split(',')[:4] for i in dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites')]:\n",
    "                pass\n",
    "            else:\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost').append(motif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add allele ranks to allele names\n",
    "dict_allele_TFBS_synopsis_allele_ranks = {}\n",
    "\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    index_frequency_list = []\n",
    "    for index, allele in enumerate(dict_allele_TFBS_synopsis.get(sample)):\n",
    "        index_frequency_list.append((float(allele.split('_')[6].split(':')[1]), allele.split('_')[6], allele, index))\n",
    "        index_frequency_list_sorted = sorted(index_frequency_list, reverse=True)\n",
    "    dict_allele_TFBS_synopsis_allele_ranks[sample] = index_frequency_list_sorted\n",
    "    \n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    for index, allele in enumerate(dict_allele_TFBS_synopsis.get(sample)):\n",
    "        for index, ranked_allele in enumerate(dict_allele_TFBS_synopsis_allele_ranks.get(sample)):\n",
    "            if allele == ranked_allele[2]:\n",
    "                allele_rank = index+1\n",
    "        dict_allele_TFBS_synopsis.get(sample).get(allele).update({'allele_rank': allele_rank})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take stock of gained and lost TFBS that positionally overlap in reference vs. allele (assessment of whether TFBS 'gained' for given TFs may in fact be 'regained' TFBS, in alleles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin interpretive assessment of whether TFBS 'gained' for given TFs may in fact be 'regained' TFBS, in alleles where TFBS for the same TF has been lost\n",
    "# In other words, small local base changes that disrupt a TFBS for a given TF may nevertheless supply a distinct TFBS for the same TF,\n",
    "# amounting to, in principle, a 'reconstitution' or preservation of potential TFBS for TF\n",
    "# First, convert lost/gained TFBS for each allele (per sample) to dataframe, allele_TFBS_synopsis_df\n",
    "\n",
    "sample_list = [] \n",
    "allele_count_list = []\n",
    "allele_list = []\n",
    "allele_sequence_list = []\n",
    "reference_sequence_list = []\n",
    "alignment_midline_list = []\n",
    "TF_list = []\n",
    "strand_list = []\n",
    "gained_TFBS_sequence_list = []\n",
    "lost_TFBS_sequence_list = []\n",
    "p_val_list = []\n",
    "lostvsgained_list = []\n",
    "allele_start_coordinate_list = []\n",
    "allele_stop_coordinate_list = []\n",
    "ref_start_coordinate_list = []\n",
    "ref_stop_coordinate_list = []\n",
    "\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    allele_count = 0\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        allele_count = allele_count+1\n",
    "        for TF_class in dict_allele_TFBS_synopsis.get(sample).get(allele):\n",
    "            if TF_class == 'lost':\n",
    "                if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost')) == 0:\n",
    "                    sample_list.append(sample)\n",
    "                    allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                    allele_list.append(allele)\n",
    "                    allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                    reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                    alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                    TF_list.append('n/a')\n",
    "                    strand_list.append('n/a')\n",
    "                    gained_TFBS_sequence_list.append('n/a')\n",
    "                    lost_TFBS_sequence_list.append('n/a')\n",
    "                    p_val_list.append('n/a')\n",
    "                    allele_start_coordinate_list.append('n/a')\n",
    "                    allele_stop_coordinate_list.append('n/a')\n",
    "                    ref_start_coordinate_list.append('n/a')\n",
    "                    ref_stop_coordinate_list.append('n/a')\n",
    "                    lostvsgained_list.append(\"No TFBS lost\")\n",
    "                else:\n",
    "                    for TF in dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost'):\n",
    "                        sample_list.append(sample)\n",
    "                        allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                        allele_list.append(allele)\n",
    "                        allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                        reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                        alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                        TF_list.append(TF.split(',')[0])\n",
    "                        strand_list.append(TF.split(',')[1])\n",
    "                        gained_TFBS_sequence_list.append('n/a')\n",
    "                        lost_TFBS_sequence_list.append(TF.split(',')[2])                            \n",
    "                        p_val_list.append(TF.split(',')[3])\n",
    "                        allele_start_coordinate_list.append('n/a')\n",
    "                        allele_stop_coordinate_list.append('n/a')\n",
    "                        ref_start_coordinate_list.append(TF.split(',')[4])\n",
    "                        ref_stop_coordinate_list.append(TF.split(',')[5])\n",
    "                        lostvsgained_list.append('lost')\n",
    "            elif TF_class == 'gained':\n",
    "                if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained')) == 0:\n",
    "                    sample_list.append(sample)\n",
    "                    allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                    allele_list.append(allele)\n",
    "                    allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                    reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                    alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())\n",
    "                    TF_list.append('n/a')\n",
    "                    strand_list.append('n/a')\n",
    "                    gained_TFBS_sequence_list.append('n/a')\n",
    "                    lost_TFBS_sequence_list.append('n/a')\n",
    "                    p_val_list.append('n/a')\n",
    "                    allele_start_coordinate_list.append('n/a')\n",
    "                    allele_stop_coordinate_list.append('n/a')\n",
    "                    ref_start_coordinate_list.append('n/a')\n",
    "                    ref_stop_coordinate_list.append('n/a')\n",
    "                    lostvsgained_list.append(\"No TFBS gained\")\n",
    "                else:\n",
    "                    for TF in dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained'):\n",
    "                        sample_list.append(sample)\n",
    "                        allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                        allele_list.append(allele)\n",
    "                        allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                        reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                        alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())\n",
    "                        TF_list.append(TF.split(',')[0])\n",
    "                        strand_list.append(TF.split(',')[1])\n",
    "                        gained_TFBS_sequence_list.append(TF.split(',')[2])\n",
    "                        lost_TFBS_sequence_list.append('n/a')                            \n",
    "                        p_val_list.append(TF.split(',')[3])\n",
    "                        allele_start_coordinate_list.append(TF.split(',')[4])\n",
    "                        allele_stop_coordinate_list.append(TF.split(',')[5])\n",
    "                        ref_start_coordinate_list.append('n/a')\n",
    "                        ref_stop_coordinate_list.append('n/a')\n",
    "                        lostvsgained_list.append('gained')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin interpretive assessment of whether TFBS 'gained' for given TFs may in fact be 'regained' TFBS, in alleles where TFBS for the same TF has been lost\n",
    "# In other words, small local base changes that disrupt a TFBS for a given TF may nevertheless supply a distinct TFBS for the same TF,\n",
    "# amounting to, in principle, a 'reconstitution' or preservation of potential TFBS for TF\n",
    "# First, convert lost/gained TFBS for each allele (per sample) to dataframe, allele_TFBS_synopsis_df\n",
    "\n",
    "sample_list = [] \n",
    "allele_count_list = []\n",
    "allele_list = []\n",
    "allele_sequence_list = []\n",
    "reference_sequence_list = []\n",
    "alignment_midline_list = []\n",
    "TF_list = []\n",
    "strand_list = []\n",
    "gained_TFBS_sequence_list = []\n",
    "lost_TFBS_sequence_list = []\n",
    "p_val_list = []\n",
    "lostvsgained_list = []\n",
    "allele_start_coordinate_list = []\n",
    "allele_stop_coordinate_list = []\n",
    "ref_start_coordinate_list = []\n",
    "ref_stop_coordinate_list = []\n",
    "\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    allele_count = 0\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        allele_count = allele_count+1\n",
    "        for TF_class in dict_allele_TFBS_synopsis.get(sample).get(allele):\n",
    "            if TF_class == 'lost':\n",
    "                # if allele alignment is to a subset of a longer user-provided reference span, coordinates\n",
    "                # must be converted to allele span coordinates (because coordinates in the 'lost' category are derived\n",
    "                # from reference sequence coordinates in dict_allele_TFBS_synopsis\n",
    "                if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost')) == 0:\n",
    "                    sample_list.append(sample)\n",
    "                    allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                    allele_list.append(allele)\n",
    "                    allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                    reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                    alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                    TF_list.append('n/a')\n",
    "                    strand_list.append('n/a')\n",
    "                    gained_TFBS_sequence_list.append('n/a')\n",
    "                    lost_TFBS_sequence_list.append('n/a')\n",
    "                    p_val_list.append('n/a')\n",
    "                    allele_start_coordinate_list.append('n/a')\n",
    "                    allele_stop_coordinate_list.append('n/a')\n",
    "                    ref_start_coordinate_list.append('n/a')\n",
    "                    ref_stop_coordinate_list.append('n/a')\n",
    "                    lostvsgained_list.append(\"No TFBS lost\")\n",
    "                else:\n",
    "                    # retrieve allele's alignment span to reference sequence\n",
    "                    ref_span = [dict_allele_TFBS_synopsis.get(sample).get(allele).get('ref_coordinates_span')[0],dict_allele_TFBS_synopsis.get(sample).get(allele).get('ref_coordinates_span')[-1]]\n",
    "                    # retrieve allele's intact span\n",
    "                    allele_length = len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].replace('-',''))\n",
    "                    # retrieve allele's alignment span(s) relative to reference sequence\n",
    "                    dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')\n",
    "                    # retrieve intervening span between allele alignment spans relative to reference sequence\n",
    "                    if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')) < 2:\n",
    "                        span = int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].count('-'))\n",
    "                    else:\n",
    "                        span = int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[1][0])-int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[0][-1])  \n",
    "                    for TF in dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost'):\n",
    "                        sample_list.append(sample)\n",
    "                        allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                        allele_list.append(allele)\n",
    "                        allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                        reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                        alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                        TF_list.append(TF.split(',')[0])\n",
    "                        strand_list.append(TF.split(',')[1])\n",
    "                        gained_TFBS_sequence_list.append('n/a')\n",
    "                        lost_TFBS_sequence_list.append(TF.split(',')[2])                            \n",
    "                        p_val_list.append(TF.split(',')[3])\n",
    "                        allele_start_coordinate_list.append('n/a')\n",
    "                        allele_stop_coordinate_list.append('n/a')\n",
    "                        # make adjustments in recorded 'lost' TFBS coordinates, to reflect coordinates as defined in allele span rather than coordinates as defined in reference span\n",
    "                        if set(range(int(TF.split(',')[4]), int(TF.split(',')[5]))).intersection(range(int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[0][0]),\n",
    "                                                                                      int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[0][-1]+1))):\n",
    "                            ref_start_coordinate_list.append(int(TF.split(',')[4])-ref_span[0])\n",
    "                            ref_stop_coordinate_list.append(int(TF.split(',')[5])-ref_span[0])\n",
    "                        elif set(range(int(TF.split(',')[4]), int(TF.split(',')[5]))).intersection(range(int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[0][1]+1),\n",
    "                                                                                      int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[1][0]))):\n",
    "                            ref_start_coordinate_list.append(int(TF.split(',')[4])-ref_span[0])\n",
    "                            ref_stop_coordinate_list.append(int(TF.split(',')[5])-ref_span[0])\n",
    "                        elif set(range(int(TF.split(',')[4]), int(TF.split(',')[5]))).intersection(range(int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[1][0]),\n",
    "                                                                                      int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[1][-1]+1))):\n",
    "                            ref_start_coordinate_list.append(int(TF.split(',')[4])-ref_span[0])\n",
    "                            ref_stop_coordinate_list.append(int(TF.split(',')[5])-ref_span[0])                       \n",
    "                        lostvsgained_list.append('lost')\n",
    "            elif TF_class == 'gained':\n",
    "                if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained')) == 0:\n",
    "                    sample_list.append(sample)\n",
    "                    allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                    allele_list.append(allele)\n",
    "                    allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                    reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                    alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())\n",
    "                    TF_list.append('n/a')\n",
    "                    strand_list.append('n/a')\n",
    "                    gained_TFBS_sequence_list.append('n/a')\n",
    "                    lost_TFBS_sequence_list.append('n/a')\n",
    "                    p_val_list.append('n/a')\n",
    "                    allele_start_coordinate_list.append('n/a')\n",
    "                    allele_stop_coordinate_list.append('n/a')\n",
    "                    ref_start_coordinate_list.append('n/a')\n",
    "                    ref_stop_coordinate_list.append('n/a')\n",
    "                    lostvsgained_list.append(\"No TFBS gained\")\n",
    "                else:\n",
    "                    # retrieve allele's alignment span to reference sequence\n",
    "                    ref_span = [dict_allele_TFBS_synopsis.get(sample).get(allele).get('ref_coordinates_span')[0],dict_allele_TFBS_synopsis.get(sample).get(allele).get('ref_coordinates_span')[-1]]\n",
    "                    # retrieve allele's intact span\n",
    "                    allele_length = len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].replace('-',''))\n",
    "                    # retrieve allele's alignment span(s) relative to reference sequence\n",
    "                    dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')\n",
    "                    # retrieve intervening span between allele alignment spans relative to reference sequence\n",
    "                    if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')) < 2:\n",
    "                        span = int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].count('-'))\n",
    "                    else:\n",
    "                        span = int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[1][0])-int(dict_allele_TFBS_synopsis.get(sample).get(allele).get('hsp_alignment_spans')[0][-1])   \n",
    "                    for TF in dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained'):\n",
    "                        sample_list.append(sample)\n",
    "                        allele_count_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_rank'))\n",
    "                        allele_list.append(allele)\n",
    "                        allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                        reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                        alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())\n",
    "                        TF_list.append(TF.split(',')[0])\n",
    "                        strand_list.append(TF.split(',')[1])\n",
    "                        gained_TFBS_sequence_list.append(TF.split(',')[2])\n",
    "                        lost_TFBS_sequence_list.append('n/a')                            \n",
    "                        p_val_list.append(TF.split(',')[3])\n",
    "                        allele_start_coordinate_list.append(TF.split(',')[4])\n",
    "                        allele_stop_coordinate_list.append(TF.split(',')[5]) \n",
    "                        ref_start_coordinate_list.append('n/a')\n",
    "                        ref_stop_coordinate_list.append('n/a')\n",
    "                        lostvsgained_list.append('gained')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*allele_TFBS_synopsis_df*: dataframe synopsis of samples and alleles, with individual rows detailing TFBSs lost or gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe synopsis of samples and alleles, with individual rows detailing TFBS lost or gained \n",
    "allele_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_count_list, \"allele ID\":allele_list, \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                   \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list, \"TF\":TF_list,\n",
    "                                   \"strand\":strand_list, \n",
    "                                   \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                   \"Lost TFBS start coordinate (in reference)\":ref_start_coordinate_list,\n",
    "                                   \"Lost TFBS end coordinate (in reference)\":ref_stop_coordinate_list,\n",
    "                                   \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":gained_TFBS_sequence_list, \n",
    "                                   \"Gained TFBS start coordinate (in allele)\":allele_start_coordinate_list,\n",
    "                                   \"Gained TFBS end coordinate (in allele)\":allele_stop_coordinate_list, \n",
    "                                   \"p-val\":p_val_list, \"lost or gained in allele (relative to ref)?\":lostvsgained_list}\n",
    "allele_TFBS_synopsis_df = pd.DataFrame(allele_TFBS_synopsis_df_columns)\n",
    "\n",
    "allele_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"lost or gained in allele (relative to ref)?\"],ascending=[True, True, True, True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add read counts and calculated frequencies to allele_TFBS_synopsis_df\n",
    "read_count_list = [i.split('_')[2].strip('[]').split('/')[0] for i in allele_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "total_reads_list = [i.split('_')[2].strip('[]').split('/')[1] for i in allele_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_total_reads_list = [i.split('_')[4].split(':')[1] for i in allele_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_reads_filtered_for_1pct_list = [float(i.split('_')[7].split(':')[1]) if i.split('_')[7].split(':')[1] != 'None' else 0 for i in allele_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_reads_filtered_for_10pct_list = [float(i.split('_')[8].split(':')[1]) if i.split('_')[8].split(':')[1] != 'None' else 0 for i in allele_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "\n",
    "# Add column with allele comment (comment if appropriate)\n",
    "# Note, pre-processing reads with a read cleaning utility such as cutadapt, trimmomatic, or fastp may remove such reads/\n",
    "# inferred alleles in advance, obviating need for this read length flag\n",
    "allele_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in allele_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "allele_TFBS_synopsis_df.insert(loc=3, column='reads', value=read_count_list)\n",
    "allele_TFBS_synopsis_df.insert(loc=4, column='total reads', value=total_reads_list)\n",
    "allele_TFBS_synopsis_df.insert(loc=5, column='% total reads', value=pct_total_reads_list)\n",
    "allele_TFBS_synopsis_df.insert(loc=6, column='% reads filtered for reads <1%', value=pct_reads_filtered_for_1pct_list)\n",
    "allele_TFBS_synopsis_df.insert(loc=7, column='% reads filtered for reads <10%', value=pct_reads_filtered_for_10pct_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of whether TFBSs in *allele_TFBS_synopsis_df* may be positionally overlapping TFBS replacements/cognates ('regains')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May-June 2021, revisited July\n",
    "# assess whether allele_TFBS_synopsis_df lost/gained TFBS may be TFBS 'replacements'/cognates\n",
    "lost_TFBS_list = []\n",
    "gained_TFBS_list = []\n",
    "\n",
    "allele_TFBS_synopsis_df_coordinates_updated = pd.DataFrame(columns=allele_TFBS_synopsis_df.columns)\n",
    "span_between_aligning_blocks_allele_list = []\n",
    "\n",
    "# coordinates for 'lost' TFBS have already been adjusted to reflect allele alignment span relative to user-provided reference sequence span\n",
    "# coordinates for 'gained' TFBS (novel to an allele relative to reference sequence) need to be corrected for positions in allele that are beyond a deletion/insertion span\n",
    "# (and would therefore not enable comparison to cognate coordinate position in reference sequence unless coordinates are adjusted for missing span)\n",
    "for index, row in allele_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"lost or gained in allele (relative to ref)?\"],ascending=[True, True, True, True, False]).iterrows():\n",
    "    if row['lost or gained in allele (relative to ref)?'] == \"lost\":\n",
    "        lost_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Lost TFBS start coordinate (in reference)'])+','+str(row['Lost TFBS end coordinate (in reference)'])+\n",
    "                              ','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "        allele_TFBS_synopsis_df_coordinates_updated.loc[index] = row\n",
    "        if re.search('-', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0]): \n",
    "            # position of longest non-corresponding span in allele, relative to reference (in alignment) (characteristic if deletion allele)\n",
    "            span_between_aligning_blocks_allele_temp = re.search(max(re.findall(r'-+', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0])),dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0]).span()\n",
    "            span_between_aligning_blocks_allele = tuple(value+1 for value in span_between_aligning_blocks_allele_temp)\n",
    "            calculated_span_between_aligning_blocks_allele = span_between_aligning_blocks_allele[1]-span_between_aligning_blocks_allele[0]\n",
    "            # position of longest non-corresponding span in reference, relative to allele (in alignment) (characteristic of insertion allele)\n",
    "            # span_between_aligning_blocks_reference = re.search(max(re.findall(r'-+', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[1])),dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0]).span()\n",
    "            span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "        else:\n",
    "            span_between_aligning_blocks_allele_list.append('n/a') \n",
    "    elif row['lost or gained in allele (relative to ref)?'] == \"gained\":\n",
    "        # retrieve allele's alignment span to reference sequence\n",
    "        ref_span = [dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('ref_coordinates_span')[0],dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('ref_coordinates_span')[-1]]\n",
    "        # retrieve allele's intact span\n",
    "        allele_length = len(dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0].replace('-',''))\n",
    "        # retrieve allele's alignment span(s) relative to reference sequence\n",
    "        dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('hsp_alignment_spans')\n",
    "        # retrieve intervening span between allele alignment spans relative to reference sequence\n",
    "        # make adjustments in 'gained' TFBS coordinates, to reflect coordinates as defined in reference span rather than coordinates as defined in allele span\n",
    "        # scenario where there was not >1 hsp detected by BLASTN (alignment is unsplit by BLASTN)\n",
    "        if len(dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('hsp_alignment_spans')) < 2:\n",
    "            # print(row['allele rank'], dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('hsp_alignment_spans'))\n",
    "            # adjust allele coordinates relative to reference coordinates\n",
    "            hsp_spans_relative_to_reference_seq = dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('hsp_alignment_spans')    \n",
    "            basal_number = int(hsp_spans_relative_to_reference_seq[0][0])\n",
    "            hsp_spans_relative_to_reference_seq_adjusted = []\n",
    "            for x in hsp_spans_relative_to_reference_seq:\n",
    "                hsp_spans_relative_to_reference_seq_adjusted.append([int(y)-basal_number for y in x])  \n",
    "            if re.search('-', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0]): \n",
    "            # position of longest non-corresponding span in allele, relative to reference (in alignment) (characteristic if deletion allele)\n",
    "                span_between_aligning_blocks_allele_temp = re.search(max(re.findall(r'-+', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0])),dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0]).span()\n",
    "                span_between_aligning_blocks_allele = tuple(value+1 for value in span_between_aligning_blocks_allele_temp)\n",
    "                calculated_span_between_aligning_blocks_allele = span_between_aligning_blocks_allele[1]-span_between_aligning_blocks_allele[0]\n",
    "            # position of longest non-corresponding span in reference, relative to allele (in alignment) (characteristic of insertion allele)\n",
    "                # span_between_aligning_blocks_reference = re.search(max(re.findall(r'-+', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[1])),dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0]).span()\n",
    "                # scenario if no coordinate adjustment is needed (TFBS end coordinate occurs before largest alignment gap:\n",
    "                if int(row['Gained TFBS end coordinate (in allele)']) in range(int(hsp_spans_relative_to_reference_seq_adjusted[0][0]),\n",
    "                                                                                      int(span_between_aligning_blocks_allele[0])):\n",
    "                    gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Gained TFBS start coordinate (in allele)'])+','+\n",
    "                              str(row['Gained TFBS end coordinate (in allele)'])+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                    allele_TFBS_synopsis_df_coordinates_updated.loc[index] = row\n",
    "                    span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "                # scenario if coordinate adjustment is needed (TFBS end coordinate occurs between start of alignment gap and alignment end):\n",
    "                elif int(row['Gained TFBS end coordinate (in allele)']) in range(int(span_between_aligning_blocks_allele[0]), int(hsp_spans_relative_to_reference_seq_adjusted[0][-1])):\n",
    "                    gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+\n",
    "                              str(int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val']) \n",
    "                    # add row with updated coordinates\n",
    "                    allele_TFBS_synopsis_df_coordinates_updated.loc[index] = {'sample':row['sample'],'allele rank':row['allele rank'], \n",
    "                                                                              'allele ID':row['allele ID'], 'reads':row['reads'], 'total reads':row['total reads'],\n",
    "                                                                              '% total reads':row['% total reads'], '% reads filtered for reads <1%':row['% reads filtered for reads <1%'],\n",
    "                                                                              '% reads filtered for reads <10%':row['% reads filtered for reads <10%'], 'alignment query\\n(allele sequence)':row['alignment query\\n(allele sequence)'],\n",
    "                                                                              'alignment midline':row['alignment midline'], 'alignment hit\\n(reference)':row['alignment hit\\n(reference)'], 'TF':row['TF'],\n",
    "                                                                              'strand':row['strand'], \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Lost TFBS start coordinate (in reference)':row['Lost TFBS start coordinate (in reference)'], 'Lost TFBS end coordinate (in reference)':row['Lost TFBS end coordinate (in reference)'],\n",
    "                                                                              \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Gained TFBS start coordinate (in allele)':int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'Gained TFBS end coordinate (in allele)':int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'p-val':row['p-val'], 'lost or gained in allele (relative to ref)?':row['lost or gained in allele (relative to ref)?'], 'comment':row['comment']}\n",
    "                    span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "            else:\n",
    "                span_between_aligning_blocks_allele = 'n/a'\n",
    "                gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(int(row['Gained TFBS start coordinate (in allele)']))+','+\n",
    "                              str(int(row['Gained TFBS end coordinate (in allele)']))+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                allele_TFBS_synopsis_df_coordinates_updated.loc[index] = row\n",
    "                span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "        # scenario where there was >1 hsp detected by BLASTN (aligning segments were split by BLASTN and required reconstruction)\n",
    "        else:\n",
    "            span_between_aligning_blocks_allele_temp = re.search(max(re.findall(r'-+', dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0])),dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('allele_sequence')[0].split('>')[1].split('<')[0]).span()\n",
    "            span_between_aligning_blocks_allele = tuple(value+1 for value in span_between_aligning_blocks_allele_temp)\n",
    "            calculated_span_between_aligning_blocks_allele = span_between_aligning_blocks_allele[1]-span_between_aligning_blocks_allele[0]\n",
    "            hsp_spans_relative_to_reference_seq = dict_allele_TFBS_synopsis.get(row['sample']).get(row['allele ID']).get('hsp_alignment_spans')    \n",
    "            basal_number = int(hsp_spans_relative_to_reference_seq[0][0])\n",
    "            hsp_spans_relative_to_reference_seq_adjusted = []\n",
    "            for x in hsp_spans_relative_to_reference_seq:\n",
    "                hsp_spans_relative_to_reference_seq_adjusted.append([int(y)-basal_number for y in x])  \n",
    "            # condition for coordinates within first alignment block/hsp (no coordinate adjustment needed)\n",
    "            if int(row['Gained TFBS end coordinate (in allele)']) in range(int(hsp_spans_relative_to_reference_seq_adjusted[0][0]),\n",
    "                                                                                      int(span_between_aligning_blocks_allele[0])):\n",
    "                gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Gained TFBS start coordinate (in allele)'])+','+\n",
    "                              str(row['Gained TFBS end coordinate (in allele)'])+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                allele_TFBS_synopsis_df_coordinates_updated.loc[index] = row\n",
    "                span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "            # condition for coordinates within gap between alignments blocks/hsp's\n",
    "            elif int(row['Gained TFBS end coordinate (in allele)']) in range(int(hsp_spans_relative_to_reference_seq_adjusted[0][-1])+1,\n",
    "                                                                                      int(hsp_spans_relative_to_reference_seq_adjusted[1][0])):\n",
    "                gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Gained TFBS start coordinate (in allele)'])+','+\n",
    "                              str(row['Gained TFBS end coordinate (in allele)'])+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                allele_TFBS_synopsis_df_coordinates_updated.loc[index] = row\n",
    "                span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "            # condition for coordinates beyond gap between alignments blocks/hsp's (coordinate adjustment needed)\n",
    "            elif int(row['Gained TFBS end coordinate (in allele)']) in range(int(hsp_spans_relative_to_reference_seq_adjusted[1][0]),\n",
    "                                                                                      int(hsp_spans_relative_to_reference_seq_adjusted[1][-1])):\n",
    "                gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+\n",
    "                              str(int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                # add row with updated coordinates\n",
    "                allele_TFBS_synopsis_df_coordinates_updated.loc[index] = {'sample':row['sample'],'allele rank':row['allele rank'], \n",
    "                                                                              'allele ID':row['allele ID'], 'reads':row['reads'], 'total reads':row['total reads'],\n",
    "                                                                              '% total reads':row['% total reads'], '% reads filtered for reads <1%':row['% reads filtered for reads <1%'],\n",
    "                                                                              '% reads filtered for reads <10%':row['% reads filtered for reads <10%'], 'alignment query\\n(allele sequence)':row['alignment query\\n(allele sequence)'],\n",
    "                                                                              'alignment midline':row['alignment midline'], 'alignment hit\\n(reference)':row['alignment hit\\n(reference)'], 'TF':row['TF'],\n",
    "                                                                              'strand':row['strand'], \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Lost TFBS start coordinate (in reference)':row['Lost TFBS start coordinate (in reference)'], 'Lost TFBS end coordinate (in reference)':row['Lost TFBS end coordinate (in reference)'],\n",
    "                                                                              \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Gained TFBS start coordinate (in allele)':int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'Gained TFBS end coordinate (in allele)':int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'p-val':row['p-val'], 'lost or gained in allele (relative to ref)?':row['lost or gained in allele (relative to ref)?'], 'comment':row['comment']}\n",
    "                span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "            else:\n",
    "                gained_TFBS_list.append(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+\n",
    "                              str(int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele)+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "                # add row with updated coordinates\n",
    "                allele_TFBS_synopsis_df_coordinates_updated.loc[index] = {'sample':row['sample'],'allele rank':row['allele rank'], \n",
    "                                                                              'allele ID':row['allele ID'], 'reads':row['reads'], 'total reads':row['total reads'],\n",
    "                                                                              '% total reads':row['% total reads'], '% reads filtered for reads <1%':row['% reads filtered for reads <1%'],\n",
    "                                                                              '% reads filtered for reads <10%':row['% reads filtered for reads <10%'], 'alignment query\\n(allele sequence)':row['alignment query\\n(allele sequence)'],\n",
    "                                                                              'alignment midline':row['alignment midline'], 'alignment hit\\n(reference)':row['alignment hit\\n(reference)'], 'TF':row['TF'],\n",
    "                                                                              'strand':row['strand'], \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Lost TFBS start coordinate (in reference)':row['Lost TFBS start coordinate (in reference)'], 'Lost TFBS end coordinate (in reference)':row['Lost TFBS end coordinate (in reference)'],\n",
    "                                                                              \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"],\n",
    "                                                                              'Gained TFBS start coordinate (in allele)':int(row['Gained TFBS start coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'Gained TFBS end coordinate (in allele)':int(row['Gained TFBS end coordinate (in allele)'])+calculated_span_between_aligning_blocks_allele,\n",
    "                                                                              'p-val':row['p-val'], 'lost or gained in allele (relative to ref)?':row['lost or gained in allele (relative to ref)?'], 'comment':row['comment']}\n",
    "                span_between_aligning_blocks_allele_list.append(span_between_aligning_blocks_allele)\n",
    "\n",
    "allele_TFBS_synopsis_df_coordinates_updated['span between alignment blocks'] = span_between_aligning_blocks_allele_list\n",
    "                \n",
    "\n",
    "potential_matched_TFBS_pairs_list = []\n",
    "\n",
    "for i in lost_TFBS_list:\n",
    "    for x in gained_TFBS_list:\n",
    "        if x.split(',')[4] == 'n/a' or x.split(',')[5] == 'n/a':\n",
    "            pass\n",
    "        else:\n",
    "            if i.split(',')[:4] == x.split(',')[:4]:\n",
    "                lost_range = range(int(i.split(',')[4]), int(i.split(',')[5])+1)\n",
    "                gained_range = range(int(x.split(',')[4]), int(x.split(',')[5])+1)\n",
    "                if len(set(lost_range) & set(gained_range)) > 0:\n",
    "                    potential_matched_TFBS_pairs_list.append((i,x))\n",
    "                \n",
    "unpaired_TFBS_gains_list = list(set(gained_TFBS_list) - \n",
    "                                set([i[0] for i in potential_matched_TFBS_pairs_list]+[i[1] for i in potential_matched_TFBS_pairs_list]))\n",
    "unpaired_TFBS_losses_list = list(set(lost_TFBS_list) - \n",
    "                                set([i[0] for i in potential_matched_TFBS_pairs_list]+[i[1] for i in potential_matched_TFBS_pairs_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 2021\n",
    "# Reconstitute data for categories ('no TFBS predicted as lost or gained in allele', 'predicted TFBS loss (TFBS lost in allele)',\n",
    "# 'predicted TFBS gain (novel to allele)')\n",
    "sample_list = []\n",
    "allele_rank_list = []\n",
    "allele_list = []\n",
    "read_count_list = []\n",
    "total_reads_count_list = []\n",
    "pct_total_reads_list = []\n",
    "pct_reads_filtered_for_1pct_list = []\n",
    "pct_reads_filtered_for_10pct_list = []\n",
    "allele_sequence_list = []\n",
    "reference_sequence_list = []\n",
    "alignment_midline_list = []\n",
    "TF_list = []\n",
    "strand_list = []\n",
    "lost_TFBS_sequence_list = []\n",
    "gained_TFBS_sequence_list = []\n",
    "allele_start_coordinate_list = []\n",
    "allele_stop_coordinate_list = []\n",
    "ref_start_coordinate_list = []\n",
    "ref_stop_coordinate_list = []\n",
    "lost_TFBS_pval_list = []\n",
    "gained_TFBS_pval_list = []\n",
    "predicted_lost_gained_pair_list = []\n",
    "\n",
    "for index, row in allele_TFBS_synopsis_df_coordinates_updated.iterrows():\n",
    "    if row['lost or gained in allele (relative to ref)?'] == 'lost':\n",
    "        lost_test_phrase = ''.join(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Lost TFBS start coordinate (in reference)'])+','+str(row['Lost TFBS end coordinate (in reference)'])+\n",
    "                              ','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "        if lost_test_phrase in set([i[0] for i in potential_matched_TFBS_pairs_list]+[i[1] for i in potential_matched_TFBS_pairs_list]):\n",
    "            for match_pair in potential_matched_TFBS_pairs_list:\n",
    "                if lost_test_phrase in match_pair:\n",
    "                    sample_list.append(row['sample'])\n",
    "                    allele_rank_list.append(row['allele rank'])\n",
    "                    allele_list.append(row['allele ID'])\n",
    "                    read_count_list.append(row['reads'])\n",
    "                    total_reads_count_list.append(row['total reads'])\n",
    "                    pct_total_reads_list.append(row['% total reads'])\n",
    "                    pct_reads_filtered_for_1pct_list.append(row['% reads filtered for reads <1%'])\n",
    "                    pct_reads_filtered_for_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                    allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                    reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                    alignment_midline_list.append(row['alignment midline'])\n",
    "                    TF_list.append(row['TF'])\n",
    "                    strand_list.append(row['strand'])\n",
    "                    lost_TFBS_sequence_list.append(match_pair[0].split(',')[7])\n",
    "                    ref_start_coordinate_list.append(match_pair[0].split(',')[4])\n",
    "                    ref_stop_coordinate_list.append(match_pair[0].split(',')[5])\n",
    "                    gained_TFBS_sequence_list.append(match_pair[1].split(',')[7])\n",
    "                    allele_start_coordinate_list.append(match_pair[1].split(',')[4])\n",
    "                    allele_stop_coordinate_list.append(match_pair[1].split(',')[5])\n",
    "                    lost_TFBS_pval_list.append(match_pair[0].split(',')[8])\n",
    "                    gained_TFBS_pval_list.append(match_pair[1].split(',')[8])\n",
    "                    predicted_lost_gained_pair_list.append('predicted lost-regained TFBS pair')\n",
    "                else:\n",
    "                    pass\n",
    "        elif lost_test_phrase in unpaired_TFBS_losses_list:\n",
    "            sample_list.append(row['sample'])\n",
    "            allele_rank_list.append(row['allele rank'])\n",
    "            allele_list.append(row['allele ID'])\n",
    "            read_count_list.append(row['reads'])\n",
    "            total_reads_count_list.append(row['total reads'])\n",
    "            pct_total_reads_list.append(row['% total reads'])\n",
    "            pct_reads_filtered_for_1pct_list.append(row['% reads filtered for reads <1%'])\n",
    "            pct_reads_filtered_for_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "            allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "            reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "            alignment_midline_list.append(row['alignment midline'])\n",
    "            TF_list.append(row['TF'])\n",
    "            strand_list.append(row['strand'])\n",
    "            lost_TFBS_sequence_list.append(lost_test_phrase.split(',')[7])\n",
    "            ref_start_coordinate_list.append(lost_test_phrase.split(',')[4])\n",
    "            ref_stop_coordinate_list.append(lost_test_phrase.split(',')[5])\n",
    "            gained_TFBS_sequence_list.append('n/a')\n",
    "            allele_start_coordinate_list.append('n/a')\n",
    "            allele_stop_coordinate_list.append('n/a')\n",
    "            lost_TFBS_pval_list.append(lost_test_phrase.split(',')[8])\n",
    "            gained_TFBS_pval_list.append('n/a (>1e-4 threshold)')\n",
    "            predicted_lost_gained_pair_list.append('predicted TFBS loss (TFBS lost in allele)')\n",
    "    elif row['lost or gained in allele (relative to ref)?'] == 'gained':    \n",
    "        gained_test_phrase = ''.join(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Gained TFBS start coordinate (in allele)'])+','+\n",
    "                              str(row['Gained TFBS end coordinate (in allele)'])+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "        if gained_test_phrase in unpaired_TFBS_gains_list:\n",
    "            sample_list.append(row['sample'])\n",
    "            allele_rank_list.append(row['allele rank'])\n",
    "            allele_list.append(row['allele ID'])\n",
    "            read_count_list.append(row['reads'])\n",
    "            total_reads_count_list.append(row['total reads'])\n",
    "            pct_total_reads_list.append(row['% total reads'])\n",
    "            pct_reads_filtered_for_1pct_list.append(row['% reads filtered for reads <1%'])\n",
    "            pct_reads_filtered_for_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "            allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "            reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "            alignment_midline_list.append(row['alignment midline'])\n",
    "            TF_list.append(row['TF'])\n",
    "            strand_list.append(row['strand'])\n",
    "            lost_TFBS_sequence_list.append('n/a')\n",
    "            ref_start_coordinate_list.append('n/a')\n",
    "            ref_stop_coordinate_list.append('n/a')\n",
    "            gained_TFBS_sequence_list.append(gained_test_phrase.split(',')[7])\n",
    "            allele_start_coordinate_list.append(gained_test_phrase.split(',')[4])\n",
    "            allele_stop_coordinate_list.append(gained_test_phrase.split(',')[5])\n",
    "            lost_TFBS_pval_list.append('n/a (>1e-4 threshold)')\n",
    "            gained_TFBS_pval_list.append(gained_test_phrase.split(',')[8])\n",
    "            predicted_lost_gained_pair_list.append('predicted TFBS gain (novel to allele)')\n",
    "    elif row['lost or gained in allele (relative to ref)?'] == 'No TFBS gained':\n",
    "        lost_test_phrase = ''.join(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Lost TFBS start coordinate (in reference)'])+','+str(row['Lost TFBS end coordinate (in reference)'])+\n",
    "                              ','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val'])\n",
    "        if lost_test_phrase in unpaired_TFBS_losses_list:\n",
    "            pass\n",
    "        else:\n",
    "            if row['allele ID'] in allele_list:\n",
    "                pass\n",
    "            else:\n",
    "                sample_list.append(row['sample'])\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "                allele_list.append(row['allele ID'])\n",
    "                read_count_list.append(row['reads'])\n",
    "                total_reads_count_list.append(row['total reads'])\n",
    "                pct_total_reads_list.append(row['% total reads'])\n",
    "                pct_reads_filtered_for_1pct_list.append(row['% reads filtered for reads <1%'])\n",
    "                pct_reads_filtered_for_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                alignment_midline_list.append(row['alignment midline'])\n",
    "                TF_list.append(row['TF'])\n",
    "                strand_list.append(row['strand'])\n",
    "                lost_TFBS_sequence_list.append('n/a')\n",
    "                ref_start_coordinate_list.append('n/a')\n",
    "                ref_stop_coordinate_list.append('n/a')\n",
    "                gained_TFBS_sequence_list.append('n/a')\n",
    "                allele_start_coordinate_list.append('n/a')\n",
    "                allele_stop_coordinate_list.append('n/a')\n",
    "                lost_TFBS_pval_list.append('n/a')\n",
    "                gained_TFBS_pval_list.append('n/a')\n",
    "                predicted_lost_gained_pair_list.append('no TFBS predicted as lost or gained in allele')\n",
    "    elif row['lost or gained in allele (relative to ref)?'] == 'No TFBS lost':\n",
    "        gained_test_phrase = ''.join(row['sample']+','+str(row['allele rank'])+','+row['TF']+','+row['strand']+','+\n",
    "                              str(row['Gained TFBS start coordinate (in allele)'])+','+\n",
    "                              str(row['Gained TFBS end coordinate (in allele)'])+','+row['lost or gained in allele (relative to ref)?']+','\n",
    "                              +row[\"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\"]+\n",
    "                             ','+row['p-val']) \n",
    "        if gained_test_phrase in unpaired_TFBS_gains_list:\n",
    "            if row['allele ID'] in allele_list:\n",
    "                pass\n",
    "            else:\n",
    "                sample_list.append(row['sample'])\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "                allele_list.append(row['allele ID'])\n",
    "                read_count_list.append(row['reads'])\n",
    "                total_reads_count_list.append(row['total reads'])\n",
    "                pct_total_reads_list.append(row['% total reads'])\n",
    "                pct_reads_filtered_for_1pct_list.append(row['% reads filtered for reads <1%'])\n",
    "                pct_reads_filtered_for_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                alignment_midline_list.append(row['alignment midline'])\n",
    "                TF_list.append(row['TF'])\n",
    "                strand_list.append(row['strand'])\n",
    "                lost_TFBS_sequence_list.append('n/a')\n",
    "                ref_start_coordinate_list.append('n/a')\n",
    "                ref_stop_coordinate_list.append('n/a')\n",
    "                gained_TFBS_sequence_list.append('n/a')\n",
    "                allele_start_coordinate_list.append('n/a')\n",
    "                allele_stop_coordinate_list.append('n/a')\n",
    "                lost_TFBS_pval_list.append('n/a')\n",
    "                gained_TFBS_pval_list.append('n/a')\n",
    "                predicted_lost_gained_pair_list.append('no TFBS predicted as lost or gained in allele')\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*interpreted_TFBS_synopsis_df*: dataframe synopsis of TFBS interpretations as isolated 'lost' or 'gained' instances relative to reference sequence, vs. positionally overlapping 'lost-regained' pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe synopsis of samples and alleles, with individual rows mapping potential lost-regained TFBS pairs for in-common TFs\n",
    "interpreted_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                        'read count':read_count_list,\n",
    "                                        'total reads':total_reads_count_list,\n",
    "                                        '% total reads':pct_total_reads_list,\n",
    "                                        '% reads filtered for reads <1%':pct_reads_filtered_for_1pct_list,\n",
    "                                        '% reads filtered for reads <10%':pct_reads_filtered_for_10pct_list,\n",
    "                                        \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                        \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                        \"TF\":TF_list,\n",
    "                                        \"strand\":strand_list, \n",
    "                                        \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                        \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":gained_TFBS_sequence_list, \n",
    "                                        \"Lost TFBS coordinate start (in reference)\":ref_start_coordinate_list,\n",
    "                                        \"Lost TFBS coordinate end (in reference)\":ref_stop_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate start (in allele)\":allele_start_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate end (in allele)\":allele_stop_coordinate_list,\n",
    "                                        \"Lost TFBS p-val (in reference)\":lost_TFBS_pval_list,\n",
    "                                        \"Gained TBFS p-val (in allele)\":gained_TFBS_pval_list,\n",
    "                                        \"interpretation\":predicted_lost_gained_pair_list}\n",
    "\n",
    "interpreted_TFBS_synopsis_df = pd.DataFrame(interpreted_TFBS_synopsis_df_columns)\n",
    "\n",
    "# Add column with allele comment (comment if appropriate)\n",
    "interpreted_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in interpreted_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "interpreted_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"interpretation\"],ascending=[True, True, True, True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele_rank_count_list = []\n",
    "sample_set = set(interpreted_TFBS_synopsis_df['sample'].to_list())\n",
    "\n",
    "for sample in sample_set:\n",
    "    allele_rank_list = []\n",
    "    for index, row in interpreted_TFBS_synopsis_df.iterrows():\n",
    "        if row['sample'] == sample:\n",
    "            if row['allele rank'] not in allele_rank_list:\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "    allele_rank_count_list.append((sample, sorted(allele_rank_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up interpreted_TFBS_synopsis_df; some sample alleles have been assigned rows that include the\n",
    "# label 'no TFBS predicted as lost or gained in allele', because of order of operations above,\n",
    "# but in fact have TFBS(s) predicted as lost or gained; find and remove these rows from interpreted_TFBS_synopsis_df\n",
    "# (in new dataframe called interpreted_TFBS_syopsis_df_updated)\n",
    "suspects = []\n",
    "for sample in allele_rank_count_list:\n",
    "    for allele_rank in sample[1]:\n",
    "        for index1, row in interpreted_TFBS_synopsis_df.iterrows():\n",
    "            if row['sample'] == sample[0] and row['allele rank'] == allele_rank and row['interpretation'] == 'no TFBS predicted as lost or gained in allele':\n",
    "                test_singularity = True\n",
    "                for index, row in interpreted_TFBS_synopsis_df.iterrows():\n",
    "                    if row['sample'] == sample[0] and row['allele rank'] == allele_rank and row['interpretation'] != 'no TFBS predicted as lost or gained in allele':\n",
    "                        test_singularity = False\n",
    "                if test_singularity == True:\n",
    "                    pass\n",
    "                elif test_singularity == False:\n",
    "                    suspects.append((sample[0], allele_rank, index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_drop_list = [i[2] for i in suspects]\n",
    "interpreted_TFBS_synopsis_df_updated = interpreted_TFBS_synopsis_df.drop(index_drop_list)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**: if TF of interest was specified as input, the script will now query lost-gained outputs for the specific TF of interest, to interpret the following properties: **(1) lost without corresponding regain**, **(2) lost without corresponding regain *and* without positionally coinciding TFBS for new TF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, query output for specific TF of interest, based on the following properties:\n",
    "# (1) lost without corresponding re-gain, (2) lost without corresponding re-gain and without positionally coinciding TFBS for new TF\n",
    "# TF of interest was provided by user at script outset (encoded by variable samples_list = set(interpreted_TFBS_synopsis_df['sample'].to_list())\n",
    "\n",
    "samples_list = set(interpreted_TFBS_synopsis_df['sample'].to_list())\n",
    "                   \n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    interpretation_dict = {}\n",
    "    for sample in samples_list:\n",
    "        sample_interpretation_dict = {}\n",
    "        for index, row in interpreted_TFBS_synopsis_df_updated.sort_values(by=['sample','allele rank','TF',\"strand\",\"interpretation\"],ascending=[True, True, True, True, False]).iterrows():\n",
    "            if row['sample'] == sample:\n",
    "                if row['interpretation'] == \"no TFBS predicted as lost or gained in allele\":\n",
    "                    sample_interpretation_dict[row['allele rank']] = [row.to_list()]  \n",
    "                elif re.search(r'\\b'+TF_of_interest+r'\\b', row['TF']) and row['interpretation'] == 'predicted TFBS loss (TFBS lost in allele)':\n",
    "                    if row['allele rank'] not in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']] = [row.to_list()]\n",
    "                    elif row['allele rank'] in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']].append(row.to_list())\n",
    "                elif re.search(r'\\b'+TF_of_interest+r'\\b', row['TF']) and row['interpretation'] == 'predicted TFBS gain (novel to allele)':\n",
    "                    if row['allele rank'] not in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']] = [row.to_list()]\n",
    "                    elif row['allele rank'] in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']].append(row.to_list())\n",
    "                elif re.search(r'\\b'+TF_of_interest+r'\\b', row['TF']) and row['interpretation'] == 'predicted lost-regained TFBS pair':\n",
    "                    if row['allele rank'] not in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']] = [row.to_list()]\n",
    "                    elif row['allele rank'] in sample_interpretation_dict:\n",
    "                        sample_interpretation_dict[row['allele rank']].append(row.to_list())         \n",
    "        interpretation_dict[sample] = sample_interpretation_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 2021\n",
    "# Now assess potential samples of particular interest based on 2 criteria above\n",
    "# Iterate through alleles to bin alleles for samples among the indicated lists below\n",
    "# (1) lost without corresponding gain, (2) lost without corresponding gain and without positionally coinciding TFBS for new TF\n",
    "# for (2), check for 'predicted TFBS gain (novel to allele)' that coincides with position of TFBS loss for TF of interest\n",
    "\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:               \n",
    "    predicted_loss_of_target_TFBS_list = []\n",
    "    predicted_exclusive_loss_of_target_TFBS_list = []\n",
    "    predicted_loss_with_regain_of_different_TFBS_for_same_TF_list = []\n",
    "    predicted_loss_with_gain_of_different_TFBS_list = []\n",
    "\n",
    "    for sample in interpretation_dict:\n",
    "        for allele in interpretation_dict.get(sample):\n",
    "            for instance in interpretation_dict.get(sample).get(allele):\n",
    "                # loss with corresponding \"re-gain\" of 'replacement' TFBS for TF of interest\n",
    "                if instance[21] == 'predicted lost-regained TFBS pair':\n",
    "                    predicted_loss_with_regain_of_different_TFBS_for_same_TF_list.append(instance)\n",
    "                # loss without corresponding gain of 'replacement' TFBS for TF of interest\n",
    "                elif instance[21] == 'predicted TFBS loss (TFBS lost in allele)':\n",
    "                    predicted_loss_of_target_TFBS_list.append((sample,allele,instance))\n",
    "                    exclusive_loss_check = True\n",
    "                # further filter, for loss without corresponding gain of 'replacement' TFBS for TF of interest & no predicted positionally coinciding novel TFBS\n",
    "                    # consider two coordinate ranges to check (regarding losses):\n",
    "                    # first, check for allele's span between alignment blocks\n",
    "                    span_between_alignment_blocks = allele_TFBS_synopsis_df_coordinates_updated.loc[\n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['sample'] == sample) & \n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['allele rank'] == allele) &\n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['strand'] == instance[12]) &\n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['TF'].str.match(r'\\b'+TF_of_interest+r'\\b')) &\n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['Lost TFBS start coordinate (in reference)'] == int(instance[15])) &\n",
    "                        (allele_TFBS_synopsis_df_coordinates_updated['Lost TFBS end coordinate (in reference)'] == int(instance[16]))]['span between alignment blocks'].values[0]\n",
    "                    if span_between_alignment_blocks != 'n/a':\n",
    "                        for index, row in interpreted_TFBS_synopsis_df_updated.iterrows():\n",
    "                            if row['sample'] == sample and row['allele rank'] == allele and row['interpretation'] == 'predicted TFBS gain (novel to allele)':\n",
    "                                if set(range(int(row['Gained TFBS coordinate start (in allele)']),int(row['Gained TFBS coordinate end (in allele)']))).intersection(range(span_between_alignment_blocks[0],span_between_alignment_blocks[1])):\n",
    "                                    predicted_loss_with_gain_of_different_TFBS_list.append(((sample,allele,instance, row.to_list())))\n",
    "                                    exclusive_loss_check = False\n",
    "                    else:\n",
    "                        coordinate_range_to_check = range(int(interpreted_TFBS_synopsis_df_updated.loc[(interpreted_TFBS_synopsis_df_updated['sample'] == sample) &\n",
    "                                                 (interpreted_TFBS_synopsis_df_updated['allele rank'] == allele) &\n",
    "                                                  (interpreted_TFBS_synopsis_df_updated['strand'] == instance[12]) &\n",
    "                                                 (interpreted_TFBS_synopsis_df_updated['interpretation'] == 'predicted TFBS loss (TFBS lost in allele)') &\n",
    "                                                (interpreted_TFBS_synopsis_df_updated['TF'].str.match(r'\\b'+TF_of_interest+r'\\b'))]['Lost TFBS coordinate start (in reference)'].values[0]),\n",
    "                                                  int(interpreted_TFBS_synopsis_df_updated.loc[(interpreted_TFBS_synopsis_df_updated['sample'] == sample) &\n",
    "                                                 (interpreted_TFBS_synopsis_df_updated['allele rank'] == allele) &\n",
    "                                                  (interpreted_TFBS_synopsis_df_updated['strand'] == instance[12]) &\n",
    "                                                 (interpreted_TFBS_synopsis_df_updated['interpretation'] == 'predicted TFBS loss (TFBS lost in allele)') &\n",
    "                                                (interpreted_TFBS_synopsis_df_updated['TF'].str.match(r'\\b'+TF_of_interest+r'\\b'))]['Lost TFBS coordinate end (in reference)'].values[0]))\n",
    "                        for index, row in interpreted_TFBS_synopsis_df_updated.iterrows():\n",
    "                            if row['sample'] == sample and row['allele rank'] == allele and row['interpretation'] == 'predicted TFBS gain (novel to allele)':\n",
    "                                if set(coordinate_range_to_check).intersection(range(int(row['Gained TFBS coordinate start (in allele)']), \n",
    "                                                                             int(row['Gained TFBS coordinate end (in allele)']))):\n",
    "                                    predicted_loss_with_gain_of_different_TFBS_list.append(((sample,allele,instance, row.to_list())))\n",
    "                                    exclusive_loss_check = False\n",
    "                    if exclusive_loss_check == True:\n",
    "                        predicted_exclusive_loss_of_target_TFBS_list.append((sample,allele,instance))\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*predicted_loss_of_TFBS_synopsis_df*: dataframe that catalogs sample alleles identified as having lost TFBS for TF of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output to indicate **loss of TFBS for TF of interest**, without regain of different TFBS for same TF\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    sample_list = []\n",
    "    allele_rank_list = []\n",
    "    allele_list = []\n",
    "    read_count_list = []\n",
    "    total_reads_count_list = []\n",
    "    pct_total_reads_list = []\n",
    "    pct_reads_filtered_for_1pct_list = []\n",
    "    pct_reads_filtered_for_10pct_list = []\n",
    "    allele_sequence_list = []\n",
    "    reference_sequence_list = []\n",
    "    alignment_midline_list = []\n",
    "    TF_lost_list = []\n",
    "    TF_lost_strand_list = []\n",
    "    lost_TFBS_sequence_list = []\n",
    "    lost_TFBS_start_coordinate_list = []\n",
    "    lost_TFBS_end_coordinate_list = []\n",
    "    lost_TFBS_pval_list = []\n",
    "\n",
    "    for pair in predicted_loss_of_target_TFBS_list:\n",
    "        sample_list.append(pair[0])\n",
    "        allele_rank_list.append(pair[1])\n",
    "        allele_list.append(pair[2][2])\n",
    "        read_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[0])\n",
    "        total_reads_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[1])\n",
    "        pct_total_reads_list.append(pair[2][5])\n",
    "        pct_reads_filtered_for_1pct_list.append(pair[2][6])\n",
    "        pct_reads_filtered_for_10pct_list.append(pair[2][7])\n",
    "        allele_sequence_list.append(pair[2][8])\n",
    "        reference_sequence_list.append(pair[2][10])\n",
    "        alignment_midline_list.append(pair[2][9])\n",
    "        TF_lost_list.append(pair[2][11])\n",
    "        TF_lost_strand_list.append(pair[2][12])\n",
    "        lost_TFBS_sequence_list.append(pair[2][13])\n",
    "        lost_TFBS_start_coordinate_list.append(pair[2][15])\n",
    "        lost_TFBS_end_coordinate_list.append(pair[2][16])\n",
    "        lost_TFBS_pval_list.append(pair[2][19])\n",
    "        \n",
    "# create dataframe for predicted_loss_of_TFBS_synopsis\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    predicted_loss_of_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                        'read count':read_count_list,\n",
    "                                        'total reads':total_reads_count_list,\n",
    "                                        '% total reads':pct_total_reads_list,\n",
    "                                        '% reads filtered for reads <1%':pct_reads_filtered_for_1pct_list,\n",
    "                                        '% reads filtered for reads <10%':pct_reads_filtered_for_10pct_list,\n",
    "                                        \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                        \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                        \"TF lost\":TF_lost_list,                  \n",
    "                                        \"TF lost strand\":TF_lost_strand_list,                            \n",
    "                                        \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                        \"Lost TFBS coordinate start (in reference)\":lost_TFBS_start_coordinate_list,\n",
    "                                        \"Lost TFBS coordinate end (in reference)\":lost_TFBS_end_coordinate_list,\n",
    "                                        \"Lost TFBS p-val (in reference)\":lost_TFBS_pval_list}\n",
    "    predicted_loss_of_TFBS_synopsis_df = pd.DataFrame(predicted_loss_of_TFBS_synopsis_df_columns)\n",
    "\n",
    "    # Add column with allele comment (comment if appropriate)\n",
    "    predicted_loss_of_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in predicted_loss_of_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "    predicted_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df*: dataframe that catalogs sample alleles identified as having lost TFBS for TF of interest, but the genetic variant nevertheless regained a distinct TFBS for the same TF of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output to indicate **loss of TFBS for TF of interest with regain of different TFBS for same TF**\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    sample_list = []\n",
    "    allele_rank_list = []\n",
    "    allele_list = []\n",
    "    read_count_list = []\n",
    "    total_reads_count_list = []\n",
    "    pct_total_reads_list = []\n",
    "    pct_reads_filtered_for_1pct_list = []\n",
    "    pct_reads_filtered_for_10pct_list = []\n",
    "    allele_sequence_list = []\n",
    "    reference_sequence_list = []\n",
    "    alignment_midline_list = []\n",
    "    TF_lost_list = []\n",
    "    TF_lost_strand_list = []\n",
    "    lost_TFBS_sequence_list = []\n",
    "    lost_TFBS_start_coordinate_list = []\n",
    "    lost_TFBS_end_coordinate_list = []\n",
    "    lost_TFBS_pval_list = []\n",
    "    TF_gained_list = []\n",
    "    TF_gained_strand_list = []\n",
    "    gained_TFBS_sequence_list = []\n",
    "    gained_TFBS_start_coordinate_list = []\n",
    "    gained_TFBS_end_coordinate_list = []\n",
    "    gained_TFBS_pval_list = []\n",
    "\n",
    "    for allele in predicted_loss_with_regain_of_different_TFBS_for_same_TF_list:\n",
    "        sample_list.append(allele[0])\n",
    "        allele_rank_list.append(allele[1])\n",
    "        allele_list.append(allele[2])\n",
    "        read_count_list.append(allele[2].split('_')[2].strip('[]').split('/')[0])\n",
    "        total_reads_count_list.append(allele[2].split('_')[2].strip('[]').split('/')[1])\n",
    "        pct_total_reads_list.append(allele[5])\n",
    "        pct_reads_filtered_for_1pct_list.append(allele[6])\n",
    "        pct_reads_filtered_for_10pct_list.append(allele[7])\n",
    "        allele_sequence_list.append(allele[8])\n",
    "        reference_sequence_list.append(allele[10])\n",
    "        alignment_midline_list.append(allele[9])\n",
    "        TF_lost_list.append(allele[11])\n",
    "        TF_lost_strand_list.append(allele[12])\n",
    "        lost_TFBS_sequence_list.append(allele[13])\n",
    "        lost_TFBS_start_coordinate_list.append(allele[15])\n",
    "        lost_TFBS_end_coordinate_list.append(allele[16])\n",
    "        lost_TFBS_pval_list.append(allele[19])\n",
    "        TF_gained_list.append(allele[11])\n",
    "        TF_gained_strand_list.append(allele[12])\n",
    "        gained_TFBS_sequence_list.append(allele[14])\n",
    "        gained_TFBS_start_coordinate_list.append(allele[17])\n",
    "        gained_TFBS_end_coordinate_list.append(allele[18])\n",
    "        gained_TFBS_pval_list.append(allele[20])\n",
    "        \n",
    "# create dataframe for predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                        'read count':read_count_list,\n",
    "                                        'total reads':total_reads_count_list,\n",
    "                                        '% total reads':pct_total_reads_list,\n",
    "                                        '% reads filtered for reads <1%':pct_reads_filtered_for_1pct_list,\n",
    "                                        '% reads filtered for reads <10%':pct_reads_filtered_for_10pct_list,\n",
    "                                        \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                        \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                        \"TF lost\":TF_lost_list,\n",
    "                                        \"TF gained\":TF_gained_list,                    \n",
    "                                        \"TF lost strand\":TF_lost_strand_list,\n",
    "                                        \"TF gained strand\":TF_gained_strand_list,                        \n",
    "                                        \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                        \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":gained_TFBS_sequence_list, \n",
    "                                        \"Lost TFBS coordinate start (in reference)\":lost_TFBS_start_coordinate_list,\n",
    "                                        \"Lost TFBS coordinate end (in reference)\":lost_TFBS_end_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate start (in allele)\":gained_TFBS_start_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate end (in allele)\":gained_TFBS_end_coordinate_list,\n",
    "                                        \"Lost TFBS p-val (in reference)\":lost_TFBS_pval_list,\n",
    "                                        \"Gained TBFS p-val (in allele)\":gained_TFBS_pval_list}\n",
    "\n",
    "    predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df = pd.DataFrame(predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df_columns)\n",
    "\n",
    "    # Add column with allele comment (comment if appropriate)\n",
    "    predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "    predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*predicted_loss_with_gain_of_different_TFBS_synopsis_df*: dataframe that catalogs sample alleles identified as having lost TFBS for TF of interest, but the genetic variant gained a distinct TFBS for an entirely different TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output to indicate **loss of TFBS for TF of interest with gain of TFBS for novel TF**\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    sample_list = []\n",
    "    allele_rank_list = []\n",
    "    allele_list = []\n",
    "    read_count_list = []\n",
    "    total_reads_count_list = []\n",
    "    pct_total_reads_list = []\n",
    "    pct_reads_filtered_for_1pct_list = []\n",
    "    pct_reads_filtered_for_10pct_list = []\n",
    "    allele_sequence_list = []\n",
    "    reference_sequence_list = []\n",
    "    alignment_midline_list = []\n",
    "    TF_lost_list = []\n",
    "    TF_lost_strand_list = []\n",
    "    lost_TFBS_sequence_list = []\n",
    "    lost_TFBS_start_coordinate_list = []\n",
    "    lost_TFBS_end_coordinate_list = []\n",
    "    lost_TFBS_pval_list = []\n",
    "    TF_gained_list = []\n",
    "    TF_gained_strand_list = []\n",
    "    gained_TFBS_sequence_list = []\n",
    "    gained_TFBS_start_coordinate_list = []\n",
    "    gained_TFBS_end_coordinate_list = []\n",
    "    gained_TFBS_pval_list = []\n",
    "\n",
    "    for pair in predicted_loss_with_gain_of_different_TFBS_list:\n",
    "        sample_list.append(pair[0])\n",
    "        allele_rank_list.append(pair[1])\n",
    "        allele_list.append(pair[2][2])\n",
    "        read_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[0])\n",
    "        total_reads_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[1])\n",
    "        pct_total_reads_list.append(pair[2][5])\n",
    "        pct_reads_filtered_for_1pct_list.append(pair[2][6])\n",
    "        pct_reads_filtered_for_10pct_list.append(pair[2][7])\n",
    "        allele_sequence_list.append(pair[2][8])\n",
    "        reference_sequence_list.append(pair[2][10])\n",
    "        alignment_midline_list.append(pair[2][9])\n",
    "        TF_lost_list.append(pair[2][11])\n",
    "        TF_lost_strand_list.append(pair[2][12])\n",
    "        lost_TFBS_sequence_list.append(pair[2][13])\n",
    "        lost_TFBS_start_coordinate_list.append(pair[2][15])\n",
    "        lost_TFBS_end_coordinate_list.append(pair[2][16])\n",
    "        lost_TFBS_pval_list.append(pair[2][19])\n",
    "        TF_gained_list.append(pair[3][11])\n",
    "        TF_gained_strand_list.append(pair[3][12])\n",
    "        gained_TFBS_sequence_list.append(pair[3][14])\n",
    "        gained_TFBS_start_coordinate_list.append(pair[3][17])\n",
    "        gained_TFBS_end_coordinate_list.append(pair[3][18])\n",
    "        gained_TFBS_pval_list.append(pair[3][20])\n",
    "        \n",
    "# create dataframe for predicted_loss_with_gain_of_different_TFBS_synopsis\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    predicted_loss_with_gain_of_different_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                        'read count':read_count_list,\n",
    "                                        'total reads':total_reads_count_list,\n",
    "                                        '% total reads':pct_total_reads_list,\n",
    "                                        '% reads filtered for reads <1%':pct_reads_filtered_for_1pct_list,\n",
    "                                        '% reads filtered for reads <10%':pct_reads_filtered_for_10pct_list,\n",
    "                                        \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                        \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                        \"TF lost\":TF_lost_list,\n",
    "                                        \"TF gained\":TF_gained_list,                    \n",
    "                                        \"TF lost strand\":TF_lost_strand_list,\n",
    "                                        \"TF gained strand\":TF_gained_strand_list,                        \n",
    "                                        \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                        \"Gained TFBS sequence (not in reference at this position, novel to allele)\\n*Note: this TFBS sequence is in the allele, 5'-3' on strand indicated in 'strand'\":gained_TFBS_sequence_list, \n",
    "                                        \"Lost TFBS coordinate start (in reference)\":lost_TFBS_start_coordinate_list,\n",
    "                                        \"Lost TFBS coordinate end (in reference)\":lost_TFBS_end_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate start (in allele)\":gained_TFBS_start_coordinate_list,\n",
    "                                        \"Gained TFBS coordinate end (in allele)\":gained_TFBS_end_coordinate_list,\n",
    "                                        \"Lost TFBS p-val (in reference)\":lost_TFBS_pval_list,\n",
    "                                        \"Gained TBFS p-val (in allele)\":gained_TFBS_pval_list}\n",
    "\n",
    "    predicted_loss_with_gain_of_different_TFBS_synopsis_df = pd.DataFrame(predicted_loss_with_gain_of_different_TFBS_synopsis_df_columns)\n",
    "\n",
    "    # Add column with allele comment (comment if appropriate)\n",
    "    predicted_loss_with_gain_of_different_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in predicted_loss_with_gain_of_different_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "    predicted_loss_with_gain_of_different_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*predicted_exclusive_loss_of_TFBS_synopsis_df*: dataframe that catalogs sample alleles identified as having exclusively lost TFBS for TF of interest (in other words, the genetic variant led to loss of TFBS for TF of interest, and FIMO did not identify a coinciding 'regained' TFBS for the TF of interest or a 'gained' TFBS for a different TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output to indicate **loss of TFBS for TF of interest with no predicted gain of TFBS for novel TF** (at pval threshold)\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    sample_list = []\n",
    "    allele_rank_list = []\n",
    "    allele_list = []\n",
    "    read_count_list = []\n",
    "    total_reads_count_list = []\n",
    "    pct_total_reads_list = []\n",
    "    pct_reads_filtered_for_1pct_list = []\n",
    "    pct_reads_filtered_for_10pct_list = []\n",
    "    allele_sequence_list = []\n",
    "    reference_sequence_list = []\n",
    "    alignment_midline_list = []\n",
    "    TF_lost_list = []\n",
    "    TF_lost_strand_list = []\n",
    "    lost_TFBS_sequence_list = []\n",
    "    lost_TFBS_start_coordinate_list = []\n",
    "    lost_TFBS_end_coordinate_list = []\n",
    "    lost_TFBS_pval_list = []\n",
    "    TF_gained_list = []\n",
    "    TF_gained_strand_list = []\n",
    "    gained_TFBS_sequence_list = []\n",
    "    gained_TFBS_start_coordinate_list = []\n",
    "    gained_TFBS_end_coordinate_list = []\n",
    "    gained_TFBS_pval_list = []\n",
    "\n",
    "    for pair in predicted_exclusive_loss_of_target_TFBS_list:\n",
    "        sample_list.append(pair[0])\n",
    "        allele_rank_list.append(pair[1])\n",
    "        allele_list.append(pair[2][2])\n",
    "        read_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[0])\n",
    "        total_reads_count_list.append(pair[2][2].split('_')[2].strip('[]').split('/')[1])\n",
    "        pct_total_reads_list.append(pair[2][5])\n",
    "        pct_reads_filtered_for_1pct_list.append(pair[2][6])\n",
    "        pct_reads_filtered_for_10pct_list.append(pair[2][7])\n",
    "        allele_sequence_list.append(pair[2][8])\n",
    "        reference_sequence_list.append(pair[2][10])\n",
    "        alignment_midline_list.append(pair[2][9])\n",
    "        TF_lost_list.append(pair[2][11])\n",
    "        TF_lost_strand_list.append(pair[2][12])\n",
    "        lost_TFBS_sequence_list.append(pair[2][13])\n",
    "        lost_TFBS_start_coordinate_list.append(pair[2][15])\n",
    "        lost_TFBS_end_coordinate_list.append(pair[2][16])\n",
    "        lost_TFBS_pval_list.append(pair[2][19])\n",
    "        \n",
    "# create dataframe for predicted_exclusive_loss_of_TFBS_synopsis\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    predicted_exclusive_loss_of_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                        'read count':read_count_list,\n",
    "                                        'total reads':total_reads_count_list,\n",
    "                                        '% total reads':pct_total_reads_list,\n",
    "                                        '% reads filtered for reads <1%':pct_reads_filtered_for_1pct_list,\n",
    "                                        '% reads filtered for reads <10%':pct_reads_filtered_for_10pct_list,\n",
    "                                        \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                        \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                        \"TF lost\":TF_lost_list,                  \n",
    "                                        \"TF lost strand\":TF_lost_strand_list,                            \n",
    "                                        \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":lost_TFBS_sequence_list,\n",
    "                                        \"Lost TFBS coordinate start (in reference)\":lost_TFBS_start_coordinate_list,\n",
    "                                        \"Lost TFBS coordinate end (in reference)\":lost_TFBS_end_coordinate_list,\n",
    "                                        \"Lost TFBS p-val (in reference)\":lost_TFBS_pval_list}\n",
    "    predicted_exclusive_loss_of_TFBS_synopsis_df = pd.DataFrame(predicted_exclusive_loss_of_TFBS_synopsis_df_columns)\n",
    "\n",
    "    # Add column with allele comment (comment if appropriate)\n",
    "    predicted_exclusive_loss_of_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in predicted_exclusive_loss_of_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "    predicted_exclusive_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*samples_predicted_to_have_lost_TFBS_synopsis_df*: dataframe that catalogs all ranked alleles for samples with at least one ranked allele identified as having lost TFBS for TF of interest; this dataframe becomes the basis for sample curation to facilitate identification of samples of potential experimental interest (for having lost TFBS for TF of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 2021\n",
    "# Take stock of samples with alleles having lost TFBS for TF of interest without regain/gain of TFBS for distinct TF\n",
    "# Re-populate remaining ranked alleles for these samples, to facilitate genotype inference\n",
    "if TF_of_interest == '':\n",
    "    pass\n",
    "else:\n",
    "    sample_list = []\n",
    "    allele_rank_list = []\n",
    "    allele_list = []\n",
    "    read_count_list = []\n",
    "    total_reads_list = []\n",
    "    total_reads_pct_list = []\n",
    "    total_reads_1pct_list = []\n",
    "    total_reads_10pct_list = []\n",
    "    allele_sequence_list = []\n",
    "    alignment_midline_list = []\n",
    "    reference_sequence_list = []\n",
    "    TF_list = []\n",
    "    TF_exlusively_lost_list = []\n",
    "    TF_lost_with_regain_of_TFBS_for_same_TF_list = []\n",
    "    TF_lost_with_gain_of_distinct_TF_list = []\n",
    "    reference_TFBS_unchanged_list = []\n",
    "    strand_list = []\n",
    "    TFBS_sequence_list = []\n",
    "    allele_start_coordinate_list = []\n",
    "    allele_stop_coordinate_list = []\n",
    "    p_val_list = []\n",
    "    comment_list = []\n",
    "\n",
    "    for sample in set(predicted_loss_of_TFBS_synopsis_df['sample'].to_list()).union(\n",
    "        set(predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df['sample'].to_list())).union(\n",
    "        set(predicted_loss_with_gain_of_different_TFBS_synopsis_df['sample'].to_list())):\n",
    "        allele_lost_TFBS_list = []\n",
    "        allele_rank_lost_TFBS_list = []\n",
    "        allele_rank_other_list = []\n",
    "        \n",
    "        for index, row in predicted_exclusive_loss_of_TFBS_synopsis_df.iterrows():\n",
    "            if row['sample'] == sample:\n",
    "                sample_list.append(row['sample'])\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "                allele_rank_lost_TFBS_list.append(row['allele rank'])\n",
    "                allele_list.append(row['allele ID'])\n",
    "                read_count_list.append(row['read count'])\n",
    "                total_reads_list.append(row['total reads'])\n",
    "                total_reads_pct_list.append(row['% total reads'])\n",
    "                total_reads_1pct_list.append(row['% reads filtered for reads <1%']) \n",
    "                total_reads_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                alignment_midline_list.append(row['alignment midline'])\n",
    "                reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                TF_list.append(row['TF lost'])\n",
    "                TF_exlusively_lost_list.append('x')\n",
    "                TF_lost_with_regain_of_TFBS_for_same_TF_list.append('')\n",
    "                TF_lost_with_gain_of_distinct_TF_list.append('')\n",
    "                reference_TFBS_unchanged_list.append('')\n",
    "                strand_list.append(row['TF lost strand']) \n",
    "                TFBS_sequence_list.append(row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"])\n",
    "                allele_start_coordinate_list.append(row['Lost TFBS coordinate start (in reference)'])\n",
    "                allele_stop_coordinate_list.append(row['Lost TFBS coordinate end (in reference)'])\n",
    "                p_val_list.append(row['Lost TFBS p-val (in reference)'])\n",
    "                comment_list.append(row['comment']) \n",
    "                allele_rank_lost_TFBS_list.append(row['allele rank'])\n",
    "                allele_lost_TFBS_list.append(row['allele ID'])\n",
    "                \n",
    "        for index, row in predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df.iterrows():\n",
    "             if row['sample'] == sample:\n",
    "                allele_rank_other_list.append(row['allele rank'])\n",
    "                allele_list.append(row['allele ID'])\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "                sample_list.append(row['sample'])\n",
    "                read_count_list.append(row['read count'])\n",
    "                total_reads_list.append(row['total reads'])\n",
    "                total_reads_pct_list.append(row['% total reads'])\n",
    "                total_reads_1pct_list.append(row['% reads filtered for reads <1%']) \n",
    "                total_reads_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                alignment_midline_list.append(row['alignment midline'])\n",
    "                reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                TF_list.append(row['TF lost'])\n",
    "                TF_exlusively_lost_list.append('')\n",
    "                TF_lost_with_regain_of_TFBS_for_same_TF_list.append('x')\n",
    "                TF_lost_with_gain_of_distinct_TF_list.append('')\n",
    "                reference_TFBS_unchanged_list.append('')\n",
    "                strand_list.append(row['TF lost strand']) \n",
    "                TFBS_sequence_list.append(row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"])\n",
    "                allele_start_coordinate_list.append(row['Lost TFBS coordinate start (in reference)'])\n",
    "                allele_stop_coordinate_list.append(row['Lost TFBS coordinate end (in reference)'])\n",
    "                p_val_list.append(row['Lost TFBS p-val (in reference)'])\n",
    "                comment_list.append(row['comment'])            \n",
    "                \n",
    "        for index, row in predicted_loss_with_gain_of_different_TFBS_synopsis_df.iterrows():\n",
    "            if row['sample'] == sample:\n",
    "                allele_rank_other_list.append(row['allele rank'])\n",
    "                allele_list.append(row['allele ID'])\n",
    "                allele_rank_list.append(row['allele rank'])\n",
    "                sample_list.append(row['sample'])\n",
    "                read_count_list.append(row['read count'])\n",
    "                total_reads_list.append(row['total reads'])\n",
    "                total_reads_pct_list.append(row['% total reads'])\n",
    "                total_reads_1pct_list.append(row['% reads filtered for reads <1%']) \n",
    "                total_reads_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                alignment_midline_list.append(row['alignment midline'])\n",
    "                reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                TF_list.append(row['TF lost'])\n",
    "                TF_exlusively_lost_list.append('')\n",
    "                TF_lost_with_regain_of_TFBS_for_same_TF_list.append('')\n",
    "                TF_lost_with_gain_of_distinct_TF_list.append('x')\n",
    "                reference_TFBS_unchanged_list.append('')\n",
    "                strand_list.append(row['TF lost strand']) \n",
    "                TFBS_sequence_list.append(row[\"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\"])\n",
    "                allele_start_coordinate_list.append(row['Lost TFBS coordinate start (in reference)'])\n",
    "                allele_stop_coordinate_list.append(row['Lost TFBS coordinate end (in reference)'])\n",
    "                p_val_list.append(row['Lost TFBS p-val (in reference)'])\n",
    "                comment_list.append(row['comment'])  \n",
    "                        \n",
    "        for index, row in allele_TFBS_synopsis_df.iterrows():\n",
    "            if row['sample'] == sample:\n",
    "                if row['allele rank'] not in allele_rank_lost_TFBS_list:\n",
    "                    if row['allele rank'] not in allele_rank_other_list:\n",
    "                        allele_rank_other_list.append(row['allele rank'])\n",
    "                        allele_list.append(row['allele ID'])\n",
    "                        allele_rank_list.append(row['allele rank'])\n",
    "                        sample_list.append(row['sample'])\n",
    "                        read_count_list.append(row['reads'])\n",
    "                        total_reads_list.append(row['total reads'])\n",
    "                        total_reads_pct_list.append(row['% total reads'])\n",
    "                        total_reads_1pct_list.append(row['% reads filtered for reads <1%']) \n",
    "                        total_reads_10pct_list.append(row['% reads filtered for reads <10%'])\n",
    "                        allele_sequence_list.append(row['alignment query\\n(allele sequence)'])\n",
    "                        alignment_midline_list.append(row['alignment midline'])\n",
    "                        reference_sequence_list.append(row['alignment hit\\n(reference)'])\n",
    "                        TF_list.append('n/a')\n",
    "                        TF_exlusively_lost_list.append('')\n",
    "                        TF_lost_with_regain_of_TFBS_for_same_TF_list.append('')\n",
    "                        TF_lost_with_gain_of_distinct_TF_list.append('')\n",
    "                        reference_TFBS_unchanged_list.append('x')\n",
    "                        strand_list.append('n/a') \n",
    "                        TFBS_sequence_list.append('n/a')\n",
    "                        allele_start_coordinate_list.append('n/a')\n",
    "                        allele_stop_coordinate_list.append('n/a')\n",
    "                        p_val_list.append('n/a')\n",
    "                        comment_list.append(row['comment'])\n",
    "    \n",
    "    samples_predicted_to_have_lost_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list,\n",
    "                                \"read count\": read_count_list, \"total reads\": total_reads_list,\n",
    "                                \"% total reads\": total_reads_pct_list, \"% reads filtered for reads <1%\": total_reads_1pct_list,\n",
    "                                \"% reads filtered for reads <10%\": total_reads_10pct_list, \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                \"TF lost (lost TFBS; no predicted regain of related for same TF, or gain of novel TFBS for distinct TF)\":TF_list,\n",
    "                                \"TFBS for TF exclusively lost\": TF_exlusively_lost_list,\n",
    "                                \"TFBS for TF lost with regain of different TFBS for same TF\": TF_lost_with_regain_of_TFBS_for_same_TF_list,\n",
    "                                \"TFBS for TF lost with gain of TFBS for different TF\": TF_lost_with_gain_of_distinct_TF_list,\n",
    "                                \"TFBS for TF unchanged relative to reference\": reference_TFBS_unchanged_list,\n",
    "                                \"TF lost strand\":strand_list, \n",
    "                                \"Lost TFBS sequence (in reference at this position, lost in allele)\\n*Note: this TFBS sequence is in the reference, 5'-3' on strand indicated in 'strand'\":TFBS_sequence_list,\n",
    "                                \"Lost TFBS coordinate start (in reference)\":allele_start_coordinate_list,\n",
    "                                \"Lost TFBS coordinate end (in reference)\":allele_stop_coordinate_list,\n",
    "                                \"Lost TFBS p-val (in reference)\":p_val_list, \"comment\":comment_list}\n",
    "\n",
    "    samples_predicted_to_have_lost_TFBS_synopsis_df = pd.DataFrame(samples_predicted_to_have_lost_TFBS_synopsis_df_columns)\n",
    "    \n",
    "    samples_predicted_to_have_lost_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost (lost TFBS; no predicted regain of related for same TF, or gain of novel TFBS for distinct TF)',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_predicted_to_have_lost_TFBS_synopsis_df.drop_duplicates(inplace=True)\n",
    "samples_predicted_to_have_lost_TFBS_synopsis_df = samples_predicted_to_have_lost_TFBS_synopsis_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF of interest**--*samples_predicted_to_have_lost_TFBS_synopsis_df*: genotype inferences (assuming diploidy for simplicity), used as a basis for flagging samples as having lost TFBS for TF of interest in high-ranking (major) alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype_interpretation_dict = {}\n",
    "for sample in set(samples_predicted_to_have_lost_TFBS_synopsis_df['sample'].to_list()):\n",
    "    allele_ranks_read_pct_list = []\n",
    "    for index, row in samples_predicted_to_have_lost_TFBS_synopsis_df.sort_values(by=['sample','allele rank']).iterrows():\n",
    "        if row['sample'] == sample:\n",
    "            allele_ranks_read_pct_list.append((row['allele rank'], row['% reads filtered for reads <10%'], row['TFBS for TF exclusively lost'], row['TFBS for TF lost with regain of different TFBS for same TF'],\n",
    "                                               row['TFBS for TF lost with gain of TFBS for different TF'], row['TFBS for TF unchanged relative to reference']))\n",
    "    for index, i in enumerate(sorted(set(allele_ranks_read_pct_list))):\n",
    "        if sample not in genotype_interpretation_dict:\n",
    "            if int(i[0]) == 1 and int(i[1]) > 90 and i[2] == 'x':\n",
    "                genotype_interpretation_dict[sample] = 'predicted homozygous loss in high-ranking allele (no regain or gain)'\n",
    "            elif int(i[0]) == 1 and int(i[1]) > 90 and i[3] == 'x':\n",
    "                genotype_interpretation_dict[sample] = 'predicted homozygous loss in high-ranking allele, with loss having regained a TFBS for TF'\n",
    "            elif int(i[0]) == 1 and int(i[1]) > 90 and i[4] == 'x':\n",
    "                genotype_interpretation_dict[sample] = 'predicted homozygous loss in high-ranking allele, with loss having gained a novel TFBS for a distinct TF'\n",
    "            elif int(i[0]) == 1 and int(i[1]) > 90 and i[5] == 'x':\n",
    "                genotype_interpretation_dict[sample] = 'predicted loss in inconsequential (low-ranking) allele rank(s)'\n",
    " \n",
    "            elif int(i[0]) == 1 and 35 < int(i[1]) < 90:\n",
    "                if i[2] == 'x':\n",
    "                    if int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][2] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles (no regain or gain)'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][3] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has regained a TFBS for TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][4] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][5] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles'\n",
    "                    else:\n",
    "                        genotype_interpretation_dict[sample] = 'predicted loss among high-ranking allele'\n",
    "                elif i[3] == 'x':\n",
    "                    if int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][2] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has regained a TFBS for TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][3] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but both of the 2 losses have regained a TFBS for TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][4] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][5] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles, with single loss having regained a TFBS for TF'\n",
    "                    else:\n",
    "                        genotype_interpretation_dict[sample] = 'predicted loss among high-ranking allele, with loss having regained a TFBS for TF'\n",
    "                elif i[4] == 'x':\n",
    "                    if int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][2] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][3] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has regained a TFBS for TF and 1 has gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][4] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted biallelic loss among high-ranking alleles, but both of the 2 losses have gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][5] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles, with single loss having gained a novel TFBS for a distinct TF'\n",
    "                    else:\n",
    "                        genotype_interpretation_dict[sample] = 'predicted loss among high-ranking allele, with loss having gained a novel TFBS for a distinct TF'\n",
    "                elif i[5] == 'x':\n",
    "                    if int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][2] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][3] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles, with single loss having regained a TFBS for TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][4] == 'x':\n",
    "                        genotype_interpretation_dict[sample] = 'predicted heterozygous loss among high-ranking alleles, with single loss having gained a novel TFBS for a distinct TF'\n",
    "                    elif int(sorted(set(allele_ranks_read_pct_list))[index+1][0]) == 2 and 30 < int(sorted(set(allele_ranks_read_pct_list))[index+1][1]) < 90 and sorted(set(allele_ranks_read_pct_list))[index+1][5] == 'x':\n",
    "                         genotype_interpretation_dict[sample] = 'predicted loss in inconsequential (low-ranking) allele rank(s)'\n",
    "                    else:\n",
    "                         genotype_interpretation_dict[sample] = 'predicted loss in inconsequential (low-ranking) allele rank(s)'  \n",
    "            else:\n",
    "                genotype_interpretation_dict[sample] = 'shrug?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype_inference_list = []\n",
    "for index, row in samples_predicted_to_have_lost_TFBS_synopsis_df.iterrows():\n",
    "    genotype_inference_list.append(genotype_interpretation_dict.get(row['sample']))\n",
    "    \n",
    "samples_predicted_to_have_lost_TFBS_synopsis_df['genotype inference'] = genotype_inference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "cat_genotype_order = CategoricalDtype(\n",
    "    ['predicted loss among high-ranking allele',\n",
    "\n",
    "'predicted biallelic loss among high-ranking alleles (no regain or gain)',\n",
    "\n",
    " 'predicted loss among high-ranking allele, with loss having regained a TFBS for TF',\n",
    "\n",
    " 'predicted loss among high-ranking allele, with loss having gained a novel TFBS for a distinct TF',\n",
    "\n",
    " 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has regained a TFBS for TF',\n",
    "\n",
    " 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has gained a novel TFBS for a distinct TF',\n",
    "     \n",
    " 'predicted homozygous loss in high-ranking allele, with loss having regained a TFBS for TF',\n",
    "     \n",
    " 'predicted homozygous loss in high-ranking allele, with loss having gained a novel TFBS for a distinct TF',\n",
    "     \n",
    " 'predicted biallelic loss among high-ranking alleles, but both of the 2 losses have regained a TFBS for TF',\n",
    "\n",
    " 'predicted biallelic loss among high-ranking alleles, but 1 of the 2 losses has regained a TFBS for TF and 1 has gained a novel TFBS for a distinct TF',\n",
    "\n",
    " 'predicted biallelic loss among high-ranking alleles, but both of the 2 losses have gained a novel TFBS for a distinct TF',\n",
    "\n",
    " 'predicted heterozygous loss among high-ranking alleles',\n",
    "\n",
    " 'predicted heterozygous loss among high-ranking alleles, with single loss having regained a TFBS for TF',\n",
    "\n",
    " 'predicted heterozygous loss among high-ranking alleles, with single loss having gained a novel TFBS for a distinct TF',\n",
    "\n",
    " 'predicted loss in inconsequential (low-ranking) allele rank(s)'], \n",
    "    ordered=True\n",
    ")\n",
    "samples_predicted_to_have_lost_TFBS_synopsis_df['genotype inference'] = samples_predicted_to_have_lost_TFBS_synopsis_df['genotype inference'].astype(cat_genotype_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*all_TFBS_synopsis_df*: dataframe that catalogs all TFBSs detected for sample alleles (i.e., all FIMO outputs, not filtered for 'lost' or 'gained' status relative to reference sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, prepare output that summarizes all TFBS detected for given samples\n",
    "sample_list = []\n",
    "allele_rank_list = []\n",
    "allele_list = []\n",
    "allele_sequence_list = []\n",
    "reference_sequence_list = []\n",
    "alignment_midline_list = []\n",
    "TF_list = []\n",
    "strand_list = []\n",
    "TFBS_sequence_list = []\n",
    "p_val_list = []\n",
    "lostvsgained_list = []\n",
    "allele_start_coordinate_list = []\n",
    "allele_stop_coordinate_list = []\n",
    "ref_start_coordinate_list = []\n",
    "ref_stop_coordinate_list = []\n",
    "\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    allele_count = 0\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        allele_count = allele_count+1\n",
    "        for TFBS in dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites'):\n",
    "            if len(dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites')) == 0:\n",
    "                sample_list.append(sample)\n",
    "                allele_rank_list.append(allele_count)\n",
    "                allele_list.append(allele)\n",
    "                allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                TF_list.append(TFBS.split(',')[0])\n",
    "                strand_list.append(TFBS.split(',')[1])                           \n",
    "                p_val_list.append(TFBS.split(',')[3])\n",
    "                allele_start_coordinate_list.append(TFBS.split(',')[4])\n",
    "                allele_stop_coordinate_list.append(TFBS.split(',')[5])\n",
    "                TFBS_sequence_list.append(TFBS.split(',')[2])\n",
    "            else:\n",
    "                sample_list.append(sample)\n",
    "                allele_rank_list.append(allele_count)\n",
    "                allele_list.append(allele)\n",
    "                allele_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0].strip())\n",
    "                reference_sequence_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[1].split('>')[1].split('<')[0].strip())\n",
    "                alignment_midline_list.append(dict_allele_TFBS_synopsis.get(sample).get(allele).get('allele_sequence')[2].split('>')[1].split('<')[0].strip())   \n",
    "                TF_list.append(TFBS.split(',')[0])\n",
    "                strand_list.append(TFBS.split(',')[1])                           \n",
    "                p_val_list.append(TFBS.split(',')[3])\n",
    "                allele_start_coordinate_list.append(TFBS.split(',')[4])\n",
    "                allele_stop_coordinate_list.append(TFBS.split(',')[5])\n",
    "                TFBS_sequence_list.append(TFBS.split(',')[2])\n",
    "                \n",
    "all_TFBS_synopsis_df_columns = {\"sample\":sample_list, \"allele rank\":allele_rank_list, \"allele ID\":allele_list, \n",
    "                                \"alignment query\\n(allele sequence)\":allele_sequence_list,\n",
    "                                \"alignment midline\":alignment_midline_list, \"alignment hit\\n(reference)\":reference_sequence_list,\n",
    "                                \"TF\":TF_list,\n",
    "                                \"strand\":strand_list, \n",
    "                                \"TFBS sequence\":TFBS_sequence_list,\n",
    "                                \"TFBS coordinate start (in allele)\":allele_start_coordinate_list,\n",
    "                                \"TFBS coordinate end (in allele)\":allele_stop_coordinate_list,\n",
    "                                \"TFBS p-val\":p_val_list}\n",
    "\n",
    "all_TFBS_synopsis_df = pd.DataFrame(all_TFBS_synopsis_df_columns)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add read count data\n",
    "read_count_list = [i.split('_')[2].strip('[]').split('/')[0] for i in all_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "total_reads_list = [i.split('_')[2].strip('[]').split('/')[1] for i in all_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_total_reads_list = [i.split('_')[4].split(':')[1] for i in all_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_reads_filtered_for_1pct_list = [float(i.split('_')[7].split(':')[1]) if i.split('_')[7].split(':')[1] != 'None' else 0 for i in all_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "pct_reads_filtered_for_10pct_list = [float(i.split('_')[8].split(':')[1]) if i.split('_')[8].split(':')[1] != 'None' else 0 for i in all_TFBS_synopsis_df['allele ID'].to_list()]\n",
    "\n",
    "all_TFBS_synopsis_df.insert(loc=3, column='reads', value=read_count_list)\n",
    "all_TFBS_synopsis_df.insert(loc=4, column='total reads', value=total_reads_list)\n",
    "all_TFBS_synopsis_df.insert(loc=5, column='% total reads', value=pct_total_reads_list)\n",
    "all_TFBS_synopsis_df.insert(loc=6, column='% reads filtered for reads <1%', value=pct_reads_filtered_for_1pct_list)\n",
    "all_TFBS_synopsis_df.insert(loc=7, column='% reads filtered for reads <10%', value=pct_reads_filtered_for_10pct_list)\n",
    "\n",
    "# Add column with allele comment (comment if appropriate)\n",
    "all_TFBS_synopsis_df['comment'] = ['note: inferred allele length <=50 bp; read may be primer dimer; consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs' if i <=50 else '' for i in [len(x) for x in all_TFBS_synopsis_df['alignment query\\n(allele sequence)'].to_list()]]\n",
    "\n",
    "all_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"TFBS coordinate start (in allele)\"],ascending=[True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Process key output files  \n",
    "#### *Finalize reports of interpretations for TFBSs lost/regained/gained *  \n",
    "\n",
    "**Data availability:** The raw data underlying allele definitions and interpreted TFBS losses/gains relative to reference sequence are available to a user in a multi-worksheet Excel file, **collated_TFBS.xlsx**.\n",
    "\n",
    "**Lost/gained TFBSs mapped onto BLASTN alignments:** Positional overlays on sequence alignments are available for TFBSs lost or gained in ranked alleles relative to reference sequence, in **collated_TFBS.txt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print dataframes to output file (Excel): **collated_motifs.xlsx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collatedTFBS_csv_output = Path(str(output_path)+ '/'+processdate+'_collated_TFBS.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(collatedTFBS_csv_output) as writer:  \n",
    "    allele_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"lost or gained in allele (relative to ref)?\"],ascending=[True, True, True, True, False]).to_excel(writer, sheet_name='1 TFBS, predicted lost, gained', index=False)\n",
    "    interpreted_TFBS_synopsis_df_updated.sort_values(by=['sample','allele rank','TF',\"strand\",\"interpretation\"],ascending=[True, True, True, True, False]).to_excel(writer, sheet_name='2 TFBS, lost-regained pairs', index=False)\n",
    "    if TF_of_interest == '':\n",
    "        all_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"TFBS coordinate start (in allele)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='3 All TFBS in alleles', index=False)\n",
    "    # TF_of_interest length is assessed to account for Excel maximum of 31 characters in tab names\n",
    "    elif len(TF_of_interest) <= 6:\n",
    "        predicted_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='3 '+TF_of_interest+', lost (all)', index=False)\n",
    "        predicted_exclusive_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='4 '+TF_of_interest+', lost (-gain,-regain)', index=False)\n",
    "        predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='5 '+TF_of_interest+', lost (+regain)', index=False)\n",
    "        predicted_loss_with_gain_of_different_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='6 '+TF_of_interest+', lost (+gain)', index=False)\n",
    "        samples_predicted_to_have_lost_TFBS_synopsis_df.sort_values(by=['genotype inference', 'sample','allele rank',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='7 '+TF_of_interest+', curated samples', index=False)\n",
    "        all_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"TFBS coordinate start (in allele)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='8 All TFBS in alleles', index=False)\n",
    "    else:\n",
    "        adjusted_TF_of_interest = TF_of_interest[:7]\n",
    "        predicted_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='3 '+adjusted_TF_of_interest+' lost (all)', index=False)\n",
    "        predicted_exclusive_loss_of_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='4 '+adjusted_TF_of_interest+' lost (-gain,-regain)', index=False)\n",
    "        predicted_loss_with_regain_of_new_TFBS_for_same_TF_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='5 '+adjusted_TF_of_interest+' lost (+regain)', index=False)\n",
    "        predicted_loss_with_gain_of_different_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF lost',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='6 '+adjusted_TF_of_interest+' lost (+gain)', index=False)\n",
    "        samples_predicted_to_have_lost_TFBS_synopsis_df.sort_values(by=['genotype inference', 'sample','allele rank',\"TF lost strand\",\"Lost TFBS coordinate start (in reference)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='7 '+adjusted_TF_of_interest+' curated samples', index=False)\n",
    "        all_TFBS_synopsis_df.sort_values(by=['sample','allele rank','TF',\"strand\",\"TFBS coordinate start (in allele)\"],ascending=[True, True, True, True, True]).to_excel(writer, sheet_name='8 All TFBS in alleles', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print alignments and associated lost/gained TFBS collations to output file: **collated_motifs.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Relate allele definition (alignments in alignmentoutput_dict2) to TFBS collation for each allele of each sample (with focus on lost and gained TFBS for each allele, relative to reference)\n",
    "collatedTFBS_output = Path(str(output_path)+ '/'+processdate+'_collated_TFBS.txt')\n",
    "with open(str(collatedTFBS_output), 'a+') as f:\n",
    "    print('CollatedMotifs.py: Summary of matches to TFBS motifs detected in sample sequence(s) relative to reference\\nDate: ' + (datetime.today().strftime(\"%m/%d/%Y\")) + '\\n\\n', file = f)\n",
    "    for i in sorted(dict_allele_TFBS_synopsis):\n",
    "        print((len(i)*'=')+'\\n'+i+'\\n'+(len(i)*'='), file = f)\n",
    "        for allele in sorted(dict_allele_TFBS_synopsis.get(i), key=lambda x: x.split('_')[3]):\n",
    "            for x in range(0, len(alignmentoutput_dict2.get(i))):\n",
    "                if alignmentoutput_dict2.get(i)[x][1].split('>')[1].split('<')[0] == allele:\n",
    "                    test = alignmentoutput_dict2.get(i)[x]\n",
    "            sum_gained_motifs = []\n",
    "            sum_lost_motifs = []\n",
    "            sum_motifs = []\n",
    "            sum_TFs = []\n",
    "            TFs_gt1 = []\n",
    "            lost_motifs_plus_strand = []\n",
    "            lost_motifs_minus_strand = []\n",
    "            total_lost_motifs = []\n",
    "            total_lost_motifs_list = []\n",
    "            gained_motifs_plus_strand = []\n",
    "            gained_motifs_minus_strand = []\n",
    "            total_gained_motifs = []\n",
    "            sum_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('all_sites')))\n",
    "            #print(allele+' '+sum_motifs)\n",
    "            sum_TFs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs')))\n",
    "            TFs_gt1 = str(len([TF for TF in dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs') if dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs').get(TF) > 1]))\n",
    "            sum_lost_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('lost')))\n",
    "            sum_gained_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('gained')))\n",
    "            # lost\n",
    "            lost_motifs_plus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost') if motif.split(',')[1] == '+']\n",
    "            lost_motifs_minus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost') if motif.split(',')[1] == '-']\n",
    "            total_lost_motifs = lost_motifs_plus_strand+lost_motifs_minus_strand\n",
    "            total_lost_motifs_dict = dict(Counter(total_lost_motifs))\n",
    "            total_lost_motifs_list = [i+':'+str(total_lost_motifs_dict.get(i)) for i in total_lost_motifs_dict]\n",
    "            total_lost_motifs_list = sorted(total_lost_motifs_list)\n",
    "            # gained\n",
    "            gained_motifs_plus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained') if motif.split(',')[1] == '+']\n",
    "            gained_motifs_minus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained') if motif.split(',')[1] == '-']\n",
    "            total_gained_motifs = gained_motifs_plus_strand+gained_motifs_minus_strand\n",
    "            total_gained_motifs_dict = dict(Counter(total_gained_motifs))\n",
    "            total_gained_motifs_list = [i+':'+str(total_gained_motifs_dict.get(i)) for i in total_gained_motifs_dict]\n",
    "            total_gained_motifs_list = sorted(total_gained_motifs_list)\n",
    "            print(3*' '+'Allele: '+allele.replace('_',' | ')+'\\n   Motifs: total distinct sites |'+sum_motifs+'|, total unique TFs |'+sum_TFs+'| (motifs for '+TFs_gt1+' TFs occur >1x)', file = f)\n",
    "            print(' Synopsis: relative to reference sequence--# lost sites |'+sum_lost_motifs+'|, # new sites |'+sum_gained_motifs+'|', file = f)\n",
    "            print('  Details: lost |'+str(total_lost_motifs_list).strip('[]').replace(\"'\",\"\")+'|', file = f)\n",
    "            print('            new |'+str(total_gained_motifs_list).strip('[]').replace(\"'\",\"\")+'|', file = f)\n",
    "            if len(dict_allele_TFBS_synopsis.get(i).get(allele).get('allele_sequence')[0].split('>')[1].split('<')[0]) <= 50:\n",
    "                print(5*' '+'Note: inferred allele length <=50 bp; read may be primer dimer;\\n'+11*' '+'consult fasta file for this inferred allele, and/or consider pre-processing fastq file (filter reads) prior to running CollatedMotifs', file = f)\n",
    "            # prepare complete visual mapping of new motifs above allele sequence\n",
    "            if int(sum_gained_motifs) > 0: \n",
    "                print('\\n'+11*' '+'NEW motifs:', file = f)\n",
    "                motif_plus_tracker = []\n",
    "                motif_minus_tracker = []\n",
    "                new_motif_plus_list = []\n",
    "                new_motif_minus_list = []\n",
    "                for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained'):\n",
    "                    if motif.split(',')[1] == '+':\n",
    "                        new_motif_plus_list.append(motif)\n",
    "                    elif motif.split(',')[1] == '-':\n",
    "                        new_motif_minus_list.append(motif)\n",
    "                for new_motif_plus in new_motif_plus_list:\n",
    "                    if len(motif_plus_tracker) == 0:\n",
    "                        print(11*' '+'plus(+) strand:', file = f)\n",
    "                        motif_plus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    if re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0]):\n",
    "                        match = re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0])\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+new_motif_plus.split(',')[0]+' (pval '+new_motif_plus.split(',')[3]+')', file = f)\n",
    "                    elif re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0].replace('-','')):\n",
    "                        match = re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0].replace('-',''))\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+new_motif_plus.split(',')[0]+' (pval '+new_motif_plus.split(',')[3]+')'+' [note, approx. position]', file = f)\n",
    "                for new_motif_minus in new_motif_minus_list:\n",
    "                    if len(motif_minus_tracker) == 0:\n",
    "                        print(11*' '+'minus(-) strand:', file = f)\n",
    "                        motif_minus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    seq_revcomp = ''.join(reversed(''.join(nt_dict.get(nt) for nt in test[7].split('>')[1].split('<')[0])))\n",
    "                    if re.search(new_motif_minus.split(',')[2],seq_revcomp):\n",
    "                        match = re.search(new_motif_minus.split(',')[2],seq_revcomp)\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(new_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+new_motif_minus.split(',')[0]+' (pval '+new_motif_minus.split(',')[3]+')'), file = f)\n",
    "                    elif re.search(new_motif_minus.split(',')[2],seq_revcomp.replace('-','')):\n",
    "                        match = re.search(new_motif_minus.split(',')[2],seq_revcomp.replace('-',''))\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(new_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+new_motif_minus.split(',')[0]+' (pval '+new_motif_minus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "            else:\n",
    "                pass\n",
    "            print('\\n'+4*' '+'query  '+test[7].split('>')[1].split('<')[0]+'\\n'+11*' '+test[9].split('>')[1].split('<')[0]+'\\n'+' reference '+test[8].split('>')[1].split('<')[0]+'\\n', file = f)\n",
    "            if int(sum_lost_motifs) > 0:\n",
    "                print(11*' '+'LOST motifs:', file = f)\n",
    "                motif_plus_tracker = []\n",
    "                motif_minus_tracker = []\n",
    "                lost_motif_plus_list = []\n",
    "                lost_motif_minus_list = []\n",
    "                for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost'):\n",
    "                    if motif.split(',')[1] == '+':\n",
    "                        lost_motif_plus_list.append(motif)\n",
    "                    elif motif.split(',')[1] == '-':\n",
    "                        lost_motif_minus_list.append(motif)\n",
    "                for lost_motif_plus in lost_motif_plus_list:\n",
    "                    if len(motif_plus_tracker) == 0:\n",
    "                        print(11*' '+'plus(+) strand:', file = f)\n",
    "                        motif_plus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    if re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0]):\n",
    "                        match = re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0])\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+lost_motif_plus.split(',')[0]+' (pval '+lost_motif_plus.split(',')[3]+')', file = f)\n",
    "                    elif re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0].replace('-','')):\n",
    "                        match = re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0].replace('-',''))\n",
    "                        distance = len(test[8].split('>')[1].split('<')[0])-match.span()[0]-len(lost_motif_plus.split(',')[2])\n",
    "                        sequence = match.group()\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_plus.split(',')[0]+' (pval '+lost_motif_plus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "                for lost_motif_minus in lost_motif_minus_list:\n",
    "                    if len(motif_minus_tracker) == 0:\n",
    "                        print(11*' '+'minus(-) strand:', file = f)\n",
    "                        motif_minus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    seq_revcomp = ''.join(reversed(''.join(nt_dict.get(nt) for nt in test[8].split('>')[1].split('<')[0])))\n",
    "                    if re.search(lost_motif_minus.split(',')[2],seq_revcomp):\n",
    "                        match = re.search(lost_motif_minus.split(',')[2],seq_revcomp)\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(lost_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_minus.split(',')[0]+' (pval '+lost_motif_minus.split(',')[3]+')'), file = f)\n",
    "                    elif re.search(lost_motif_minus.split(',')[2],seq_revcomp.replace('-','')):\n",
    "                        match = re.search(lost_motif_minus.split(',')[2],seq_revcomp.replace('-',''))\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(lost_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_minus.split(',')[0]+' (pval '+lost_motif_minus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "                print('', file = f)\n",
    "            else:\n",
    "                pass\n",
    "                print('\\n', file = f)\n",
    "            \n",
    "# Log TFBS collation operations time duration\n",
    "TFBScollationDuration = str(datetime.now() - startTime_TFBScollation).split(':')[0]+' hr|'+str(datetime.now() - startTime_TFBScollation).split(':')[1]+' min|'+str(datetime.now() - startTime_TFBScollation).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_TFBScollation).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII. Process accessory file  \n",
    "#### *Finalize report of script operation metrics*   \n",
    "\n",
    "**Script metrics:** Along with operating system properties, user-specified variables, input fastq file properties recorded earlier, and TF positional frequency matrices provided to FIMO, metadata concerning output file sizes and script operation time durations are recorded in **script_metrics.txt**.  Samples and/or ranked alleles not present in final output files (due to multiple BLASTN hits or overlapping hsp's) are also noted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Assess files in output directory\n",
    "file_set = [file for file in os.listdir(output_directory) if Path(file).suffix in ('.txt','.fa')] \n",
    "\n",
    "# Assign script end time\n",
    "endTime = datetime.now()\n",
    "endTimestr = str(endTime).split(' ')[1].split('.')[0]\n",
    "\n",
    "# Log entire script operations time duration\n",
    "processingDuration = str(datetime.now() - startTime).split(':')[0]+' hr|'+str(datetime.now() - startTime).split(':')[1]+' min|'+str(datetime.now() - startTime).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Prepare final report of file size metrics and time durations to **script_metrics.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(str(output_path)+ '/'+processdate+'_script_metrics.txt')\n",
    "with open(filename, 'a') as f:\n",
    "    print(\"\"\"File output information:\n",
    "    Output directory: \"\"\" + str(output_directory) +\n",
    "'\\n    Total file #: ' + str(len(file_set)) +\n",
    "'\\n    Total file output sizes: '+path_size(str(output_directory)), file = f)\n",
    "    for file in file_set:\n",
    "        print('    '+file+': '+path_size(str(output_directory)+'/'+file), file = f)\n",
    "    print(\"\"\"\\n\\nScript operation times:\n",
    "    start time: \"\"\"+startTimestr+\n",
    "    '\\n    makeblastdb and fasta-get-markov processing time: '+makeblastdb_fastagetmarkov_operationsDuration+\n",
    "    '\\n    fasta processing time: '+readcountDuration+\n",
    "    '\\n    alignments processing time: '+alignmentsDuration+\n",
    "    '\\n    allele definitions processing time: '+allele_definitionsDuration+\n",
    "    '\\n    TFBS processing time (FIMO): '+fimoDuration+\n",
    "    '\\n    TFBS collation processing time: '+TFBScollationDuration+\n",
    "    '\\n    total processing time: '+processingDuration+\n",
    "    '\\n    end time: '+endTimestr, file = f)\n",
    "f.close()\n",
    "          \n",
    "# End of script operations\n",
    "print(\"\\nScript has completed.  Please find output files at \"+str(output_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "############################################################################# end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
