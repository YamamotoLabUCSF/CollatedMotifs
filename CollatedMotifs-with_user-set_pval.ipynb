{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## CollatedMotifs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "https://github.com/YamamotoLabUCSF/CollatedMotifs  \n",
    "v1.0/Committed 8-03-2019\n",
    " \n",
    "DNA sequence-selective transcription factors (TFs) mediate gene regulation; their interactions with DNA contribute to the formation of nucleoprotein structures that modulate transcription at target genes.  These functional units -- **response elements** (*e.g.*, enhancers/*cis*-regulatory modules) -- integrate cellular signals to regulate the types of gene transcripts produce by a cell, and when and how much of each transcript type is made.  CRISPR-Cas9 editing routinely yields mixed allelic mutation at target loci (*e.g.*, variable insertion *vs.* deletion, indel length across edited cells).  For editing efforts targeted to putative response elements, widely available pattern-matching tools enable prediction of transcription factor binding sites (TFBS) at altered loci, based on matches to position frequency matrices of known TFs.  Awareness of altered TFBS in Cas9-edited alleles can aid prediction and/or interpretation of functional consequences associated with mutations.\n",
    "\n",
    "<img src=\"CollatedMotifs_img/CollatedMotifs_thumbnail.png\" align=\"right\" width=\"650\"> \n",
    "\n",
    "**This script automates allele prediction and TFBS collation for deeply sequenced amplicons, and reports TFBS 'lost' and 'new' relative to user-supplied reference sequence(s).**\n",
    "\n",
    "### Potential uses:  \n",
    "This script was developed to enable rapid assessment of TFBS differences at target loci in mutant clones, following Cas9-editing (CRISPR-Cas9 mutagenesis) and clonal isolation.\n",
    "\n",
    "### Synopsis:  \n",
    "**This script returns allele definitions annotated with lost and/or gained TFBS (relative to a reference sequence), for samples from a demultiplexed NGS fastq dataset** \n",
    ">(see 'Output notes' for file output details).  \n",
    "\n",
    "**Users are asked for paths to specific directories (*e.g.*, output and input directories), locally installed executables (BLASTN & MAKEBLASTDB (NCBI), FIMO & FASTA-GET-MARKOV (MEME)), and files (fasta file containing reference sequence(s) for TFBS comparison, fasta file containing reference sequence(s) for alignment, text file containing position frequency matrices for TFBS)**  \n",
    ">(see 'Input notes' for details).\n",
    "    \n",
    "Python3, BLASTN (NCBI), MAKEBLASTDB (NCBI), FIMO (MEME), and FASTA-GET-MARKOV (MEME) are required for operation.  \n",
    "\n",
    "BLASTN & its associated executable MAKEBLASTDB can be downloaded and locally installed at https://www.ncbi.nlm.nih.gov/guide/howto/run-blast-local/.  \n",
    "\n",
    "FIMO, its associated executable FASTA-GET-MARKOV, and positional frequency matrix files can be downloaded and locally installed at http://meme-suite.org/doc/fimo.html.\n",
    "\n",
    "For usage details, please refer to README file at GitHub and to the following manuscript:  \n",
    ">*Ehmsen, Knuesel, Martinez, Asahina, Aridomi, Yamamoto (2021)*\n",
    "    \n",
    "Please cite usage as:  \n",
    ">CollatedMotifs.py  \n",
    ">*Ehmsen, Knuesel, Martinez, Asahina, Aridomi, Yamamoto (2021)*\n",
    " \n",
    "--------\n",
    "\n",
    "### Operation notes:  \n",
    "*What does this script do?*\n",
    " 1. **classify & count reads:** merges R1 and R2 sequences into a single read, counts unique read types per well (*i.e.*, sample); fastq file name provides the sample name  \n",
    " \n",
    " \n",
    " 2. **identify top 5 reads** per well (in terms of read abundance); calculates representation among reads within the well at four levels:  \n",
    " \n",
    "   (a) raw frequency (% read type in question, relative to total reads)  \n",
    "   (b) percentile (% of other read types that fall below the frequency of the read type in question)  \n",
    "   (c) adjusted frequency @ 1% (% read type in question, relative to reads that occur at >1% frequency)  \n",
    "   (d) adjusted frequency @ 10% (% read type in question, relative to reads that occur at >10% frequency)  \n",
    " \n",
    " \n",
    " 3. **align to reference database:** aligns top 5 reads to reference sequence(s) using BLASTN  \n",
    " *(National Center for Biotechnology Information;\n",
    "    Altschul S.F. et al. (1990) \"Basic local alignment search tool\", J Mol Biol. 15(3):403-10)*  \n",
    "      * Alignment database is created within the script by MAKEBLASTDB, from user-provided, fasta-formatted reference sequence(s)  \n",
    "    <img src=\"CollatedMotifs_img/MAKEBLASTDB_and_BLASTN_reference_database_thumbnail.png\" align=\"left\" width=\"300\">\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    " 4. **identify TFBS in reference and allele sequences:** for user-provided reference sequences, uses FIMO and user-provided positional frequency matrix file to find matches to TFBS motifs  \n",
    "     * Background Markov file for TFBS match statistics is created within the script by FASTA-GET-MARKOV, from user-provided, fasta-formatted reference sequence(s)  \n",
    "    *(FIMO: Grant C.E. et al. (2011) \"FIMO: Scanning for occurrences of a given motif\", Bioinformatics 27(7):1017–1018)*  \n",
    "    *(MEME Suite; Bailey T.L. et al. (2015) \"The MEME Suite\", Nucleic Acids Res 43(Web Server issue):W39–W49)*\n",
    "<img src=\"CollatedMotifs_img/FASTAGETMARKOV_and_Markov_background_thumbnail.png\" align=\"left\" width=\"350\">\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    " 5. **return collation of novel *vs.* lost TFBS:** compares TFBS in reads to TFBS in specific reference sequence, outputting 'new' and 'lost' TFBS relative to the reference sequence.\n",
    " \n",
    "\n",
    " \n",
    "--------\n",
    "### Input notes:\n",
    "You will be prompted for the following user-specific information (10 items):\n",
    "\n",
    "**Required** (10 strings: 9 strings specifying directory, executable, or file locations, + 1 string specifying prefix to be assigned to BLASTN database files):  \n",
    "\n",
    "  * paths to directories (2)\n",
    "    <ul>\n",
    "    <li>where should output files go?</li>  \n",
    "    <i>path to <strong>output directory</strong> for output files</i>\n",
    "    <li>where are input files found?</li>\n",
    "    <i>path to single directory containing <strong>demultiplexed fastq files</strong></i> \n",
    "    </ul>\n",
    "<br clear=\"all\" />\n",
    "  * paths to executables (4)  \n",
    "     <ul>\n",
    "     <li>where is BLASTN executable found?</li>\n",
    "         <i>path to BLASTN installation</i>\n",
    "     <li>where is MAKEBLASTDB executable found?</li>\n",
    "         <i>path to <strong>MAKEBLASTDB</strong> installation</i>\n",
    "     <li>where is FIMO executable found?</li>\n",
    "         <i>path to <strong>FIMO</strong> installation</i>\n",
    "     <li>where is FASTA-GET-MARKOV executable found?</li>\n",
    "         <i>path to <strong>FASTA-GET-MARKOV</strong> installation</i>\n",
    "     </ul>\n",
    "<br clear=\"all\" />      \n",
    "  * paths to files (3)\n",
    "     <ul>\n",
    "     <li>what are your reference sequence(s), to which you will (a) align sequenced reads, and (b) compare sequenced reads for TFBS occurrence?</li>\n",
    "        <i>path to single <strong>fasta file</strong>, containing <strong>reference sequence(s)</strong> for processing by (a) MAKEBLASTDB, to generate a database reference for BLASTN, and (b) FIMO, to establish TFBS occurrence(s) to be evaluated relative to sequenced reads</i>\n",
    "     <li>what are the TFBS motif(s) for which you will search, and for which you will draw comparisons for presence/absence between sequences?</li>\n",
    "        <i>path to single <strong>text file</strong>, containing <strong>position frequency matrix(ces)</strong> for TFs</i>\n",
    "     <li>what DNA sequence(s) will you use as a basis for markov background estimation, to be used by FIMO</li>\n",
    "        <i>path to single <strong>text/fasta file</strong>, containing DNA sequence(s) from which a <strong>markov background file</strong> will be generated for use by FIMO</i>    \n",
    "     </ul>\n",
    "<br clear=\"all\" />\n",
    "  * label for database files created in 'alignment_directory' by MAKEBLASTDB (1)\n",
    "     <ul>\n",
    "     <li>what common prefix (\\*) will you assign to the six files (*.nin, *.nhr, *.nog, *.nsd, *.nsg, *.nsi) created by MAKEBLASTDB, as the alignment database for BLASTN?</li>\n",
    "    </ul>\n",
    "\n",
    "------\n",
    "\n",
    "### Output notes:\n",
    "This script produces 5 output files in the user-specified output directory, plus three sub-directories:  \n",
    "\n",
    "  - **sub-directories** comprise outputs of MAKEBLASTDB and FIMO:  \n",
    "  \n",
    "    - two directories contain FIMO output files (fimo_out and fimo_out_ref); each of these sub-directories contains 5 subsidiary files created by FIMO (cisml.xml, fimo.gff, fimo.html, fimo.tsv, fimo.xml)  \n",
    "    - one directory comprises BLASTN alignment database (alignment_databse); this directory contains 6 subsidiary files created by MAKEBLASTDB operation on user-supplied fasta file containing reference sequence(s) (\\*.nin, \\*.nhr, \\*.nog, \\*.nsd, \\*.nsg, \\*.nsi)  \n",
    " <br clear=\"all\" />\n",
    "  - 5 **output files** in the user-specified output directory; these include:\n",
    "     \n",
    "  \n",
    "    1. fasta.fa  \n",
    "        (collection of fasta entries representing top 5 most abundant sequences assigned to a single sample ID)  \n",
    "\n",
    "\t2. blastn_alignments.txt  \n",
    "        (output of BLASTN operation on fasta.fa)  \n",
    "        \n",
    "     3. markov_background.txt  \n",
    "        (output of FASTA-GET-MARKOV operation on user-supplied fasta reference file)  \n",
    "        \n",
    "     4. collated_TFBS.txt  \n",
    "        (output of script operation on FIMO-generated .tsv files in fimo_out and fimo_out_ref)  \n",
    "        \n",
    "     5. script_metrics.txt  \n",
    "        (summary/analysis of script operation metrics \\[metadata\\])\n",
    "\n",
    "           Directory structure under an output directory specified as 'CollatedMotifs', for example,\n",
    "           would contain the following subdirectories and files following CollatedMotifs.py operations:\n",
    "\n",
    "           /CollatedMotifs \n",
    "                          `-----/alignment_database\n",
    "                                        `----------*.nin\n",
    "                                        `----------*.nhr\n",
    "                                        `----------*.nog\n",
    "                                        `----------*.nsd\n",
    "                                        `----------*.nsg\n",
    "                                        `----------*.nsi\n",
    "                          `-----blastn_alignments.txt\n",
    "                          `-----collated_TFBS.txt\n",
    "                          `-----fasta.fa\n",
    "                          `-----/fimo_out\n",
    "                                        `----------cisml.xml\n",
    "                                        `----------fimo.gff\n",
    "                                        `----------fimo.html\n",
    "                                        `----------fimo.tsv\n",
    "                                        `----------fimo.xml\n",
    "                          `-----/fimo_out_ref\n",
    "                                        `----------cisml.xml\n",
    "                                        `----------fimo.gff\n",
    "                                        `----------fimo.html\n",
    "                                        `----------fimo.tsv\n",
    "                                        `----------fimo.xml\n",
    "                          `-----markov_background.txt\n",
    "\n",
    "                          `-----script_metrics.txt\n",
    "--------\n",
    "### Visual summary of key script operations:  \n",
    "In short, sequencing data in a sample-specific **fastq file** (*e.g.,* below), are converted to user-interpretable allele definitions (alignments to a reference sequence) annotated with **TFBS motif lost and/or gained relative to a reference sequence** (**key output file**, below), for 100s to 1000s of samples.  \n",
    "<img src=\"CollatedMotifs_img/fastq_example.png\" align=\"left\" width=\"700\">\n",
    "<br clear=\"all\" />\n",
    "#### Key output file:  \n",
    "##### collated_TFBS.txt\n",
    "<img src=\"CollatedMotifs_img/example_CollatedMotifs_output.png\" align=\"left\" width=\"900\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "**Welcome.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### I. Setup  \n",
    "Import libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for availability of Python dependencies in path\n",
    "missing_dependencies_list = []\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('psutil')\n",
    "    \n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('numpy')\n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "except ImportError:\n",
    "    missing_dependencies_list.append('scipy')\n",
    "    \n",
    "if len(missing_dependencies_list) > 0:\n",
    "    print('ModuleNotFoundError\\n')\n",
    "    print('Please note, the following required Python module(s) are not found in your Python system path:')\n",
    "    for i in missing_dependencies_list:\n",
    "        print('   '+i)\n",
    "    print('\\nPlease exit the script and install these Python dependencies in your system path.')\n",
    "    print(\"\"\"\\nGuidelines for installation of Python dependencies can be found in the README file for CollatedMotifs.py ('System Setup')\"\"\")\n",
    "    print(\"\"\"    (Creation of a Python virtual environment is recommended)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Operating system interfaces\n",
    "import os\n",
    "\n",
    "# Time access and conversions, Basic data and time types\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# System-specific parameters and functions\n",
    "import sys\n",
    "\n",
    "# Process and system utilities\n",
    "import psutil\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Gzip to read GNU zipped files\n",
    "import gzip\n",
    "\n",
    "# Low-level networking interface\n",
    "import socket\n",
    "\n",
    "# System version information\n",
    "import platform\n",
    "\n",
    "# Unix-style pathname pattern expansion\n",
    "import glob\n",
    "\n",
    "# NumPy (numeric operations)\n",
    "import numpy\n",
    "\n",
    "# SciPy (for percentile) \n",
    "from scipy import stats\n",
    "\n",
    "# Container datatypes (for Counter operation)\n",
    "from collections import Counter\n",
    "\n",
    "# Decimal fixed point and floating point arithmetic\n",
    "from decimal import Decimal\n",
    "\n",
    "# Regular expression operations\n",
    "import re\n",
    "\n",
    "# Object-oriented filesystem paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Internationalization services (for use of thousands separator in numbers where appropriate)\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "# start time\n",
    "initialTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions  \n",
    "*User inputs can be entered either in rapid succession ('List' format), or in response to individually coached prompts. 'Prompts' defines a series of 6 coached entries that provide a user with instructive detail regarding the nature of required input.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 'prompts' function for coached user input\n",
    "def prompts():\n",
    "    \"\"\"Coached prompts to collect user input\"\"\"\n",
    "    # Make variables assigned in prompts() function globally available\n",
    "    global output_directory\n",
    "    global fastq_directory\n",
    "    global fasta_ref\n",
    "    global blastn_path\n",
    "    global makeblastdb_path\n",
    "    global db_prefix\n",
    "    global fimo_path\n",
    "    global fimo_motifs_path\n",
    "    global fasta_get_markov_path\n",
    "    global markov_background_file\n",
    "    global pval_threshold\n",
    "    # 1-Specify output directory.\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Location of OUTPUT DIRECTORY for output files\n",
    "---------------------------------------------\n",
    "    \n",
    "This script produces 5 output files in the user-specified output directory, plus three directories:\n",
    "two directories and subsidiary files created by FIMO (fimo_out and fimo_out_ref) and one directory\n",
    "and subsidiary files created by MAKEBLASTDB (alignment_database).\n",
    "    \n",
    "CollatedMotifs.py output files include:\n",
    "    \n",
    "    1. fasta.fa\n",
    "\n",
    "    2. blastn_alignments.txt\n",
    "        (output of BLASTN operation on fasta.fa)\n",
    "\n",
    "    3. markov_background.txt\n",
    "        (output of FASTA-GET-MARKOV operation on user-supplied fasta reference file)\n",
    "\n",
    "    4. collated_TFBS.txt\n",
    "        (output of script operation on FIMO-generated .tsv files in fimo_out and fimo_out_ref)\n",
    "            \n",
    "    5. script_metrics.txt (summary/analysis of script operation metrics [metadata])\n",
    "  \n",
    "        Note: \n",
    "        * These files do not exist before the script is run. The files are made by the script.\n",
    "        * The primary data outputs for TFBS comparisons are found in collated_TFBS.txt\n",
    "        \n",
    "At this prompt, indicate an absolute path to a ** directory ** that will be created by the script as the location\n",
    "for output files.  This directory should not exist yet -- it will be created as an output of this script, and will\n",
    "be populated with the file outputs of this specific instance of the script operation.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators, regardless of operating system (Mac or Windows).\n",
    "\n",
    "Example: if you'd like to create a directory ('CollatedMotifs') in an existing directory ('Illumina'), accessed\n",
    "with absolute path of '/Users/myname/Illumina/CollatedMotifs' (Mac) or 'C:\\Users\\myname\\Illumina\\CollatedMotifs'\n",
    "(Windows), enter '/Users/myname/Illumina/CollatedMotifs' at the command line prompt. Replace 'myname' with the\n",
    "appropriate intervening directory identifiers. Do *not* flank your entry with quotation marks (') at the\n",
    "command-line.\n",
    "    \n",
    "Alternatively, simply enter a desired directory name (e.g., 'CollatedMotifs') and run this script from\n",
    "within a directory where you'd like to create this new directory.\"\"\"+'\\n')\n",
    "    output_directory = input(r\"\"\"    -----> Output directory name and path:  \"\"\")\n",
    "    # 2-Specify the fastq files to be used for input, by indicating directory location of the file list.\n",
    "    print(r\"\"\"\n",
    "------------------------------------------------------------------------------\n",
    "Location of INPUT FILES (single directory containing demutiplexed fastq files)\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "You will now be asked to enter the path to the directory containing the fastq files\n",
    "to be processed as CollatedMotifs.py input.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your fastq input files are named file1.fastq, file2.fastq, etc. and are found in a directory\n",
    "named 'Sequences' with absolute path of '/Users/myname/Sequences' (Mac) or 'C:\\Users\\myname\\Sequences' (PC),\n",
    "enter '/Users/myname/Sequences' at the command line prompt.\n",
    "\n",
    "When you're done entering the fastq file location, press 'Enter' again to proceed in the script.\"\"\"+'\\n')\n",
    "    fastq_directory = input(r\"\"\"    -----> Directory name and path:  \"\"\")\n",
    "    # 3-Specify fasta file containing reference sequences as basis for TFBS motif comparisons/contrasts.\n",
    "    print(r\"\"\"\n",
    "-----------------------------------------\n",
    "Location of FIMO REFERENCE SEQUENCES FILE\n",
    "-----------------------------------------\n",
    "\n",
    "This script aligns and compares your top sample read sequence(s) to a defined reference sequence,\n",
    "as its basis for determining distinct vs. common TFBS motifs. Please indicate the absolute path to a\n",
    "fasta file containing reference sequences.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if you have three sample names relating to three different reference sequences, enter these\n",
    "sequences in fasta format, saved in a single text file. Each fasta entry definition line (defline)\n",
    "should be named such that the defline name matches a unique descriptor in fastq file names.\n",
    "    \n",
    "    >Sample1\n",
    "    GATCGACTAGAGCGAGCATTCATCATATCACGAGTAGCATCGACGTGCACGATCGATCGTAGCTAGCTAGTCATGCATGCATGCTAGATTCGAGCATGCATGCTAC\n",
    "    >Sample2\n",
    "    AGTAGCTGTGATGCTAGTCATCTAGCTAGCAGCGTAGCTAGCGATCGATCTAGAGCCGATCGATCGAGCATCTAGCTATCAGCGGCGGGATCATCTATCTACGGG\n",
    "    >Sample3\n",
    "    CGATGCAGCGCGATCGAGCGCGATCGATATTAGCATGCGCAGCTAGCTAGCTGGCGATCGATGCATGCTAGCTGTGTCAGTCGACGATCACACGATCACACTGTGTG\n",
    "\n",
    "When you're done entering the list of reference sequences, press 'Enter' again to proceed in the script.\"\"\"+'\\n')\n",
    "    fasta_ref = input(r\"\"\"    -----> Path to fasta file containing reference sequences:  \"\"\")\n",
    "    # 4-Collect path to blastn executable.\n",
    "    print(r\"\"\"\n",
    "-----------------------------\n",
    "Location of BLASTN EXECUTABLE\n",
    "-----------------------------\n",
    "\n",
    "This script uses BLASTN (NCBI) to align reads from your fastq files to a reference sequence database.\n",
    "Please indicate the absolute path to the BLASTN executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your BLASTN executable is found at absolute path /Users/myname/blastn, type '/Users/myname/blastn'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    blastn_path = input(r\"\"\"    -----> Path to BLASTN executable:  \"\"\")\n",
    "    # 5-Collect path to makeblastdb executable.\n",
    "    print(r\"\"\"\n",
    "----------------------------------\n",
    "Location of MAKEBLASTDB EXECUTABLE\n",
    "----------------------------------\n",
    "\n",
    "Because this script uses BLASTN (NCBI) to align reads from your fastq files to a reference sequence database,\n",
    "a compatible reference sequence database is required. This script uses MAKEBLASTDB (NCBI) to generate\n",
    "a reference sequence database from the reference sequences in the fasta file you provided earlier.\n",
    "    \n",
    "Please indicate the absolute path to the MAKEBLASTDB executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your MAKEBLASTDB executable is found at absolute path /Users/myname/makeblastdb,\n",
    "type '/Users/myname/makeblastdb' and press Enter.\"\"\"+'\\n')\n",
    "    makeblastdb_path = input(r\"\"\"    -----> Path to MAKEBLASTDB executable:  \"\"\")\n",
    "    # 6-Specify prefix to files in database\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Prefix for files in BLASTN ALIGNMENT DATABASE\n",
    "---------------------------------------------\n",
    "\n",
    "Because this script uses BLASTN (NCBI) and an alignment reference database, a common prefix identifier for the six\n",
    "database files generated by MAKEBLASTDB is needed.\n",
    "\n",
    "Please indicate a prefix to assign to each of the database files.\n",
    "\n",
    "Example: if your alignment reference was generated by MAKEBLASTDB from a fasta file called GRCh38.fa,\n",
    "the alignment database files will have been assigned the prefix 'GRCh38'; you would type 'GRCh38'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    db_prefix = input(r\"\"\"    -----> Prefix for alignment reference sequence database files:  \"\"\")\n",
    "    # 7-Specify path to FIMO installation\n",
    "    print(r\"\"\"\n",
    "---------------------------\n",
    "Location of FIMO EXECUTABLE\n",
    "---------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate the absolute path to the FIMO installation.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "Example: if your FIMO executable is found at absolute path /Users/myname/fimo, type '/Users/myname/fimo'\n",
    "and press Enter.\"\"\"+'\\n')\n",
    "    fimo_path = input(r\"\"\"    -----> Path to FIMO executable:  \"\"\")\n",
    "    # 8-Specify path to FIMO motif file.\n",
    "    print(r\"\"\"\n",
    "----------------------------\n",
    "Location of FIMO MOTIFS FILE\n",
    "----------------------------\n",
    "\n",
    "This script uses FIMO from the meme suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBS motifs.\n",
    "\n",
    "Please indicate the absolute path to the FIMO motifs file (containing position frequency matrix/matrices).\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "When you're done entering the location of the motifs file, press Enter.\"\"\"+'\\n')\n",
    "    fimo_motifs_path = input(r\"\"\"    -----> Path to FIMO motifs file:  \"\"\")\n",
    "    # 9-Specify path to FIMO fasta-get-markov installation.\n",
    "    print(r\"\"\"\n",
    "---------------------------------------------\n",
    "Location of FIMO FASTA-GET-MARKOV EXECUTABLE\n",
    "---------------------------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate an absolute path to the location of the FASTA-GET-MARKOV executable.\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "\n",
    "When you're done entering the location of the executable, press Enter.\"\"\"+'\\n')\n",
    "    fasta_get_markov_path = input(r\"\"\"    -----> Path to FASTA-GET-MARKOV executable:  \"\"\")\n",
    "    # 10-Specify path to markov background file.\n",
    "    print(r\"\"\"\n",
    "------------------------------------------------------------\n",
    "Location of FIMO FASTA-GET-MARKOV BACKGROUND REFERENCE FILE\n",
    "------------------------------------------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.\n",
    "\n",
    "Please indicate an absolute path to the location of the fasta file you will use as your background reference\n",
    "(on which FASTA-GET-MARKOV will operate to generate a markov background file).\n",
    "\n",
    "Use only forward slashes ('/') as directory separators.\n",
    "    \n",
    "When you're done entering the location of the reference sequence, press Enter.\"\"\"+'\\n')\n",
    "    markov_background_file = input(r\"\"\"    -----> Path to background reference file:  \"\"\")\n",
    "    print(r\"\"\"    \n",
    "----------------------------------------------------------------------------------------\n",
    "P-VALUE THRESHOLD to be used by FIMO in identification & reporting of TFBS motif matches\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "This script uses FIMO from the MEME suite of sequence analysis tools as its basis for determining distinct vs.\n",
    "common TFBSs.  FIMO can be provided with a user-specified p-value threshold to adjust stringency of reported\n",
    "TFBS matches to TF position frequency matrix/matrices.\n",
    "\n",
    "Please indicate the p-value threshold you'd like FIMO to use in its TFBS reporting. CollatedMotifs.py uses\n",
    "a threshold of p-value = 0.001 (1e-3) in its default; in this script ('with pval threshold setting'), specify\n",
    "your choice of threshold.\n",
    "\n",
    "Input the threshold setting in the scientific notation format based on 1e (10 raised to a specified power), such as,\n",
    "'1e-4' for 10^-4 (0.0001), or '5e-3' for 5x10^-3 (0.005). Do not include quotes flanking your text, or spaces\n",
    "between the characters.\n",
    "    \n",
    "When you're done entering the customized p-value threshold, press Enter.\"\"\"+'\\n')\n",
    "    pval_threshold = input(r\"\"\"    -----> Customized p-value threshold setting:  \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*'allele_output' defines a function that is called upon when populating the 'imputed_genotypes.txt' output file; the function reports alleles for samples that belong to a specified genotype class (e.g., homozygous deletion)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 'convert_bytes' and 'path_size' functions to be used in data collection for script_metrics.txt        \n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    This function converts bytes to convenient order of magnitude prefixes\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "        \n",
    "def path_size(given_path):\n",
    "    \"\"\"\n",
    "    This function returns file or directory size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(given_path):\n",
    "        file_info = os.stat(given_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "    elif os.path.isdir(given_path):\n",
    "        dir_info = os.stat(given_path)\n",
    "        return convert_bytes(dir_info.st_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*'merge' and 'merge1' define functions that merge R1 & R2 (reverse complement), or append if they do not overlap; nt_dict is called upon to reverse complement R2* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define 'merge' function to merge R1 & R2 reads\n",
    "def merge(s1, s2):\n",
    "    i = 0\n",
    "    while not s2.startswith(s1[i:]):\n",
    "        i += 1\n",
    "    if i < len(s2):\n",
    "        return s1[:i] + s2\n",
    "    else:\n",
    "        return 'no overlap'\n",
    "    \n",
    "# Define 'merge1' function to append two strings that do not overlap\n",
    "def merge1(s1, s2):\n",
    "    i = 0\n",
    "    while not s2.startswith(s1[i:]):\n",
    "        i += 1\n",
    "    return s1[:i] + s2\n",
    "\n",
    "# Define nt complement dictionary      \n",
    "nt_dict = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N', '-':'-'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### II. Define user-specified variables\n",
    "\n",
    "A user defines input variables by entering individual lines of text at the Jupyter interface.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specify whether user input is provided at individual coached prompts or as single-list entry\n",
    "print(r\"\"\"\n",
    "---------------------------------------------------------------------\n",
    "User-specified input: choice of coached prompts vs. single list entry\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Values for the user-specified input indicated above can be entered at individually coached command-line prompts\n",
    "(default), or as a single list of variables provided in a single command-line entry without coached prompts.\n",
    "\n",
    "To proceed with input at individual command-line PROMPTS, type 'Prompt' and press Enter;\n",
    "To proceed with input provided as a single LIST in one command-line entry, type 'List' and press Enter:  \n",
    "    \"\"\")\n",
    "user_input = input(r\"\"\"    -----> List or Prompt: \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if user_input == 'Prompt':\n",
    "    prompts()\n",
    "elif user_input == 'List':\n",
    "    print(\"\"\"\n",
    "----------------------------------\n",
    "User-specified input (list format)\n",
    "----------------------------------\n",
    "    \n",
    "Please paste individual input values directly at the interpreter prompts, specifying the following 11 values\n",
    "in the specified order.\n",
    "\n",
    "Press 'Enter' twice to complete.\n",
    "    \n",
    "    1-Location of OUTPUT DIRECTORY for output files\n",
    "    2-Location of INPUT FILES (directory containing fastq files)\n",
    "    3-Location of REFERENCE FASTA FILE\n",
    "    4-Location of BLASTN EXECUTABLE\n",
    "    5-Location of MAKEBLASTDB EXECUTABLE\n",
    "    6-Prefix to assign to BLASTN sequence database files\n",
    "    7-Location of FIMO EXECUTABLE\n",
    "    8-Location of POSITION FREQUENCY MATRIX FILE\n",
    "    9-Location of FASTA-GET-MARKOV EXECUTABLE\n",
    "    10-Location of MARKOV BACKGROUND FILE\n",
    "    11-p-value threshold for motif match to TFBS (FIMO); default in CollatedMotifs.py is 0.001 (1e-3)\n",
    "    \n",
    "\"\"\")\n",
    "    input_list = []\n",
    "    stopword = \"\"\n",
    "    while True:\n",
    "        input_str = input()\n",
    "        if input_str.strip() == stopword:\n",
    "            break\n",
    "        else:\n",
    "            input_list.append(input_str)       \n",
    "    output_directory = input_list[0].strip()\n",
    "    fastq_directory = input_list[1].strip()\n",
    "    fasta_ref = input_list[2].strip()\n",
    "    blastn_path = input_list[3].strip()\n",
    "    makeblastdb_path = input_list[4].strip()\n",
    "    db_prefix = input_list[5].strip()\n",
    "    fimo_path = input_list[6].strip()\n",
    "    fimo_motifs_path = input_list[7].strip()\n",
    "    fasta_get_markov_path = input_list[8].strip()\n",
    "    markov_background_file = input_list[9].strip()\n",
    "    pval_threshold = input_list[10].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Convert directory and executable strings to operating system-appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Wait to create the directories and files until after input has been reviewed and accepted.\n",
    "# Convert fastq_directory input to operating system-appropriate filepath.\n",
    "output_directory = Path(str(output_directory))\n",
    "# Convert fastq_directory input to operating system-appropriate filepath.\n",
    "fastq_directory = Path(str(fastq_directory))\n",
    "# Convert fasta_ref input to operating system-appropriate filepath.\n",
    "fasta_ref = Path(str(fasta_ref))\n",
    "# Convert blastn_path input to operating system-appropriate filepath.\n",
    "blastn_path = Path(str(blastn_path))\n",
    "# Convert makeblastdb_path input to operating system-appropriate filepath.\n",
    "makeblastdb_path = Path(str(makeblastdb_path))\n",
    "# Convert fimo_path input to operating system-appropriate filepath.\n",
    "fimo_path = Path(str(fimo_path))\n",
    "# Convert fimo_motifs_path input to operating system-appropriate filepath.\n",
    "fimo_motifs_path = Path(str(fimo_motifs_path))\n",
    "# Convert fasta_get_markov_path input to operating system-appropriate filepath.\n",
    "fasta_get_markov_path = Path(str(fasta_get_markov_path))\n",
    "# Convert markov_background_file input to operating system-appropriate filepath.\n",
    "markov_background_file = Path(str(markov_background_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Collect fastq files from directory; sort alphanumerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "myFastqFilenames = [file for file in glob.glob(str(fastq_directory)+'/*') if Path(file).suffix in [\".gz\",\".fastq\"]]\n",
    "\n",
    "#Sort fastq file names\n",
    "myFastqFilenames = sorted(myFastqFilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Print fastq file names, to double-check file inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for file in myFastqFilenames:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Collect overview of fastq file contents:  \n",
    "<ul>\n",
    "  <li>Illumina runID</li>   \n",
    "  <li>read count in each fastq file</li>    \n",
    "  <li>file size</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect Illumina run IDs from fastq files, consolidate to unique run IDs\n",
    "runIDlist = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:\n",
    "            runID = \":\".join(f.readline().split(\":\",-2)[:2])\n",
    "            if not runID in runIDlist:\n",
    "                runIDlist.append(runID) \n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        with open(sourcefile, \"r\") as f:\n",
    "            runID = \":\".join(f.readline().split(\":\",-2)[:2])\n",
    "            if not runID in runIDlist:\n",
    "                runIDlist.append(runID)\n",
    "\n",
    "# Collect total read counts for fastq files\n",
    "readcount = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:    \n",
    "            readcount.append(int(len((f).readlines())/4))\n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        with open(sourcefile, \"r\") as f:\n",
    "            readcount.append(int(len((f).readlines())/4))\n",
    "        \n",
    "# Collect file sizes for fastq files\n",
    "filesize = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if Path(sourcefile).suffix == \".gz\":\n",
    "        with gzip.open(sourcefile, \"rt\") as f:\n",
    "            filesize.append(round((os.path.getsize(sourcefile)/1048576),5))\n",
    "    elif Path(sourcefile).suffix == \".fastq\":\n",
    "        filesize.append(round((os.path.getsize(sourcefile)/1048576),5))\n",
    "\n",
    "# fastq_overview prepares summation of fastq file names, their sizes, and read counts, to be reported in script_metrics.txt    \n",
    "fastq_overview = list(zip(myFastqFilenames, filesize, readcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Double-check whether user-specified entries look good. If a variable is inaccurately assigned, prompt user to restart kernel to begin again.\n",
    "\n",
    "Retrieve and/or calculate the following properties across the fastq files to be processed (these values will be reported in script_metrics.txt):  \n",
    "<ul>\n",
    "  <li>Illumina sequencing run ID(s)</li>\n",
    "  <li>Total number of fastq files</li>\n",
    "  <li>Total number of sequencing reads</li>\n",
    "  <li>Size distribution of fastq files</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Double-check whether entries look good:\n",
    "print(\"\"\"\n",
    "---------------------------------------------------------------\n",
    "Preparation for output:\n",
    "Please double-check that your inputs were recorded as expected.\n",
    "---------------------------------------------------------------\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "Your OUTPUT DIRECTORY was recorded as:\n",
    "\"\"\")\n",
    "print(str(output_directory))\n",
    "\n",
    "print(\"\"\"\n",
    "Your directory containing fastq INPUT FILES was recorded as:\n",
    "\"\"\")\n",
    "print(str(fastq_directory))\n",
    "\n",
    "print(\"\"\"The following data were collected:  \"\"\")\n",
    "print(\"    Illumina sequencing run ID(s): \")\n",
    "for i in runIDlist:\n",
    "    print('        '+i)\n",
    "\n",
    "print(\"    # of fastq files to process: {0}\".format(len(myFastqFilenames)))\n",
    "\n",
    "print(\"    size distribution of fastq files to process: \\n      total... \"+str(round((sum(file for file in filesize))))+' MB \\n      range... max: '+str(round((max(file for file in filesize)),2))+' MB; min: '+str(round((min(file for file in filesize)),5))+' MB; median: '+str(round((numpy.median([file for file in filesize])),3))+' MB; mean +/- stdev: '+str(round((numpy.mean([file for file in filesize])),3))+' +/- '+str(round((numpy.std([file for file in filesize])),3))+' MB')\n",
    "\n",
    "print(\"    read distribution within fastq files to process: \\n      total... \"+locale.format_string(\"%d\", sum(readcount), grouping=True)+' reads \\n      range... max: '+str((max(file for file in readcount)))+' reads; min: '+str((min(file for file in readcount)))+' reads; median: '+str((numpy.median([file for file in readcount])))+' reads; mean +/- stdev: '+str(round((numpy.mean([file for file in readcount]))))+' +/- '+str(round((numpy.std([file for file in readcount]))))+' reads')\n",
    "\n",
    "print(\"\"\"\n",
    "Your FASTA REFERENCE FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(str(fasta_ref))\n",
    "\n",
    "print(\"\"\"\n",
    "Your BLASTN EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(str(blastn_path))\n",
    "\n",
    "print(\"\"\"\n",
    "Your MAKEBLASTDB EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(makeblastdb_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your BLASTN DATABASE FILE PREFIX was recorded as:\n",
    "\"\"\")\n",
    "print(db_prefix)\n",
    "\n",
    "print(\"\"\"\n",
    "Your FIMO EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(fimo_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your POSITION FREQUENCY FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(fimo_motifs_path)\n",
    "\n",
    "# Examine the reference file and indicate the ID, number of motifs, etc. print out list of factors for query\n",
    "motifcountlist = []\n",
    "with open(fimo_motifs_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if bool(re.search('MOTIF', line)): \n",
    "            motifcountlist.append(line.strip())\n",
    "\n",
    "print(\"\"\"\n",
    "# of TFBS motifs examined: \"\"\"+str(len(motifcountlist)))\n",
    "\n",
    "motifID = [i.split(' ')[2] for i in motifcountlist]\n",
    "motifID = sorted(motifID)\n",
    "chunked_motifID = [motifID[i: i+8] for i in range(0, len(motifID), 8)]\n",
    "\n",
    "print('Identities of TFBS motifs examined: ')\n",
    "\n",
    "for row in chunked_motifID:\n",
    "    itemnumber = (len(row)*'{: ^13} ').rstrip()\n",
    "    print(itemnumber.format(*row))\n",
    "\n",
    "print(\"\"\"\n",
    "Your FASTA-GET-MARKOV EXECUTABLE location was recorded as:\n",
    "\"\"\")\n",
    "print(fasta_get_markov_path)\n",
    "\n",
    "print(\"\"\"\n",
    "Your MARKOV BACKGROUND FILE location was recorded as:\n",
    "\"\"\")\n",
    "print(markov_background_file)\n",
    "\n",
    "check = input(\"\"\"\n",
    "Is this list accurately recorded? Type 'Y' or 'N': \n",
    "\"\"\")\n",
    "\n",
    "if check == 'Y':\n",
    "    pass\n",
    "elif check == 'N':\n",
    "    print(\"\"\"\n",
    "If you have corrections to make, please return to the appropriate cell to reset variables.\n",
    "To continue in the script, move to the next cell.\n",
    "To restart the script, click on the menu 'Kernel -> Restart'.  \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### III. Generate output directory and files, ready for script output  \n",
    "Script generates a single directory, populated with 5 files ready to accept script output.  \n",
    "Files are automatically named as in **'Output notes'** above, with current date appended to filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on script operation duration\n",
    "startTime = datetime.now()\n",
    "startTimestr = str(startTime).split(' ')[1].split('.')[0]\n",
    "\n",
    "# Proceed to file processing\n",
    "# Generate the directory and its files (to accept content later in script)\n",
    "path = str(output_directory)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "output_path = Path(output_directory)\n",
    "\n",
    "# Create output files\n",
    "filename_list = ['fasta.fa', 'blastn_alignments.txt', 'collated_TFBS.txt', 'markov_background.txt', 'script_metrics.txt']\n",
    "\n",
    "# Define current date as prefix to all filenames\n",
    "processdate = datetime.today().strftime(\"%m%d%Y\")\n",
    "\n",
    "for filename in filename_list:\n",
    "    with open(os.path.join(path, processdate+'_'+filename), 'wb') as file:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The file **script_metrics.txt** records script operation metadata (summarizes script input and performance); peform initial log of system information, user-defined variables and fastq file properties to script_metrics.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Collect RAM info\n",
    "mem = virtual_memory()\n",
    "ramem = mem.total/1073741824\n",
    "\n",
    "# Use print redirection to write to target file, in append mode (begin script_metrics.txt)\n",
    "with open(fasta_ref, \"r\") as f:\n",
    "    ref_seqs = f.readlines()\n",
    "    \n",
    "filename = Path(str(output_path)+'/'+processdate+'_script_metrics.txt')\n",
    "with open(filename, 'a') as f:\n",
    "    print(\"\"\"CollatedMotifs.py: Script Metrics\n",
    "Date: \"\"\" + (datetime.today().strftime(\"%m/%d/%Y\")) +\n",
    "\"\"\"\\n\\nOperating system information:\n",
    "    name: \"\"\" + socket.gethostname() +\n",
    "'\\n    platform: ' + platform.platform() +\n",
    "'\\n    RAM (GB): ' + str(ramem) +\n",
    "'\\n    physical CPU/effective CPU: ' + str(psutil.cpu_count(logical=False)) +'/'+ str(psutil.cpu_count()) +\n",
    "'\\n    executable: ' + psutil.Process().exe() +\n",
    "\"\"\"\\n\\nUser-entered variables:\n",
    "    output_directory: \"\"\"+ str(output_directory) +\n",
    "\"\\n    fastq_directory: \"+ str(fastq_directory) +\n",
    "\"\\n    fasta_ref: \"+ str(fasta_ref) +\n",
    "\"\\n    blastn_path: \"+ str(blastn_path) +\n",
    "\"\\n    makeblastdb_path: \"+ str(makeblastdb_path) +\n",
    "\"\\n    db_prefix: \"+ str(db_prefix) +\n",
    "\"\\n    fimo_path: \"+ str(fimo_path) +\n",
    "\"\\n    fimo_motifs_path: \"+ str(fimo_motifs_path) +\n",
    "\"\\n    fasta_get_markov_path: \"+ str(fasta_get_markov_path) +\n",
    "\"\\n    markov_background_file: \"+ str(markov_background_file) +\n",
    "\"\"\"\\n\\nfastq file information:\n",
    "    Illumina sequencing run ID(s): \"\"\"+ str(runIDlist).strip('[]').replace(\"'\",\"\") +\n",
    "\"\\n    Number of fastq files processed: \"+ str(len(myFastqFilenames)) +\n",
    "\"\"\"\\n    Size distribution of fastq files processed: \n",
    "        total... \"\"\" +str(round((sum(file for file in filesize))))+' MB \\n        range... max: '+str(round((max(file for file in filesize)),2))+' MB; min: '+str(round((min(file for file in filesize)),5))+' MB; median: '+str(round((numpy.median([file for file in filesize])),3))+' MB; mean +/- stdev: '+str(round((numpy.mean([file for file in filesize])),3))+' +/- '+str(round((numpy.std([file for file in filesize])),3))+' MB' +\n",
    "\"\\n    Read distribution within fastq files to process: \\n        total... \"+locale.format_string(\"%d\", sum(readcount), grouping=True)+' reads \\n        range... max: '+str((max(file for file in readcount)))+' reads; min: '+str((min(file for file in readcount)))+' reads; median: '+str((numpy.median([file for file in readcount])))+' reads; mean +/- stdev: '+str(round((numpy.mean([file for file in readcount]))))+' +/- '+str(round((numpy.std([file for file in readcount]))))+' reads', file = f)\n",
    "    print(\"\\nfastq files processed (name, size (MB), reads): \", file = f)\n",
    "    for i in (sorted(fastq_overview)):\n",
    "        print(\"    \" + str(i).strip(\"()\").replace(\"'\",\"\"), file = f)\n",
    "    print(\"\\nReference sequences provided in fasta_ref file: \", file = f)\n",
    "    for i in ref_seqs:\n",
    "        print(\"    \" + i.strip('\\n'), file = f)     \n",
    "    print(\"\\n# of TFBS motifs examined: \"+str(len(motifcountlist))+\n",
    "\"\\nIdentities of TFBS motifs examined: \", file = f)\n",
    "    for row in chunked_motifID:\n",
    "        itemnumber = (len(row)*'{: ^13} ').rstrip()\n",
    "        print(itemnumber.format(*row), file = f)        \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Create accessory files for BLASTN and FIMO operations:  \n",
    "- **MAKEBLASTDB** (NCBI) will now be used to prepare a reference sequence database for BLASTN alignments.  \n",
    "*The output of this operation is a set of 6 database files in alignments_directory.*  \n",
    "\n",
    "\n",
    "- **FASTA-GET-MARKOV** (MEME) will then be used to prepare a background markov file for FIMO statistical operations.  \n",
    "*The output of this operation is a single file, markov_background.txt, supplied to FIMO with sample and reference fasta files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on makeblastdb and fastgetmarkov operations\n",
    "startTime_makeblastdb_fastagetmarkov_operations = datetime.now()\n",
    "\n",
    "# Construct alignment database, in alignment_database directory\n",
    "# Reference sequence input\n",
    "mydb_input = Path(fasta_ref)\n",
    "\n",
    "# Alignment database directory\n",
    "mydb_output = Path(str(output_directory)+'/alignment_database')\n",
    "\n",
    "os.makedirs(mydb_output)\n",
    "\n",
    "# 'Make blastn database' command (usage: makeblastdb -in mydb.fsa -parse_seqids -dbtype nucl -out path)\n",
    "cmd_makeblastndb = str(makeblastdb_path)+' -in '+str(mydb_input)+' -parse_seqids -dbtype nucl -out '+str(mydb_output)+'/'+db_prefix\n",
    "\n",
    "os.system(cmd_makeblastndb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Construct background markov file to be used by FIMO\n",
    "# Markov input file (markov background file)\n",
    "markovbackground_input = Path(markov_background_file)\n",
    "\n",
    "# Markov background output file  \n",
    "markovbackground_output = Path(str(output_directory)+'/'+processdate+'_markov_background.txt')\n",
    "\n",
    "# 'Make markov background file' command (usage: fasta-get-markov [options] [<sequence file> [<background file>]])\n",
    "cmd_fastagetmarkov = str(fasta_get_markov_path)+' -dna '+str(markovbackground_input)+' '+str(markovbackground_output)\n",
    "\n",
    "os.system(cmd_fastagetmarkov)\n",
    "\n",
    "# Log makeblastdb and fastgetmarkov operations time duration\n",
    "makeblastdb_fastagetmarkov_operationsDuration = str(datetime.now()- startTime_makeblastdb_fastagetmarkov_operations).split(':')[0]+' hr|'+str(datetime.now() - startTime_makeblastdb_fastagetmarkov_operations).split(':')[1]+' min|'+str(datetime.now() - startTime_makeblastdb_fastagetmarkov_operations).split(':')[2].split('.')[0]+' sec|'+str(datetime.now()- startTime_makeblastdb_fastagetmarkov_operations).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Identify candidate alleles: fasta file, BLASTN alignment, and assignment of alleles to samples  \n",
    "Deep sequencing of amplicons can yield hundreds to thousands of reads per sample; read frequencies can be used to gauge relative read abundance and, ultimately, to infer probable genotype (sequence ID(s) of the source template(s)).\n",
    "<img src=\"CollatedMotifs_img/fasta_thumbnail.png\" align=\"left\" width=\"560\">  \n",
    "\n",
    "**Count reads.** This script parses sample-specific fastq files for unique read types, counts the abundance of these read types, and reports the top 5 most abundant read types in the form of fasta entries. For each sample, **each of the 5 ranked sequences is reported with its frequency metrics** in a corresponding fasta definition line (defline).  \n",
    "The output of this step is a fasta file (.fa) that will be created in the user-specified OUTPUT DIRECTORY.  \n",
    "\n",
    "**Align reads to reference.** This fasta file is then presented to **BLASTN** (with the reference sequence database specified during user input) for alignments.\n",
    "\n",
    "**Define candidate alleles.** The script then parses the alignments to organize alignment data for the 'top 10' reads assigned to each sample, in a single dictionary called **'alignmentoutput_dict'**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on read count duration      \n",
    "startTime_readcount = datetime.now()\n",
    "\n",
    "# Populate fasta files for fasta.fa, in preparation for fimo analysis      \n",
    "query_input = Path(str(output_directory)+'/'+processdate+'_fasta.fa')\n",
    "\n",
    "# Merge R1 and R2 reads, if present, as single sequence\n",
    "R1_file_list = [sourcefile for sourcefile in myFastqFilenames if bool(re.split('_',os.path.basename(sourcefile))[3] == 'R1')]\n",
    "R2_file_list = [sourcefile for sourcefile in myFastqFilenames if bool(re.split('_',os.path.basename(sourcefile))[3] == 'R2')]  \n",
    "\n",
    "# R1, R2 cluster mapping\n",
    "processed_files_list = []\n",
    "R1_R2_map_list = []\n",
    "for sourcefile in myFastqFilenames:\n",
    "    if sourcefile in processed_files_list:\n",
    "        pass\n",
    "    else:\n",
    "        testname = ''.join(re.split('_',os.path.basename(sourcefile))[0:3])\n",
    "        for sourcefile1 in R1_file_list:\n",
    "            if testname == ''.join(re.split('_',os.path.basename(sourcefile1))[0:3]):\n",
    "                R1 = sourcefile\n",
    "                if sourcefile not in processed_files_list:\n",
    "                    processed_files_list.append(sourcefile)\n",
    "        for sourcefile2 in R2_file_list:\n",
    "            if testname == ''.join(re.split('_',os.path.basename(sourcefile2))[0:3]):\n",
    "                R2 = sourcefile2\n",
    "                if sourcefile2 not in processed_files_list:\n",
    "                    processed_files_list.append(sourcefile2)\n",
    "        R1_R2_map_list.append((R1, R2))\n",
    "        \n",
    "# Make fasta file of read entries, direct top read count output and annotation to fasta.fa   \n",
    "for file_pair in R1_R2_map_list:\n",
    "    R1_file = file_pair[0]\n",
    "    R2_file = file_pair[1]\n",
    "    fastaname = re.split('_', os.path.basename(R1_file))\n",
    "    cluster_sequence_R1_dict = {}\n",
    "    cluster_sequence_R2_dict = {}\n",
    "    cluster_sequence_R2_revcomp_dict = {}\n",
    "    cluster_merged_R1_R2revcomp_dict = {}\n",
    "    cluster_merged_R1_R2revcomp_dict2 = {}\n",
    "    merged_read_list = []\n",
    "    counter=()\n",
    "    if Path(R1_file).suffix == \".gz\":\n",
    "        with gzip.open(R1_file, \"rt\") as f:\n",
    "            lines_R1 = f.readlines()\n",
    "    elif Path(R1_file).suffix == \".fastq\":\n",
    "        with open(R1_file, 'r') as f:\n",
    "            lines_R1 = f.readlines()    \n",
    "    for x in range(0,len(lines_R1),4): \n",
    "        cluster_sequence_R1_dict[lines_R1[x].split(':')[5]+':'+lines_R1[x].split(':')[6].split(' ')[0]] = lines_R1[x+1].strip('\\n')\n",
    "    #cluster_IDs_list_R1 = [x.split(':')[5]+':'+x.split(':')[6].split(' ')[0] for x in lines_R1[0::4]]\n",
    "    if Path(R2_file).suffix == \".gz\":\n",
    "        with gzip.open(R2_file, \"rt\") as f:\n",
    "            lines_R2 = f.readlines()\n",
    "    elif Path(R2_file).suffix == \".fastq\":\n",
    "        with open(R2_file, 'r') as f:\n",
    "            lines_R2 = f.readlines()\n",
    "    for x in range(0,len(lines_R2),4): \n",
    "        cluster_sequence_R2_dict[lines_R2[x].split(':')[5]+':'+lines_R2[x].split(':')[6].split(' ')[0]] = lines_R2[x+1].strip('\\n')\n",
    "    #cluster_IDs_list_R2 = [x.split(':')[5]+':'+x.split(':')[6].split(' ')[0] for x in lines_R2[0::4]]\n",
    "    for cluster in cluster_sequence_R2_dict:\n",
    "        cluster_sequence_R2_revcomp_dict[cluster] = ''.join(reversed(''.join(nt_dict.get(nt) for nt in cluster_sequence_R2_dict.get(cluster))))\n",
    "    for cluster in cluster_sequence_R1_dict:\n",
    "        if cluster in cluster_sequence_R2_revcomp_dict:\n",
    "            if merge(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster)) != 'no overlap':\n",
    "                cluster_merged_R1_R2revcomp_dict[cluster] = merge(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster))\n",
    "            else:\n",
    "                cluster_merged_R1_R2revcomp_dict2[cluster] = merge1(cluster_sequence_R1_dict.get(cluster), cluster_sequence_R2_revcomp_dict.get(cluster))\n",
    "    for cluster in cluster_merged_R1_R2revcomp_dict:\n",
    "        merged_read_list.append(cluster_merged_R1_R2revcomp_dict.get(cluster))\n",
    "    counter=Counter(merged_read_list)\n",
    "    modified_read_list_top5 = []\n",
    "    for i in counter.most_common(5):\n",
    "        filtered1 = sum([x for x in counter.values() if x/(sum(counter.values())) > 0.01])\n",
    "        filtered10 = sum([x for x in counter.values() if x/(sum(counter.values())) > 0.1])\n",
    "        raw_freq = round((100*i[1]/sum(counter.values())),2)\n",
    "        modified_read_list_top5.append([i[0], '['+str(i[1])+'/'+str(sum(counter.values()))+']', raw_freq, int(stats.percentileofscore([i for i in counter.values()], i[1], 'rank')), round((100*i[1]/sum([i[1] for i in counter.most_common(5)])),2), round((100*i[1]/filtered1),2) if filtered1 > 0 and raw_freq >= 1 else 'None', round((100*i[1]/filtered10),2) if filtered10 > 0 and raw_freq >= 10 else 'None'])\n",
    "    with open(str(query_input), 'a+') as file:\n",
    "        for i in modified_read_list_top5:\n",
    "              file.write('>'+fastaname[0]+'_'+'R1+R2'+'_'+str(i[1])+'_%totalreads:'+str(i[2])+'_percentile:'+str(i[3])+'_%top5reads:'+str(i[4])+'_%readsfilteredfor1%:'+str(i[5])+'_%readsfilteredfor10%:'+str(i[6])+'\\n'+i[0]+'\\n')\n",
    "                \n",
    "# Log read count time duration      \n",
    "readcountDuration = str(datetime.now()- startTime_readcount).split(':')[0]+' hr|'+str(datetime.now() - startTime_readcount).split(':')[1]+' min|'+str(datetime.now() - startTime_readcount).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_readcount).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process alignments to reference sequence database, using **BLASTN** (NCBI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on blastn alignments duration  \n",
    "startTime_alignments = datetime.now()\n",
    "\n",
    "# Process alignments relative to reference sequence database, using blastn\n",
    "# Reference database\n",
    "db_input = mydb_output / db_prefix\n",
    "\n",
    "# Alignment output\n",
    "query_output = str(output_directory)+'/'+processdate+'_blastn_alignments.txt'\n",
    "\n",
    "# Alignment command\n",
    "cmd_align = str(blastn_path)+' -strand plus -query '+str(query_input)+' -db '+str(db_input)+' -out '+str(query_output)+' -gapopen 1 -gapextend 1 -outfmt \"5\"'\n",
    "\n",
    "os.system(cmd_align)\n",
    "\n",
    "# Log alignment time duration\n",
    "alignmentsDuration = str(datetime.now()- startTime_alignments).split(':')[0]+' hr|'+str(datetime.now()- startTime_alignments).split(':')[1]+' min|'+str(datetime.now()- startTime_alignments).split(':')[2].split('.')[0]+' sec|'+str(datetime.now()- startTime_alignments).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Define alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on allele definitions duration      \n",
    "startTime_alleles = datetime.now()\n",
    " \n",
    "# Import blastn alignments output as a list of strings (each string corresponds to a query alignment)      \n",
    "alignments_list = []\n",
    "with open(str(query_output), 'r') as file:\n",
    "    reader = file.read()\n",
    "    for i,part in enumerate(reader.split('<Iteration_iter-num>')):\n",
    "        alignments_list.append(part)\n",
    "# Remove blastn header line from alignments_list\n",
    "alignments_list = alignments_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert alignments_list to list of lists (i.e., each query alignment string is encapsulateed into its own sublist within alignments_list2)\n",
    "alignments_list2 = [alignments_list[i:i+1] for i in range(0, len(alignments_list))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Subset sample IDs and/or associated reads for which *(1) no alignment* was found in reference database, or *(2) multiple hits* were identified in reference database. These are ultimately removed from further analysis, but the identities of samples and/or associated reads that were filtered by these criteria are ultimately reported in 'population_summary.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify & subset queries for which no alignments were found in reference database ('no hits found')\n",
    "no_hits_list = []\n",
    "for i in alignments_list2:\n",
    "    if re.search('No hits found', str(i)):\n",
    "        no_hits_list.append(str(i).split('<Iteration_query-def>')[1].split('</Iteration_query-def>')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Further subset 'No hits found' queries for R1 vs. R2      \n",
    "no_hits_R1_read_list = []\n",
    "no_hits_R2_read_list = []\n",
    "for i in no_hits_list:\n",
    "    if i.split('_')[1] == 'R1':\n",
    "        no_hits_R1_read_list.append(i.split('_')[0]+' '+i.split('_')[2]+' '+i.split('_')[3].split(':')[1]+'%')\n",
    "    elif i.split('_')[1] == 'R2':\n",
    "        no_hits_R2_read_list.append(i.split('_')[0]+' '+i.split('_')[2]+' '+i.split('_')[3].split(':')[1]+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Record sample names having reads with no alignment hits      \n",
    "no_hits_samplename_list = []\n",
    "for i in no_hits_list:\n",
    "    samplename = i.split('_')[0]\n",
    "    if samplename not in no_hits_samplename_list:\n",
    "        no_hits_samplename_list.append(samplename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Within each sublist of alignments_list2, split each line into an individual string, remove beginning and trailing whitespace, and recapture specified subset of alignment information in alignments_list3\n",
    "alignments_list3 = []\n",
    "for i in alignments_list2:\n",
    "    if str(i).split('<Iteration_query-def>')[1].split('</Iteration_query-def>')[0] not in no_hits_list:\n",
    "        alignments_list3.append([y.strip() for x in i for y in x.split('\\n') if y.strip().startswith(('<Iteration_query-ID>', '<Iteration_query-def>', '<Hit_num>', '<Hit_id>', '<Hit_def>', '<Hsp_hit-from>', '<Hsp_hit-to>', '<Hsp_qseq>', '<Hsp_hseq>', '<Hsp_midline>'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify & subset reads with >1 alignment to sequences in reference database\n",
    "multiple_alignments_list = []\n",
    "for i in alignments_list3:\n",
    "    if len(re.findall('<Hit_num>', str(i))) > 1:\n",
    "        multiple_alignments_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify read IDs with >1 alignment to sequences in reference database\n",
    "multiple_alignments_readID_list = []\n",
    "for i in multiple_alignments_list:\n",
    "    multiple_alignments_readID_list.append(i[1].split('>')[1].split('<')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Record sample names having reads with >1 alignment to sequences in reference database      \n",
    "multiple_alignments_samplename_list = []\n",
    "for i in multiple_alignments_readID_list:\n",
    "    samplename = i.split('_')[0]\n",
    "    if samplename not in multiple_alignments_samplename_list:\n",
    "        multiple_alignments_samplename_list.append(samplename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare dictionary linking sample names to their reads having >1 alignment to sequences in reference database      \n",
    "multiple_alignments_dict = {}\n",
    "for i in multiple_alignments_samplename_list:\n",
    "    multiple_alignments_dict [\"{0}\".format(i)] = tuple(x for x in multiple_alignments_list if bool(re.search(i, x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finalize list containing candidate alleles with single alignment hit in reference database.  \n",
    "Prepare **'alignmentoutput_dict'**, a dictionary that aggregates all sample-associated alleles as sublists within a single list (value) assigned to appropriate sample name ID (key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare alignment_list4 for reads with exclusively 1 alignment hit in reference database\n",
    "alignments_list4 = []\n",
    "for i in alignments_list3:\n",
    "    if i not in multiple_alignments_list:\n",
    "        alignments_list4.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Among lists containing alignment data in alignments_list4, determine which queries (reads) correspond to the same sample; where querydef = i[1].split(\">\")[1].split(\"_[\")[0], reads belonging to the same sample share identical querydef\n",
    "# Fasta deflines encode frequency metrics for reads, based on defline format:\n",
    "# sampleID_[reads/total reads]_percentile_% read abundance_% top 10 reads_% reads filtered for 1%_% reads filtered for 10%\n",
    "querydef_list = []\n",
    "for i in alignments_list3:\n",
    "    querydef = i[1].split(\">\")[1].split(\"_\")[0]\n",
    "    querydef_list.append(querydef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "querydef_uniq_list = []\n",
    "for i in querydef_list:\n",
    "    if i in querydef_uniq_list:\n",
    "        pass\n",
    "    else:\n",
    "        querydef_uniq_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare dictionary relating sample IDs to their associated reads ('alleles')      \n",
    "alignmentoutput_dict = {}\n",
    "for i in querydef_uniq_list:\n",
    "    alignmentoutput_dict[\"{0}\".format(i)] = tuple(x for x in alignments_list4 if bool(re.search(i, x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Identify sample IDs for which no valid candidate alleles were identified. These samples are not further analyzed, but their identities are reported in 'population_summary.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Identify & subset sample ID's that do not have output alleles (empty tuple values in dictionary)\n",
    "empty_sampleIDs_list = []\n",
    "for i in alignmentoutput_dict:\n",
    "    if bool(alignmentoutput_dict.get(i) == ()):\n",
    "        empty_sampleIDs_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of alignmentoutput_dict, removing dictionary keys with empty tuple values\n",
    "alignmentoutput_dict2 = { k : v for k,v in alignmentoutput_dict.items() if v}\n",
    "# Alignmentoutput_dict2 is the key dictionary for alignment information\n",
    "\n",
    "# Log allele definitions time duration\n",
    "allele_definitionsDuration = str(datetime.now() - startTime_alleles).split(':')[0]+' hr|'+str(datetime.now() - startTime_alleles).split(':')[1]+' min|'+str(datetime.now() - startTime_alleles).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_alleles).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Identify TFBS matches to motifs (FIMO) in reference and candidate allele sequences\n",
    "Data for sample-specific alleles were assembled in **alignmentoutput_dict**, a dictionary that collected alignment data for each sample's top 5 reads, with each read's frequency metrics maintained in the allele name (defline). The contents of this dictionary are now further parsed, along with TFBS data collected by FIMO in **fimo.tsv** files, to generate repositories for TFBS matches identified for reference and allele sequences (**dict_ref_TFBS**, **dict_allele_TFBS**). TFBS in allele sequences are then compared to TFBS in cognate reference sequences to assemble **dict_allele_TFBS_synopsis**, which logs TFBS **gained** and **lost** in each allele relative to reference sequence.\n",
    "\n",
    "--------\n",
    "The output of these analytics is reported in **'collated_TFBS.txt'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on FIMO operations duration      \n",
    "startTime_fimo = datetime.now()\n",
    "\n",
    "# Reference sequence input\n",
    "ref_input = Path(fasta_ref)\n",
    "\n",
    "# Reference sequence(s): FIMO file output directory\n",
    "ref_TFBS_output = output_path / 'fimo_out_ref'\n",
    "\n",
    "# alleles: FIMO file output directory      \n",
    "allele_TFBS_output = output_path / 'fimo_out'\n",
    "\n",
    "# Reference sequence(s): FIMO command (usage: fimo --bfile <background file> <motif file> <sequence file>)       \n",
    "cmd_TFBS = str(fimo_path)+' --bfile '+str(markovbackground_output)+' --o '+str(ref_TFBS_output)+' --thresh '+pval_threshold+' '+str(fimo_motifs_path)+' '+str(ref_input)\n",
    "\n",
    "os.system(cmd_TFBS)\n",
    "\n",
    "# Alleles: FIMO command (usage: fimo --bfile <background file> <motif file> <sequence file>) \n",
    "cmd_TFBS = str(fimo_path)+' --bfile '+str(markovbackground_output)+' --o '+str(allele_TFBS_output)+' --thresh '+pval_threshold+' '+str(fimo_motifs_path)+' '+str(query_input)\n",
    "\n",
    "os.system(cmd_TFBS)\n",
    "\n",
    "# Log FIMO operations time duration\n",
    "fimoDuration = str(datetime.now() - startTime_fimo).split(':')[0]+' hr|'+str(datetime.now() - startTime_fimo).split(':')[1]+' min|'+str(datetime.now() - startTime_fimo).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_fimo).split(':')[2].split('.')[1]+' microsec' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Collate TFBS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start the clock on TFBS collation operations duration\n",
    "startTime_TFBScollation = datetime.now()\n",
    "\n",
    "# TFBS output files exist for both reference sequences and alleles (two separate files, in two separate directories: fimo_out & fimo_out_ref)  \n",
    "# Prepare dictionary of TFBSs ID'ed, for each reference sample \n",
    "with open(str(ref_TFBS_output)+'/fimo.tsv', 'r') as file:\n",
    "    ref_lines = file.readlines()\n",
    "    \n",
    "# Remove FIMO header lines, etc.\n",
    "ref_lines = ref_lines[1:]\n",
    "for line in ref_lines.copy():\n",
    "    if len(line.split('\\t')) < 10:\n",
    "        ref_lines.remove(line)\n",
    "        \n",
    "# Convert to set for faster processing\n",
    "ref_lines = set(ref_lines)\n",
    "\n",
    "with open(str(ref_input)) as file:\n",
    "    fasta_lines = file.readlines()\n",
    "fasta_names = fasta_lines[0::2]\n",
    "fasta_names = [i.strip('\\n').strip('>') for i in fasta_names]\n",
    "ref_set = set(fasta_names)\n",
    "\n",
    "dict_ref_TFBS = {}\n",
    "for ref in ref_set:\n",
    "    dict_ref_TFBS[ref] = [] \n",
    "    \n",
    "# Take 3rd field of all lines as search for presence of key in dictionary, and add line as string in value list of key (allele)\n",
    "for line in ref_lines:\n",
    "    if line.split('\\t')[2].strip() in dict_ref_TFBS:\n",
    "        dict_ref_TFBS[line.split('\\t')[2]].append(line.strip())\n",
    "        \n",
    "# Prepare allele dictionary; first, populate with 'all_sites'.  Then, run comparison to 'dict_TFBS_ref' to define sites that are lost vs. gained relative to reference sequence.\n",
    "# Prepare dictionary of TFBSs ID'ed, for each sample allele\n",
    "\n",
    "dict_allele_TFBS = {}\n",
    "for allele in alignmentoutput_dict2:\n",
    "    dict_allele_TFBS[allele] = {}\n",
    "\n",
    "for allele in alignmentoutput_dict2:\n",
    "    for x in range(0, len(alignmentoutput_dict2.get(allele))):\n",
    "        dict_allele_TFBS[allele].update({alignmentoutput_dict2.get(allele)[x][1].split(\">\")[1].split(\"<\")[0]:[]})\n",
    "        \n",
    "with open(str(allele_TFBS_output)+'/fimo.tsv', 'r') as file:\n",
    "    allele_lines = file.readlines() \n",
    "\n",
    "# Remove FIMO header lines, etc.\n",
    "allele_lines = allele_lines[1:]\n",
    "for line in allele_lines.copy():\n",
    "    if len(line.split('\\t')) < 10:\n",
    "        allele_lines.remove(line)\n",
    "        \n",
    "# Convert to set for faster processing\n",
    "allele_lines = set(allele_lines)\n",
    "\n",
    "# Populate each allele with its 'all_sites' information\n",
    "for line in allele_lines:\n",
    "    dict_allele_TFBS_sample_key = line.split('\\t')[2].strip().split('_')[0]\n",
    "    dict_allele_TFBS_allele_key = line.split('\\t')[2].strip()\n",
    "    if dict_allele_TFBS_sample_key in dict_allele_TFBS:\n",
    "        if dict_allele_TFBS.get(dict_allele_TFBS_sample_key).get(dict_allele_TFBS_allele_key) is not None:\n",
    "            dict_allele_TFBS.get(dict_allele_TFBS_sample_key).get(dict_allele_TFBS_allele_key).append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Synopsis dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare synopsis dictionary with alleles as keys, and list of 3 sublists: gained, lost, all_sites      \n",
    "# Run comparison of 'all_sites' information relative to reference allele information.\n",
    "dict_allele_TFBS_synopsis = {}\n",
    "for allele in alignmentoutput_dict2:\n",
    "    dict_allele_TFBS_synopsis[allele] = {}\n",
    "    \n",
    "for allele in alignmentoutput_dict2:\n",
    "    for x in range(0, len(alignmentoutput_dict2.get(allele))):\n",
    "        dict_allele_TFBS_synopsis[allele].update({alignmentoutput_dict2.get(allele)[x][1].split(\">\")[1].split(\"<\")[0]:{'gained':[],'lost':[],'all_sites':[], 'TFs':{}}})\n",
    "        \n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        for motif in dict_allele_TFBS.get(sample).get(allele):\n",
    "            dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites').append(motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+'),'+motif.split('\\t')[5]+','+motif.split('\\t')[9]+','+motif.split('\\t')[7])\n",
    "            if motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')' not in dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs'):\n",
    "                count = 1\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').update({motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')':count})\n",
    "            else:\n",
    "                count = dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').get(motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')')+1\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('TFs').update({motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')':count})\n",
    "                \n",
    "# Run comparisons:\n",
    "# Make ref_TFBS_synopsis dictionary\n",
    "dict_ref_TFBS_synopsis = {}\n",
    "for ref in dict_ref_TFBS:\n",
    "    dict_ref_TFBS_synopsis[ref] = {'all_sites':[], 'TFs':{}}\n",
    "    \n",
    "\n",
    "# Summarize TF counts in reference sequences      \n",
    "for ref in dict_ref_TFBS_synopsis:    \n",
    "    for motif in dict_ref_TFBS.get(ref):\n",
    "        if motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')' not in dict_ref_TFBS_synopsis.get(ref).get('TFs'):\n",
    "            count = 1\n",
    "            dict_ref_TFBS_synopsis.get(ref).get('TFs').update({motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+')':count})\n",
    "        else:\n",
    "            count = dict_ref_TFBS_synopsis.get(ref).get('TFs').get(motif.split('\\t')[0]+' ('+motif.split('\\t')[1]+')')+1\n",
    "            dict_ref_TFBS_synopsis.get(ref).get('TFs').update({motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+')':count})\n",
    "            \n",
    "# Catalog TFBSs in reference sequences, in format akin to 'all_sites' format in dict_allele_TFBS_synopsis          \n",
    "for ref in dict_ref_TFBS_synopsis:\n",
    "    for motif in dict_ref_TFBS.get(ref):\n",
    "        dict_ref_TFBS_synopsis.get(ref).get('all_sites').append(motif.split('\\t')[1]+' ('+motif.split('\\t')[0]+'),'+motif.split('\\t')[5]+','+motif.split('\\t')[9]+','+motif.split('\\t')[7])\n",
    "        \n",
    "# Run comparisons, populating into dict_allele_TFBS_synopsis\n",
    "ref_options = [ref for ref in dict_ref_TFBS]\n",
    "for sample in dict_allele_TFBS_synopsis:\n",
    "    # define reference sequence appropriate to sample\n",
    "    for ref in ref_options:\n",
    "        if re.search(ref, sample):\n",
    "            sample_ref = ref\n",
    "    for allele in dict_allele_TFBS_synopsis.get(sample):\n",
    "        for motif in dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites'):\n",
    "            if motif in dict_ref_TFBS_synopsis.get(sample_ref).get('all_sites'):\n",
    "                pass\n",
    "            else:\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('gained').append(motif)\n",
    "        for motif in dict_ref_TFBS_synopsis.get(sample_ref).get('all_sites'):\n",
    "            if motif in dict_allele_TFBS_synopsis.get(sample).get(allele).get('all_sites'):\n",
    "                pass\n",
    "            else:\n",
    "                dict_allele_TFBS_synopsis.get(sample).get(allele).get('lost').append(motif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Print to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Relate allele definition (alignments in alignmentoutput_dict2) to TFBS collation for each allele of each sample (with focus on lost and gained TFBS for each allele, relative to reference)\n",
    "collatedTFBS_output = Path(str(output_path)+ '/'+processdate+'_collated_TFBS.txt')\n",
    "with open(str(collatedTFBS_output), 'a+') as f:\n",
    "    print('CollatedMotifs.py: Summary of matches to TFBS motifs detected in sample sequence(s) relative to reference\\nDate: ' + (datetime.today().strftime(\"%m/%d/%Y\")) + '\\n\\n', file = f)\n",
    "    for i in sorted(dict_allele_TFBS_synopsis):\n",
    "        print((len(i)*'=')+'\\n'+i+'\\n'+(len(i)*'='), file = f)\n",
    "        for allele in dict_allele_TFBS_synopsis.get(i):\n",
    "            for x in range(0, len(alignmentoutput_dict2.get(i))):\n",
    "                if alignmentoutput_dict2.get(i)[x][1].split('>')[1].split('<')[0] == allele:\n",
    "                    test = alignmentoutput_dict2.get(i)[x]\n",
    "            sum_gained_motifs = []\n",
    "            sum_lost_motifs = []\n",
    "            sum_motifs = []\n",
    "            sum_TFs = []\n",
    "            TFs_gt1 = []\n",
    "            lost_motifs_plus_strand = []\n",
    "            lost_motifs_minus_strand = []\n",
    "            total_lost_motifs = []\n",
    "            total_lost_motifs_list = []\n",
    "            gained_motifs_plus_strand = []\n",
    "            gained_motifs_minus_strand = []\n",
    "            total_gained_motifs = []\n",
    "            sum_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('all_sites')))\n",
    "            #print(allele+' '+sum_motifs)\n",
    "            sum_TFs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs')))\n",
    "            TFs_gt1 = str(len([TF for TF in dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs') if dict_allele_TFBS_synopsis.get(i).get(allele).get('TFs').get(TF) > 1]))\n",
    "            sum_lost_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('lost')))\n",
    "            sum_gained_motifs = str(len(dict_allele_TFBS_synopsis.get(i).get(allele).get('gained')))\n",
    "            # lost\n",
    "            lost_motifs_plus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost') if motif.split(',')[1] == '+']\n",
    "            lost_motifs_minus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost') if motif.split(',')[1] == '-']\n",
    "            total_lost_motifs = lost_motifs_plus_strand+lost_motifs_minus_strand\n",
    "            total_lost_motifs_dict = dict(Counter(total_lost_motifs))\n",
    "            total_lost_motifs_list = [i+':'+str(total_lost_motifs_dict.get(i)) for i in total_lost_motifs_dict]\n",
    "            total_lost_motifs_list = sorted(total_lost_motifs_list)\n",
    "            # gained\n",
    "            gained_motifs_plus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained') if motif.split(',')[1] == '+']\n",
    "            gained_motifs_minus_strand = [motif.split(' ')[0] for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained') if motif.split(',')[1] == '-']\n",
    "            total_gained_motifs = gained_motifs_plus_strand+gained_motifs_minus_strand\n",
    "            total_gained_motifs_dict = dict(Counter(total_gained_motifs))\n",
    "            total_gained_motifs_list = [i+':'+str(total_gained_motifs_dict.get(i)) for i in total_gained_motifs_dict]\n",
    "            total_gained_motifs_list = sorted(total_gained_motifs_list)\n",
    "            print(3*' '+'Allele: '+allele.replace('_',' | ')+'\\n   Motifs: total distinct sites |'+sum_motifs+'|, total unique TFs |'+sum_TFs+'| (motifs for '+TFs_gt1+' TFs occur >1x)', file = f)\n",
    "            print(' Synopsis: relative to reference sequence--# lost sites |'+sum_lost_motifs+'|, # new sites |'+sum_gained_motifs+'|', file = f)\n",
    "            print('  Details: lost |'+str(total_lost_motifs_list).strip('[]').replace(\"'\",\"\")+'|', file = f)\n",
    "            print('            new |'+str(total_gained_motifs_list).strip('[]').replace(\"'\",\"\")+'|', file = f)\n",
    "            # prepare complete visual mapping of new motifs above allele sequence\n",
    "            if int(sum_gained_motifs) > 0: \n",
    "                print('\\n'+11*' '+'NEW motifs:', file = f)\n",
    "                motif_plus_tracker = []\n",
    "                motif_minus_tracker = []\n",
    "                new_motif_plus_list = []\n",
    "                new_motif_minus_list = []\n",
    "                for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('gained'):\n",
    "                    if motif.split(',')[1] == '+':\n",
    "                        new_motif_plus_list.append(motif)\n",
    "                    elif motif.split(',')[1] == '-':\n",
    "                        new_motif_minus_list.append(motif)\n",
    "                for new_motif_plus in new_motif_plus_list:\n",
    "                    if len(motif_plus_tracker) == 0:\n",
    "                        print(11*' '+'plus(+) strand:', file = f)\n",
    "                        motif_plus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    if re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0]):\n",
    "                        match = re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0])\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+new_motif_plus.split(',')[0]+' (pval '+new_motif_plus.split(',')[3]+')', file = f)\n",
    "                    elif re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0].replace('-','')):\n",
    "                        match = re.search(new_motif_plus.split(',')[2],test[7].split('>')[1].split('<')[0].replace('-',''))\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+new_motif_plus.split(',')[0]+' (pval '+new_motif_plus.split(',')[3]+')'+' [note, approx. position]', file = f)\n",
    "                for new_motif_minus in new_motif_minus_list:\n",
    "                    if len(motif_minus_tracker) == 0:\n",
    "                        print(11*' '+'minus(-) strand:', file = f)\n",
    "                        motif_minus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    seq_revcomp = ''.join(reversed(''.join(nt_dict.get(nt) for nt in test[7].split('>')[1].split('<')[0])))\n",
    "                    if re.search(new_motif_minus.split(',')[2],seq_revcomp):\n",
    "                        match = re.search(new_motif_minus.split(',')[2],seq_revcomp)\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(new_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+new_motif_minus.split(',')[0]+' (pval '+new_motif_minus.split(',')[3]+')'), file = f)\n",
    "                    elif re.search(new_motif_minus.split(',')[2],seq_revcomp.replace('-','')):\n",
    "                        match = re.search(new_motif_minus.split(',')[2],seq_revcomp.replace('-',''))\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(new_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+new_motif_minus.split(',')[0]+' (pval '+new_motif_minus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "            else:\n",
    "                pass\n",
    "            print('\\n'+4*' '+'query  '+test[7].split('>')[1].split('<')[0]+'\\n'+11*' '+test[9].split('>')[1].split('<')[0]+'\\n'+' reference '+test[8].split('>')[1].split('<')[0]+'\\n', file = f)\n",
    "            if int(sum_lost_motifs) > 0:\n",
    "                print(11*' '+'LOST motifs:', file = f)\n",
    "                motif_plus_tracker = []\n",
    "                motif_minus_tracker = []\n",
    "                lost_motif_plus_list = []\n",
    "                lost_motif_minus_list = []\n",
    "                for motif in dict_allele_TFBS_synopsis.get(i).get(allele).get('lost'):\n",
    "                    if motif.split(',')[1] == '+':\n",
    "                        lost_motif_plus_list.append(motif)\n",
    "                    elif motif.split(',')[1] == '-':\n",
    "                        lost_motif_minus_list.append(motif)\n",
    "                for lost_motif_plus in lost_motif_plus_list:\n",
    "                    if len(motif_plus_tracker) == 0:\n",
    "                        print(11*' '+'plus(+) strand:', file = f)\n",
    "                        motif_plus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    if re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0]):\n",
    "                        match = re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0])\n",
    "                        distance = match.span()[0]\n",
    "                        sequence = match.group()\n",
    "                        print((11+distance)*' '+sequence+' |-- '+lost_motif_plus.split(',')[0]+' (pval '+lost_motif_plus.split(',')[3]+')', file = f)\n",
    "                    elif re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0].replace('-','')):\n",
    "                        match = re.search(lost_motif_plus.split(',')[2],test[8].split('>')[1].split('<')[0].replace('-',''))\n",
    "                        distance = len(test[8].split('>')[1].split('<')[0])-match.span()[0]-len(lost_motif_plus.split(',')[2])\n",
    "                        sequence = match.group()\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_plus.split(',')[0]+' (pval '+lost_motif_plus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "                for lost_motif_minus in lost_motif_minus_list:\n",
    "                    if len(motif_minus_tracker) == 0:\n",
    "                        print(11*' '+'minus(-) strand:', file = f)\n",
    "                        motif_minus_tracker.append('check')\n",
    "                    else:\n",
    "                        pass\n",
    "                    seq_revcomp = ''.join(reversed(''.join(nt_dict.get(nt) for nt in test[8].split('>')[1].split('<')[0])))\n",
    "                    if re.search(lost_motif_minus.split(',')[2],seq_revcomp):\n",
    "                        match = re.search(lost_motif_minus.split(',')[2],seq_revcomp)\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(lost_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_minus.split(',')[0]+' (pval '+lost_motif_minus.split(',')[3]+')'), file = f)\n",
    "                    elif re.search(lost_motif_minus.split(',')[2],seq_revcomp.replace('-','')):\n",
    "                        match = re.search(lost_motif_minus.split(',')[2],seq_revcomp.replace('-',''))\n",
    "                        distance = len(seq_revcomp)-match.span()[0]-len(lost_motif_minus.split(',')[2])\n",
    "                        sequence = ''.join(reversed(match.group()))\n",
    "                        print((11*' '+distance*' '+sequence+' |-- '+lost_motif_minus.split(',')[0]+' (pval '+lost_motif_minus.split(',')[3]+')'+' [note, approx. position]'), file = f)  \n",
    "                print('', file = f)\n",
    "            else:\n",
    "                pass\n",
    "                print('\\n', file = f)\n",
    "            \n",
    "# Log TFBS collation operations time duration\n",
    "TFBScollationDuration = str(datetime.now() - startTime_TFBScollation).split(':')[0]+' hr|'+str(datetime.now() - startTime_TFBScollation).split(':')[1]+' min|'+str(datetime.now() - startTime_TFBScollation).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime_TFBScollation).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Process accessory files  \n",
    "#### *Transfer allele data to .csv spreadsheet, complete population summary, finalize report of script operation metrics*  \n",
    "**Data availability:** The raw data underlying allele definitions and imputed genotypes are housed in a session-specific dictionary, **imputedgenotypes_dict**. These data are made available to a user in spreadsheet format, by transferring dictionary content to a pandas dataframe and then to a comma-separated output file, **allele_definitions.csv**.\n",
    "\n",
    "**Population summary:** This script focuses on sample-specific designation of alleles and imputed genotypes, but also reports aggregate population-level statistics in **population_summary.txt**.\n",
    "\n",
    "**Script metrics:** Along with operating system properties, user-specified variables, and input fastq file properties recorded earlier, metadata concerning output file sizes and script operation time durations are recorded in **script_metrics.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Assess files in output directory\n",
    "file_set = [file for file in os.listdir(output_directory) if Path(file).suffix in ('.txt','.fa')] \n",
    "\n",
    "# Assign script end time\n",
    "endTime = datetime.now()\n",
    "endTimestr = str(endTime).split(' ')[1].split('.')[0]\n",
    "\n",
    "# Log entire script operations time duration\n",
    "processingDuration = str(datetime.now() - startTime).split(':')[0]+' hr|'+str(datetime.now() - startTime).split(':')[1]+' min|'+str(datetime.now() - startTime).split(':')[2].split('.')[0]+' sec|'+str(datetime.now() - startTime).split(':')[2].split('.')[1]+' microsec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Prepare final report of file size metrics and time durations to **script_metrics.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "filename = Path(str(output_path)+ '/'+processdate+'_script_metrics.txt')\n",
    "with open(filename, 'a') as f:\n",
    "    print(\"\"\"\\n\\nFile output information:\n",
    "    Output directory: \"\"\" + str(output_directory) +\n",
    "'\\n    Total file #: ' + str(len(file_set)) +\n",
    "'\\n    Total file output sizes: '+path_size(str(output_directory)), file = f)\n",
    "    for file in file_set:\n",
    "        print('    '+file+': '+path_size(str(output_directory)+'/'+file), file = f)\n",
    "    print(\"\"\"\\n\\nScript operation times:\n",
    "    start time: \"\"\"+startTimestr+\n",
    "    '\\n    makeblastdb and fasta-get-markov processing time: '+makeblastdb_fastagetmarkov_operationsDuration+\n",
    "    '\\n    fasta processing time: '+readcountDuration+\n",
    "    '\\n    alignments processing time: '+alignmentsDuration+\n",
    "    '\\n    allele definitions processing time: '+allele_definitionsDuration+\n",
    "    '\\n    TFBS processing time (FIMO): '+fimoDuration+\n",
    "    '\\n    TFBS collation processing time: '+TFBScollationDuration+\n",
    "    '\\n    total processing time: '+processingDuration+\n",
    "    '\\n    end time: '+endTimestr, file = f)\n",
    "f.close()\n",
    "          \n",
    "# End of script operations\n",
    "print(\"\\nScript has completed.  Please find output files at \"+str(output_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "############################################################################# end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
